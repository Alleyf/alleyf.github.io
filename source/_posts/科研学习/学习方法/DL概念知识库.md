---
title: DL概念知识库
date: 2024-04-06 22:46:30
tags: 
sticky: 60
excerpt: 
author: fcs
index_img: https://picsum.photos/800/250
lang: zh-CN
theme: default
_class: lead
paginate: true
backgroundColor: 
backgroundImage: url('https://marp.app/assets/hero-background.svg')
number headings: auto, first-level 1, max 5, start-at 1, 1.1
---

![](https://picsum.photos/800/250)

# 1 logits

模型的 logits 是模型在训练过程中输出的**原始预测值（Tensor张量）**，它们是`模型最后一层（通常是 softmax 层）之前的输出`。在自然语言处理（NLP）中，logits 通常是一个实数向量，其**长度等于模型词汇表的大小**。每个元素在向量中对应一个词汇表中的词，并表示模型预测该词作为下一个词的概率。

$$
logits：[0.1, 0.2, 0.3, ...]-->soft(logits)=[0.05, 0.1, 0.15, ...]
$$
在 softmax 函数应用于 logits 之后，会得到一个概率分布，其中每个元素代表相应词汇表中的词作为下一个输出词的概率。具体来说，softmax 函数会将 logits 转换为概率，使得这些概率的总和为 1。这样，每个输出位置上的概率分布可以被解释为模型对于下一个词的预测。


# 2 贪婪解码

贪婪解码（Greedy Decoding）是自然语言处理中一种常见的解码策略，尤其是在使用大型语言模型（如循环神经网络RNN、长短期记忆网络LSTM、Transformer等）进行文本生成任务时。这种解码方法的核心思想是在每个时间步骤中，选择具有最高概率的单词作为输出，从而构建出一个完整的序列。 ^956a45

### 2.1.1 贪婪解码的步骤

1. **初始化**：开始时，模型接收一个起始符号（如句子的开始标记`<s>`）作为输入。
2. **预测第一个词**：模型基于起始符号预测下一个词的概率分布。
3. **选择最高概率词**：从概率分布中选择概率最高的词作为输出序列的第一个词。
4. **迭代过程**：将已选择的词添加到输出序列的末尾，并将其作为下一步的上下文输入模型，重复预测下一个词。
5. **终止条件**：当模型输出一个结束符号（如句子的结束标记`</s>`）或者达到预设的最大长度时，解码过程结束。

### 2.1.2 贪婪解码的特点

- **简单高效**：贪婪解码算法实现简单，计算效率高，因为它在每个时间步骤中只选择一个最佳选项。
- **确定性**：解码过程是确定性的，相同的输入总是产生相同的输出序列。
- **缺乏灵活性**：由于贪婪解码在每个步骤中只考虑单一的最佳选项，它可能无法捕捉到序列中的多样性和复杂性。

### 2.1.3 贪婪解码的局限性

- **子优化**：贪婪解码可能导致子优化问题，即在局部步骤中选择最佳词，但这些选择可能不会导致整体上最佳的序列。
- **缺乏全局观**：贪婪解码没有考虑整个序列的可能性，可能忽略了其他可能更好的序列。

在某些情况下，为了克服贪婪解码的局限性，研究者们开发了其他更复杂的解码策略，如束搜索（Beam Search）和采样（Sampling）等，这些方法能够在解码过程中考虑多个候选词，从而提高生成序列的多样性和质量。


# 3 降采样

池化操作（Pooling）是卷积神经网络（CNN）中的一种重要机制，用于降低特征图（feature map）的空间维度，也就是高度和宽度。降采样（downsampling）是池化操作的一种形式，它通过减少数据的维度来减少计算量和防止过拟合，同时保留重要的特征信息。

降采样的具体过程通常如下：

1. **选择池化窗口**：首先，确定一个池化窗口（pooling window），这个窗口可以是方形或矩形，大小可以是2x2、3x3等。
2. **滑动窗口**：将池化窗口沿着特征图的宽度和高度方向滑动，每次滑动都有一定的步长（stride），步长决定了窗口移动的间隔。
3. **聚合特征**：在每个窗口内，根据某种聚合函数（如最大值、平均值等）来合并窗口内的特征，得到一个新的、更小的聚合特征表示。
4. **生成新的特征图**：重复上述过程，直到覆盖了整个原始特征图，最终生成一个新的、降采样后的特征图。

降采样的主要目的是减少数据的空间尺寸，这样做有几个好处：

- **减少参数和计算量**：降采样后的特征图尺寸更小，意味着模型中的参数数量减少，计算量也随之降低，这有助于提高模型的运行效率。
- **防止过拟合**：通过降低特征的空间分辨率，模型不能学习到过于细节的特征，这有助于模型泛化到新的、未见过的数据上，从而减少过拟合的风险。
- **保留重要特征**：尽管降采样会丢失一些信息，但聚合函数（如最大池化）有助于保留最显著的特征，如边缘、纹理等，这对于识别任务来说是关键的。

然而，降采样也可能导致一些有用的信息丢失，特别是当池化窗口较大或步长较大时。因此，在设计CNN时，需要权衡降采样的程度和保持足够信息的需求。在实践中，除了最大池化和平均池化，还有许多其他的池化技术，如全局池化、空间金字塔池化等，它们都旨在以不同的方式平衡信息的保留和降维的需求。

# 4 