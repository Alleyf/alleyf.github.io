---
title: A survey of large language models
date: 2024-06-09 22:27:29
tags:
  - 科研
sticky: 80
excerpt: 
author: fcs
index_img: https://picsum.photos/800/250
lang: zh-CN
theme: am_blue
_class: lead
paginate: true
headingDivider:
  - 1
  - 2
  - 3
header: \ ![](http://www.whut.edu.cn/images/whutlogo.png)
footer: \ *[<i class="fa-solid fa-home"></i>范财胜（武汉理工大学）](http://alleyf.github.io)*  *alleyf@qq.com* *<i class="fa-solid fa-clock"></i>2024-01-02*
backgroundColor: 
backgroundImage: url('https://marp.app/assets/hero-background.svg')
---

<!-- _class: cover_a -->
<!-- _header: "" --> 
<!-- _footer: "" --> 
<!-- _paginate: "" --> 

# 主标题

##### 副标题

汇报人：[范财胜](http://alleyf.github.io)

所属单位：武汉理工大学

汇报时间：2024-06-09 22:26:10

联系方式：<alleyf@qq.com>

# 📕 目录

<!-- _class: cols2_ol_ci fglass toc_a  -->
<!-- _footer: "" -->
<!-- _header: "CONTENT" -->
<!-- _paginate: "" -->

- [引言](#📜%20引言)
- [研究方法](#📊%20研究方法)
- [实验结果](#🔬%20实验结果)
- [研究结论](#🚩%20研究结论)
- [创新点](#📌%20创新点)
- [感想&疑问](#💡%20感想%20&%20疑问)

# 📜 引言

<!-- _class: navbar  -->
<!-- _header: \ **[引言](#3)** *[研究方法](#7)* *[实验结果](#8)* *[研究结论](#9)* *[创新点](#10)* *[感想&疑问](#11)* -->

| Meta  |                                                                          Value                                                                           |
| :---: | :------------------------------------------------------------------------------------------------------------------------------------------------------: |
|  标题   |                                                            A survey of large language models                                                             |
| 期刊/会议 |                                                                          arxiv                                                                           |
|  作者   |                                                           Zhao, Wayne Xin and Zhou, Kun and Li                                                           |
|  来源   |                                           赵鑫-中国人民大学高瓴人工智能学院教授-国家优青<br>研究方向：大语言模型、自然语言处理以及推荐系统<br>谷歌学术引用1.8万余次                                            |
|  日期   |                                                                       2023年11月23日                                                                        |
| 原文链接  |                                           [A Survey of Large Language Models](http://arxiv.org/abs/2303.18223)                                           |
|  标签   |                                                                    自然语言处理、语言模型、统计语言模型                                                                    |
| 源码仓库  | [GitHub - RUCAIBox/LLMSurvey: The official GitHub page for the survey paper "A Survey of Large Language Models".](https://github.com/RUCAIBox/LLMSurvey) |

## 📑 Background

使机器能够像人类一样阅读、写作和交流，一直是一个长期的研究挑战

语言建模（LM）是提高机器语言智能的主要方法之一，其研究四个主要发展阶段：

- 统计语言模型（SLM）：基本思想是基于马尔可夫假设构建单词预测模型，例如根据最近的上下文预测下一个单词。具有固定上下文长度 n 的 SLM 也称为 n-gram 语言模型，存在维数灾难：由于需要估计指数数量的转移概率，因此很难准确估计高阶语言模型。因此，引入了专门设计的平滑策略，例如退避估计和Good-Turing估计来缓解数据稀疏问题。
- 神经语言模型（NLM）：通过神经网络（例如多层感知器（MLP）和循环神经网络（RNN））来表征单词序列的概率，旨在为各种 NLP 任务构建统一的端到端解决方案，word2vec被提议构建一个简化的浅层神经网络来学习分布式单词表示，这被证明在各种 NLP 任务中非常有效。这些研究开创了使用语言模型进行表示学习（超越词序列建模），对 NLP 领域产生了重要影响。
- 预训练语言模型 (PLM)：早期 ELMo 被提出通过首先预训练双向 LSTM (biLSTM) 网络（而不是学习固定的单词表示），然后根据特定的下游微调 biLSTM 网络来捕获上下文感知的单词表示。此外，基于具有自注意力机制的高度并行化的Transformer架构，BERT是通过在大规模未标记语料库上使用专门设计的预训练任务来预训练双向语言模型而提出的。这些预先训练的上下文感知单词表示作为通用语义特征非常有效，这在很大程度上提高了 NLP 任务的性能标准。这项研究启发了大量的后续工作，它设定了“预训练和微调”的学习范式。遵循这一范式，已经开展了大量关于 PLM 的研究，引入了不同的架构（例如 GPT-2  和 BART ）或改进的预训练策略 。在此范例中，通常需要微调 PLM 以适应不同的下游任务。
- 大型语言模型（LLM）：研究人员发现，扩展 PLM（例如扩展模型大小或数据大小）通常会提高下游任务的模型能力（即遵循尺度定律）。许多研究通过训练更大的 PLM（例如 175B 参数 GPT-3 和 540B 参数 PaLM）来探索性能极限。尽管缩放主要在模型大小上进行（具有相似的架构和预训练任务），但这些大型 PLM 显示出与小型 PLM（例如 330M 参数 BERT 和 1.5B 参数 GPT-2）不同的行为，并显示出令人惊讶的能力（称为解决一系列复杂任务的新兴能力）。例如，GPT-3 可以通过上下文学习解决小样本任务，而 GPT-2 则不能做得很好。因此，研究界为这些大型 PLM 创造了“大型语言模型 (LLM)” 一词，吸引了越来越多的研究关注（见图 1）。 LLM 的一个显着应用是 ChatGPT，它调整了 GPT 系列的 LLM 进行对话，呈现出惊人的与人类对话的能力。从图 1 可以看出，ChatGPT 发布后，与 LLM 相关的 arXiv 论文急剧增加。

起初，统计语言模型主要辅助一些特定任务（例如检索或语音任务），其中预测或估计的概率可以增强特定任务方法的性能。随后，神经语言模型专注于学习与任务无关的表示（例如特征），旨在减少人类特征工程的工作量。此外，预语言模型学习上下文感知表示，可以根据下游任务进行优化。对于最新一代的语言模型，LLM通过探索模型容量的尺度法则来增强，可以将其视为通用任务求解器。综上所述，在演化过程中，语言模型能够解决的任务范围得到了极大的扩展，语言模型所达到的任务性能也得到了显著增强。

## ⚜ Motivation

在现有文献中，PLM 已被广泛讨论和调查，而 LLM 却鲜有系统性的综述。因此，作者首先强调 LLM 和 PLM 之间的三个主要区别。

首先，LLMs展示了一些令人惊讶的新兴能力，这些能力在以前的小型 PLM 中可能无法观察到。这些能力是语言模型在复杂任务上表现的关键，使人工智能算法变得前所未有的强大和有效。

其次，LLMs将彻底改变人类开发和使用人工智能算法的方式。区别于PLMs，访问 LLM 的主要方法是通过提示界面（例如 GPT-4 API）。人们必须了解LLMs如何工作，并以LLMs可以遵循的方式安排他们的任务。

第三，LLMs的发展不再明确区分研究和工程。LLMs的开发需要丰富的大规模数据处理和分布式并行训练的实践经验。研究人员必须解决复杂的工程问题。

## 👑 Contribution

本次调查从四个主要方面对LLM的最新进展进行了文献综述，其中包括预训练（如何预训练一个有能力的人） LLM）、微调（如何有效地调整预训练的LLM以更好地使用）、应用（如何使用LLM来解决各种下游任务）和能力评估（如何评估LLM的能力和现有的实证结果）。

我们彻底梳理文献并总结法学硕士的主要发现、技术和方法。对于本次调查，我们还通过收集 LLM 的支持资源创建了一个 GitHub 项目网站，链接为 https://github.com/RUCAIBox/LLMSurvey。我们还知道一些关于 PLM 或 LLM 的相关评论文章。这些论文要么讨论 PLM，要么讨论 LLM 的一些特定（或一般）方面。与它们相比，我们重点关注开发和使用LLM的技术和方法，对LLM的重要方面提供了相对全面的参考。

# 📊 研究方法

<!-- _class: navbar  -->
<!-- _header: \ *[引言](#3)* **[研究方法](#7)** *[实验结果](#8)* *[研究结论](#9)* *[创新点](#10)* *[感想&疑问](#11)* -->

# 🔬 实验结果

<!-- _class: navbar  -->
<!-- _header: \ *[引言](#3)* *[研究方法](#7)* **[实验结果](#8)** *[研究结论](#9)* *[创新点](#10)* *[感想&疑问](#11)* -->

# 🚩 研究结论

<!-- _class: navbar  -->
<!-- _header: \ *[引言](#3)* *[研究方法](#7)* *[实验结果](#8)* **[研究结论](#9)** *[创新点](#10)* *[感想&疑问](#11)* -->

# 📌 创新点

<!-- _class: navbar  -->
<!-- _header: \ *[引言](#3)* *[研究方法](#7)* *[实验结果](#8)* *[研究结论](#9)* **[创新点](#10)** *[感想&疑问](#11)* -->

# 💡 感想 & 疑问

<!-- _class: navbar  -->
<!-- _header: \ *[引言](#3)* *[研究方法](#7)* *[实验结果](#8)* *[研究结论](#9)* *[创新点](#10)* **[感想&疑问](#11)** -->

# 致谢

<!-- _class: cover_d -->
<!-- _paginate: "" -->
<!-- _footer: 厚德博学，追求卓越 -->
<!-- _header: ![](http://www.whut.edu.cn/images/whutlogo.png) -->

感谢各位老师和师兄师姐们的聆听，如有不当敬请批评指正！

后期目标：学习 pytorch 框架的使用，继续阅读基础经典论文与前沿价值论文。
