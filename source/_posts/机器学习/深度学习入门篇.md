---
title: DeepLearing
tags: [CV，NLP]
date: 2023-05-31 14:27:39
sticky: 20
excerpt: Deep_learning
---

# 引言

![image.png|400](https://s2.loli.net/2023/05/31/mlfBuXI7pt2eCic.png)

> 特征工程的作用：
> 	- 数据特征决定了模型的上限
> 	- 预处理和特征提取是最核心的
> 	- 算法与参数选择决定了如何逼近这个上限


## 反向传播

<img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202307251455407.png"/>

## 正则化

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202307251607343.png)

## 神经元

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202307251610745.png)

## 激活函数

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202307251611276.png)

激活函数对比：
> **Relu 为为主流，Sigmoid 会出现梯度消失现象**
![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202307251616218.png)

## 数据预处理

> 不同的预处理结果会使得模型的效果发生很大的差异！
![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308012100980.png)
1. 参数初始化
 - 参数初始化同样非常重要！
 - 通常我们都使用随机策略来进行参数初始化
$$W = 0.01*np.random.randn (D, H)$$

## Drop-Out

> 过拟合是神经网络非常头疼的一个大问题！
![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308012106533.png)

## 卷积

> 计算过程如下图所示：
> - 分别计算三个通道输入与卷积核进行内积，再将三通道结果求和得到输出（特征图）

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308021949385.png)

> 特征图的个数（深度）等于卷积核的个数，如上图所见两个卷积核进行多尺度特征提取得到两个特征图。

### 步长




### 边缘填充





