---
title: 深度学习论文阅读总结  
date: 2023-03-17 19:09:35  
tags: [CV]  
sticky: 85
excerpt: some overview about CV classical papers。
banner: "https://dogefs.s3.ladydaily.com/~/source/wallhaven/small/qz/qzj5ll.jpg?w=400&h=200&fmt=webp"
banner_y: 0.5
banner_x: 0.44879
theme: gradient
---


# AlexNet

## 摘要

alex 团队使用大规模深度卷积神经网络对 ImageNet 图像分类竞赛中的120万张高分辨率图像进行分类的工作。主要总结如下:

1. 构建了一个包含**6000万参数、65万个神经元**的大型卷积神经网络,包含**5个卷积层、若干（3）最大池化层和3个全连接层**。
   
2. 在ImageNet 2010比赛的数据集上,该模型达到了**top-1错误率37.5%,top-5错误率17.0%**,显著优于此前最佳结果。
   
3. 使用**非饱和神经元（ReLU）和高效（并行）的 GPU** 卷积实现加速了训练。使用了新的 **dropout 正则化**方法减少全连接层过拟合。
   
4. 在ImageNet 2012比赛中,该模型的变体获得了top-5测试错误率15.3%的获胜结果,优于第二名的26.2%。  

---
## 背景介绍


1. 当前的对象识别方法大量依赖机器学习, 可以通过收集**更大的数据集**、学习**更强大的模型**和使用**更好的技术**来**防止过拟合**来提高性能。

2. **以前**的图像**数据集相对较小**, 新的更大数据集如 **ImageNet** 包含了**上千万个带标签的高分辨率图像**。

3. **卷积神经网络** (CNN)是一个大容量的模型, 可以通过调节其**深度和广度**来控制其能力。它对图像的假设大多是正确的, 比标准前馈神经网络的**参数更少, 更易训练**。

4. 当前的 **GPU 和高度优化的 2 D 卷积**实现足以训练有趣的大型 CNN, 大型数据集如 ImageNet 包含足够的标记样本来训练这些模型而不会严重过拟合。

5. 本文训练了迄今为止**最大的卷积神经网络之一**, 在 ImageNet 数据集上取得了最好的结果。作者编写了**高度优化的 GPU 2 D 卷积**等操作实现。

6. 网络的大小主要受当前 **GPU 内存和可接受的训练时间**的限制。作者使用了几种技术来防止过拟合。更深的网络层次似乎很重要。

7. 实验表明, 可以通过等待**更快的 GPU 和更大的数据集**来进一步改进结果。

---

## 方法

### 1.数据集

1. 将图像**下采样**至固定分辨率 **256 × 256**。
2. **重新缩放**图像，使**短边**的长度为 **256**

---
### 2.架构

#### 2.1ReLU Nonlinearity

1. 传统的神经元模型使用 **tanh 或 sigmoid** 作为激活函数, **训练时间较慢且易于梯度消失**。

2. 使用 `ReLU (f (x)=max (0, x))` 作为激活函数可以显著**加快训练速度并且防止过拟合**, 使大型网络的训练成为可能。

> 总之, 使用 ReLU 等非饱和激活函数可以显著加速训练, 这对于大规模深度学习模型非常重要。文章通过实验确证了这一点。


![image.png|325](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308181911241.png)
> ReLU（实线）在 CIFAR-10 上达到 25% 的训练错误率，比具有 tanh 神经元的等效网络（虚线）快六倍。

---
#### 2.2Training on Multiple GPUs

1. 采用的并行化方案本质上是在**每个 GPU 上各放一半的内核（或神经元）**，还有一个额外的技巧：**GPU 仅在某些层中进行通信**
   - 缺点：<span style="background:rgba(255, 183, 139, 0.55)">选择连接模式是交叉验证的一个问题</span>
   - 优点：<span style="background:rgba(205, 244, 105, 0.55)">能够精确调整通信量，直到它达到计算量的可接受的分数</span>

2. 该方案将 top-1 和 top-5 错误率降低了 **1.7% 和 1.2%**
3. 双 GPU 网络的**训练时间**比单 GPU 网络略少

---
#### 2.3Local Response Normalization

局部正则化表达式：
$$b_{x,y}^{i}=a_{x,y}^{i}(k+ \alpha \sum _{j= \min(0,i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^{j})^{2})^{\beta}$$
参数：$k = 2, n = 5, α = 10−4, β = 0.75$ 均为超参数，N：层中内核总数，n：同一空间位置的 n 个“相邻”核，

> 效果：该方案将 top-1和 top-5错误率分别降低了**0.4%和0.3%**

---
#### 2.4Overlapping Pooling

1. 池化层对同一个核映射中相邻的神经元组输出进行汇总。

2. 传统上,相邻池化单元汇总的邻域不重叠 (非重叠池化)。

3. 池化层可以看作是一个栅格, 每个池化单元之间间隔 s 像素, 并汇总一个大小为 z x z 的邻域。
   - 如果 **s=z**, 则为传统的非重叠池化。 
   - 如果 **s<z**, 则为有重叠的池化, 该网络采用了 s=2, z=3 的重叠池化。

4. 该方案将 top-1和 top-5错误率分别降低了**0.4%和0.3%**。

5. 具有重叠池化的模型在训练时往往**更难过拟合**。

> 与传统非重叠池化相比, 重叠池化层可以**汇总有重叠的邻域**, 往往可以**提高准确率并减少过拟合**。该网络采用了具体参数为 s=2, z=3 的重叠池化。

---
#### 2.5Overall Architecture

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308181930025.png)

> 1. 一个 GPU 运行图顶部的层部分，而另一个 GPU 运行底部的层部分。 GPU 仅在某些层进行通信。
> 2. 第二到第五个卷积层顺序连接,每层卷积核大小和数量分别为256个5x5x48、384个3x3x256、384个3x3x192和256个3x3x192,全连接层每个有4096个神经元。

---
### 3.防止过拟合

#### 3.1Data Augmentation

1. 普遍通用方式：使用**标签保留转换**人为地**扩大数据集**
2. 本文采用的方式：
   - 生成**图像平移和水平反射**（256 x 256 --> 224 × 224 附带水平反射）
   - **改变**训练图像中 **RGB 通道的强度**（ RGB 像素值集执行 **PCA**，对于每个训练图像，我们添加多个找到的主成分）

> [!NOTE] Conclusion
> - 该方案将 top-1错误率降低了1%以上
> - 对象身份对于**照明强度和颜色**的变化是不变的

> 新概念：**PCA**
> PCA (主成分分析)是一种广泛用于数据分析和机器学习的统计方法。它的主要作用是:
> 1. **减维**: PCA 通过正交变换, 将高维数据集投影到低维空间中, 同时尽量保留原数据的信息。
> 2. **去相关**: PCA 可以去除数据间的相关性, 将高维数据转换为线性无关的低维特征。
> 3. **特征提取**: PCA 可以将数据集中的主要特征提取出来, 去除噪声, 使得数据集中的特征更加显著。
> 其基本思想是利用正交变换把原始数据集投影到某个最佳子空间,使得投影后的样本点能够最大限度保留原样本点的信息 (样本点的方差)。数学上, 它通过求数据协方差矩阵的特征向量来实现。PCA 是一种常用的无监督学习方法。

---
#### 3.2Dropout

> 具体做法：在前两个全连接层中使用 dropout，没有 dropout 时会过拟合，Dropout 大约使迭代次数加倍来收敛

- Dropout 是一种很有效的模型集成技术, 在训练时**随机让一半的隐层单元输出为 0**。

- Dropout 使得**每次输入时神经网络采样不同的架构**, 但这些架构**共享权重**。

- 这**减少**了**神经元之间的复杂适应性**, 迫使其学习更加鲁棒的特征。

- 在测试时使用所有神经元, 但将其**输出乘以 0.5** 来近似地取指数数量 dropout 网络的预测分布的几何平均。

> 总的来说, dropout 通过随机扰动实现模型集成, 减少过拟合, 但需要更多的迭代次数。

---
## 实验

1. 随机梯度下降，批量大小为 128 ，动量为 0.9，权重衰减为 0.0005，轮数为 90
2. 初始化的每层的权重来自标准差为 0.01 的零均值高斯分布
3. 第二、第四和第五卷积层以及全连接隐藏层中的神经元偏差为常数 1，其余层中的神经元偏差为常数 0
4. 学习率初始化为0.01，当验证错误率停止以当前学习率提高时，将学习率除以 10

> 新概念：L 2
> - L 2 在深度学习中通常指 L 2 正则化 (L 2 regularization)。L 2 正则化是一种常用的正则化技术, 主要作用是为了**防止模型过拟合**。
> - L 2 正则化的具体做法是在模型的损失函数中**引入权重参数的 L 2 范数作为惩罚项**。也就是在损失函数中添加**λ*||w||^2**, 其中 w 是模型的参数,||w||^2 表示参数 w 的 L 2 范数,λ是超参数用于控制惩罚力度。
> - 引入 L 2 范数会惩罚模型参数的大小, **使得参数向 0 收缩, 从而达到减小模型复杂度**, 防止过拟合的目的。一般来说, L 2 正则化在实际应用中使用更为广泛, 因为 L 2 正则化更容易优化, 且可以**防止参数爆炸**。

---
## 结论

- 大型深层卷积神经网络通过**纯监督学习**在一个非常复杂的数据集上取得了**记录性的结果**。

- **删除任何一个卷积层**, 网络的**性能**都会**下降**, 如删除中间层会导致 top-1 性能降低约 **2%** 

- 作者希望能在**视频序列**上应用**更大更深的卷积网络**, 因为时间信息可以提供静态图像中没有的有用信息。


---

# VGG

> VGGNet 是牛津大学视觉几何组 (Visual Geometry Group)提出的模型，该模型在 2014 ImageNet 图像分类与定位挑战赛 ILSVRC-2014 中取得在分类任务第二，定位任务第一的优异成绩。VGGNet 突出的贡献是证明了很小的卷积，通过增加网络深度可以有效提高性能。VGG 很好的继承了 Alexnet 的衣钵同时拥有着鲜明的特点。相比 Alexnet ，VGG 使用了更深的网络结构，证明了增加网络深度能够在一定程度上影响网络性能。 

## 摘要

- 研究非常深的 CNN 在大规模图像分类中的效果
- 主要贡献: 通过增加网络深度来提高 ImageNet 分类性能
- 实验中深度从 11 层增加到 19 层, 使用 3 x 3 小卷积核
- 分类错误率随网络深度的增加而降低
- 在 ILSVRC 2014 分类任务中获得了第二名

---
## 背景介绍

- CNN 近年来在图像识别中成功, 得益于大数据集和 GPU 计算
- ILSVRC 竞赛推动了从浅层到深层网络的发展
- 本文通过增加深度来提升 CNN 性能
- 使用全 3 x 3 小卷积核, 增加非线性映射判别性, 减少参数量

---
## 方法

### 卷积网络配置

![image.png|400](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308191508459.png)


![image.png|400](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308191506627.png)

![image.png|325](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308191506264.png)

- 输入 224 x 224 RGB 图像, 均值归一化预处理
- 所有卷积层使用 3 x 3 小卷积核
- 1 x 1 卷积可看作通道上的线性变换
- 卷积步长为 1, 使用 padding 保持分辨率  
- 5 次最大池化下采样
- 3 个全连接层, 最后分类层 1000 个输出
- 所有隐层使用 ReLU 激活函数
- 提出了 11-19 层的 5 种配置进行比较

### 分类框架

- SGD 优化, momentum, dropout, batchsize=256
- 先训练 11 层网络 A, 初始化更深网络部分层权重
- 训练集数据增强: 随机裁剪, 翻转, 颜色抖动 
- 测试时多尺度密集预测, 平均结果

---

## 实验

### 分类实验

- 在 ImageNet 上评估不同配置
- 错误率随深度减小, 19 层效果趋于饱和
- 多尺度评估优于单尺度, 训练尺度抖动也优于固定尺度
- 模型融合进一步提升, 2 模型组合 top-5 错误率 6.8%

### 定位实验

- 使用深度 CNN 预测 bounding box 进行对象定位
- 每类回归优于共享回归, 微调所有层优于仅全连接层
- 最终定位错误率 25.3%, 优于之前 state-of-the-art (SOTA)

### 很深特征泛化性

- 在其他数据集上测试 ImageNet 预训练模型作为特征提取器
- 在 PASCAL VOC、Caltech 等数据集上获得了 state-of-the-art 的分类效果
- 显示了很深 CNN 模型学到的特征具有很强的泛化性


---

## 重点总结

> 1. **小卷积核组**: 作者通过堆叠多个 3\*3 的卷积核 (少数使用 1\*1）来替代大的卷积核，以减少所需参数；
> 
> 2. **小池化核**: 相比较于 AlexNet 使用的 3\*3 的池化核，VGG 全部为 2\*2 的池化核；
> 
> 3. **网络更深特征图更宽**: 卷积核专注于扩大通道数，池化专注于缩小高和宽，使得模型更深更宽的同时，计算量的增加不断放缓；
> 
> 4. **将卷积核替代全连接**: 作者在测试阶段将三个全连接层替换为三个卷积，使得测试得到的模型结构可以接收任意高度或宽度的输入。
> 
> 5. **多尺度**: 作者从多尺度训练可以提升性能受到启发，训练和测试时使用整张图片的不同尺度的图像，以提高模型的性能。
> 
> 6. **去掉了 LRN 层**: 作者发现深度网络中 LRN（Local Response Normalization，局部响应归一化）层作用不明显。

---
## 重要概念

### 感受野

**感受野（感受野其实就是结果层一个神经元节点发生变动能影响多少个输入层神经元节点）**

计算公式：
$$feature_{i}=(feature_{i+1}-1)\times window_{-}strides+kernel_{-}size$$
$f_i$是第i层感受野，times为第i步的步长，kernel_size为卷积核或池化核大小。

![image.png|450](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308191524259.png)


> - 感受野决定了神经网络能“看到”输入图像的哪一部分。每个神经元只连接到输入图像的一小块区域, 这块区域就是这个神经元的感受野。
> - 在卷积神经网络中, 通过设置卷积核的大小, 可以控制每个神经元的感受野大小。典型的卷积核大小是 **3 x 3 或 5 x 5**。
> - **网络层数越深, 神经元的感受野会越来越大**。这是因为每个神经元不仅连接到输入, 还连接到前一层的神经元。所以层与层堆叠, 会逐步扩大感受野。
> - 扩大感受野有利于**捕捉输入图像的全局信息**, 以及学习更抽象和高级的特征表示。但是计算量也会增加。所以需要在两者之间做 trade-off。
> - 在**全连接层**, 每个神经元的感受野是**整个输入图像**。
> 
> 总体来说, 调整卷积网络的感受野大小, 是控制网络提取**局部还是全局信息**的一个重要方法。这在卷积网络设计中需要特别考虑。

---
# GoogLeNet




---

# ResNet

## 摘要

本文提出了一个残差学习框架，用于训练深度神经网络。通过引入残差块，可以让网络更深，同时避免了梯度消失和梯度弥散的问题。在多个视觉识别任务中，残差网络都取得了比传统网络更好的结果。

---


## 背景介绍（相关研究）

在近些年中，深度网络逐渐往更深的方向发展，但是更深的网络训练更加困难，因为梯度消失和梯度弥散的问题会导致网络难以收敛。之前的研究提出了一些方法，如使用更好的初始化方法、使用更好的激活函数等，但是这些方法并不能完全解决问题。本文提出了一种新的方法，即残差学习框架，通过引入残差块来解决梯度消失和梯度弥散的问题。

![image.png|500](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211604278.png)

![image.png|500](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211613960.png)


---

## 方法

### Identity Mapping by Shortcuts

$$y=F(x, \left\{ W_{i}\right\})+x$$
$$y=F(x, \left\{ W_{i}\right\})+W_{s}x$$

> 若残差块输入输出维度一致，则直接短接即可；
   若唯独不一致，则对输入进行降维增维处理将输入输出维度统一。

### 网络架构

![image.png|450](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211621589.png)

> 实现为等维直接短接，虚线为异维进行 A (0 填充) /B（投影快捷映射）




---


## 实验

### ImageNet

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211631231.png)

### CIFAR-10

![image.png|450](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211631353.png)

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211632281.png)

上图左边指的是 Plain Net, 然而 deeper 的时候，会出现明显的 **degradation**。当深度达到 100+的时候，plain Net 的错误率达到了 60%以上。
上图中间这是 ResNet，可以看到当 deeper 的时候，错误率也在降低，并没有出现所谓的 degradation。
然而右边则显示 **1202 layers** 的 ResNet 的错误率比 **101 layers** 的**错误率高**，作者认为这**不是 degradation 导致**，而是由于这么**大的参数量在 cifar 10 这个小数据集上过拟合**导致。


---
## 重点总结

### 1.残差块
作者发现，当加深模型深度时，模型的测试效果没有变好，反而变差。不符合所想的结果，因为浅层的网络应该是深层网络的一个子集，深层网络不应该比浅层网络表现的不好。然而实验的结果表明，当在浅层网络后加恒等映射层，深层网络的效果反而没有浅层网络好。为什么会出现这种原因呢？作者猜想，可能是因为深层网络难以训练。那么为什么会难以训练呢？可能是因为深层网络的最后面的层难以学习到恒等映射，因为浅层网络已经有很好的表现效果了，最后面的层如果找不到更好的表示效果就需要学习恒等映射，而让网络去学习恒等映射是很困难的。

> 为了解决网络学习恒等映射困难的问题，作者就想，既然学习恒等映射困难，我们就让网络学习 $f(x)=0$ 这个映射。也就引出了参差学习块。

![20200221174711860.png|400](https://img-blog.csdnimg.cn/20200221174711860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ1OTYyMDUy,size_16,color_FFFFFF,t_70)

正常网络的输入是 x ，输出是f(x)，如果我们要学习恒等映射，也就是让网络的部分学会 f(x)=x，即图中 f(x) 的部分学会一个 f(x) = x 的映射关系，但是我们说直接让网络去学习恒等映射很困难，怎么办？

> 假设网络的输出是 h(x)，那么不加 shortcut connection 的网络的输出就是 h(x)=f(x)=x，为了让网络更好拟合恒等映射，我们让 h(x)=f(x)+x，那么我们看网络要学习的映射 f(x)，就变为了 f(x)=h(x)−x，这时我们发现，如果直接让 f(x)=0，那么 h(x)=x，也就是说我们让网络输出的结果和恒等映射相同，而网络只学习了 f(x)=0 这个映射，这个映射要比 f(x)=x 恒等映射更好学习，即模型会更好训练，而不受深层的影响，因为深层中多余的层我们可以都学恒等映射，最起码结果不会比浅层的结果差。然而实验结果表明，加上 shortcut connection 的深层网络比不加 shortcut connection 的浅层网络效果还好，这也就说明了深层网络所能提取的信息更高，抽象能力更强。


### 2.恒等快捷映射和投影快捷映射

当 shortcut 的输入通道数和输出通道数相同时，我们可以使用恒等映射即 $f (x)=x$，也就是将 shortcut 的输出直接加上输入即可（恒等快捷映射）。

但是，当 shortcut connection 的输入不等于输出的时候怎么办？（两种方法）
> ① 将输入数据扩充维度，多余的维度的数据用 0 填充。
> ② 使用 1 x 1 的卷积扩充维度（投影快捷映射）
> 
> 我们实际上有三种方式组合：
> ① 零填充快捷连接用来增加维度，所有的快捷连接是没有参数的。
> ② 投影快捷连接用来增加维度，其它的快捷连接是恒等的。
> ③ 所有的快捷连接都是投影。
以上三种情况都比没有加 shortcut connection 的好，效果 ③ > ② > ①，但是 ③ 的计算量太大，提升的效果也不大，所以我们一般不用，我们一般用的最多的是 ②。

投影快捷映射：

- 当输入和输出的维度不匹配时，使用一个额外的卷积层来进行维度匹配。
- 可以用于解决深度残差网络中的维度不匹配问题，从而提高网络的性能。
- 在ResNet中，投影快捷映射是指使用一个1x1的卷积层来进行维度匹配。
### 3.Deeper Bottleneck Architectures

![image.png](https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/202308211610964.png)

> 先用1x1降维，3x3进行卷积，再用1x1进行升维。  
   事实上，deeper 左边这种结构也能获得很好的效果。那么为什么要用右边这种结构呢？ 主要是源自于**practical**，因为**左边**这种结构在**训练时间上要比右边结构长的多**。

<p align="justify">当换成上图右边这种 Bottleneck 结构的时候，可以发现152层的 ResNet 竟然比VGG16/19 都要少的复杂度</p>

