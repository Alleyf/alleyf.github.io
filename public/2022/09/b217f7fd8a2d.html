

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://qnpicmap.fcsluck.top/pics/202311162214229.png">
  <link rel="icon" href="http://qnpicmap.fcsluck.top/pics/202311162214229.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="alleyf">
  <meta name="keywords" content="">
  
    <meta name="description" content="Deep_learning">
<meta property="og:type" content="article">
<meta property="og:title" content="ML_DL">
<meta property="og:url" content="https://alleyf.github.io/2022/09/b217f7fd8a2d.html">
<meta property="og:site_name" content="alleyf">
<meta property="og:description" content="Deep_learning">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/e040a31c13924cd5ae004308f73ce50c6979239be95f4f3ebb8c92b1311d77ab">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220912142745507.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220902190129721.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220902190439588.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/a8c80653f39b479dab9f6867a638b64c405e79d6540c4307a22f43c4b0e228bc">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/c8f06f423acc488fb391bca5dcf8f2b02d7444ef526f41599b6b430ae24659c1">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220910232915712.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193234679.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193426778.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193512661.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193512661.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911194420932.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/54adf1fa9d7e4dc9a6a23cdf42417fbf624a0b3406564b9990b7852ce8fac9c7">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/16edd4f2b23d48ae8ba699e4cd00d65a75a43e7a8b7e4f8a98513588031f1e32">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/3dbcf420c25e4d62b21500fa1278bb17e1c219ebcd96457a8fc5cf10e4c4e360">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/7dd04e92862540b390b11246b2375a828df2237b79a64aed9a698a2272fa7b46">
<meta property="og:image" content="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911195508553.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/9f7cc7174c6f482b9b0d3a1f9bdc1195cf9bf0bc24d140da87aceba2dde4ea5d">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/94f0437e6a454a0682f3b831c96a62bdaf40898af25145ec9b5b50bc80391f5c">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/99487dca6520441db5073d1c154b5d2fb1174b5cf4d946c29f9d80a209bc2687">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/5f8322f6172542dab0f78684b70efe45d819895332af4cabb7c536217ab0bb26">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/903f552bc55b4a5eba71caa7dd86fd2d7b71b8ebb6cb4500a5f5711f465707f3">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/9c146e7d9c4a4119a8cd09f7c8b5ee61f2ac1820a221429a80430291728b9c4a">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/a5fd990c5355426183a71b95aa28a59f979014f6905144ddb415c5a4fe647441">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/ef129caf64254318821e9410bb71ab1f45fff20e4282482986081d44a1e3bcbb">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/13a942e5ec7f4e91badb2f4613c6f71a00e51c8afb6a435e94a0b47cedac9515">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/1e0f066dc9fa4e2bbc942447bdc0578c2ffc6afc15684154ae84bcf31b298d7b">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/f4cf80f95424411a85ad74998433317e721f56ddb4f64e6f8a28a27b6a1baa6b">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/99b879c21113494a9d7315eeda74bc4c8fea07f984824a03bf8411e946c75f1b">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/53c389bb3c824706bd2fbc05f83ab0c6dd6b5b2fdedb4150a17e16a1b64c243e">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/b5a46f7e0fbe4f8686a71d9a2d330ed09f23bca565a44e0d941148729fd2f7d7">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/34de60a675b64468a2c3fee0844a168d53e891eaacf643fd8c1c9ba8e3812bcc">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/018ac3d24c22423a8a263dfd0f0f7f49898b29e707c14dbdb8f9f5abdde75449">
<meta property="article:published_time" content="2022-09-10T16:26:00.000Z">
<meta property="article:modified_time" content="2023-03-26T16:30:00.505Z">
<meta property="article:author" content="alleyf">
<meta property="article:tag" content="Numpy">
<meta property="article:tag" content="Pandas">
<meta property="article:tag" content="PIL">
<meta property="article:tag" content="Matplotlib">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://ai-studio-static-online.cdn.bcebos.com/e040a31c13924cd5ae004308f73ce50c6979239be95f4f3ebb8c92b1311d77ab">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>ML_DL - alleyf</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alleyf.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":true,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"d57048846da607439cf11718741f2eb0","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?d57048846da607439cf11718741f2eb0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  

  

  



  
<!-- hexo injector head_end start --><script> let HEXO_MMEDIA_DATA = { js: [], css: [], aplayerData: [], metingData: [], artPlayerData: [], dplayerData: []}; </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="alleyf" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mr.Alleyf</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/gallery/">
                <i class="iconfont icon-images"></i>
                画廊
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://api.likepoems.com/img/nature') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="ML_DL"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        alleyf
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-09-11 00:26" pubdate>
          2022年9月11日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          81k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          675 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">ML_DL</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：8 个月前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h1><h2 id="Paddle常用API"><a href="#Paddle常用API" class="headerlink" title="Paddle常用API"></a>Paddle常用API</h2><center>
<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html">飞浆API</a>
</center>
​	

<table>
<thead>
<tr>
<th align="center"><strong>目录</strong></th>
<th><strong>功能和包含的API</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">paddle.*</td>
<td>paddle 根目录下保留了常用API的别名，包括：paddle.tensor, paddle.framework, paddle.device 目录下的所有API</td>
</tr>
<tr>
<td align="center">paddle.tensor</td>
<td>Tensor操作相关的API，包括 创建zeros, 矩阵运算matmul, 变换concat, 计算add, 查找argmax等</td>
</tr>
<tr>
<td align="center">paddle.framework</td>
<td>框架通用API和动态图模式的API，包括 no_grad 、 save 、 load 等。</td>
</tr>
<tr>
<td align="center">paddle.device</td>
<td>设备管理相关API，包括 set_device, get_device 等。</td>
</tr>
<tr>
<td align="center">paddle.linalg</td>
<td>线性代数相关API，包括 det, svd 等。</td>
</tr>
<tr>
<td align="center">paddle.fft</td>
<td>快速傅里叶变换的相关API，包括 fft, fft2 等。</td>
</tr>
<tr>
<td align="center">paddle.amp</td>
<td>自动混合精度策略，包括 auto_cast 、 GradScaler 等。</td>
</tr>
<tr>
<td align="center">paddle.autograd</td>
<td>自动求导相关API，包括 backward、PyLayer 等。</td>
</tr>
<tr>
<td align="center">paddle.callbacks</td>
<td>日志回调类，包括 ModelCheckpoint 、 ProgBarLogger 等。</td>
</tr>
<tr>
<td align="center">paddle.distributed</td>
<td>分布式相关基础API。</td>
</tr>
<tr>
<td align="center">paddle.distributed.fleet</td>
<td>分布式相关高层API。</td>
</tr>
<tr>
<td align="center">paddle.hub</td>
<td>模型拓展相关的API，包括 list、load、help 等。</td>
</tr>
<tr>
<td align="center">paddle.io</td>
<td>数据输入输出相关API，包括 Dataset, DataLoader 等。</td>
</tr>
<tr>
<td align="center">paddle.jit</td>
<td>动态图转静态图相关API，包括 to_static、 ProgramTranslator、TracedLayer 等。</td>
</tr>
<tr>
<td align="center">paddle.metric</td>
<td>评估指标计算相关的API，包括 Accuracy, Auc等。</td>
</tr>
<tr>
<td align="center">paddle.nn</td>
<td>组网相关的API，包括 Linear 、卷积 Conv2D 、 循环神经网络 RNN 、损失函数 CrossEntropyLoss 、 激活函数 ReLU 等。</td>
</tr>
<tr>
<td align="center">paddle.onnx</td>
<td>paddle转换为onnx协议相关API，包括 export 等。</td>
</tr>
<tr>
<td align="center">paddle.optimizer</td>
<td>优化算法相关API，包括 SGD，Adagrad, Adam 等。</td>
</tr>
<tr>
<td align="center">paddle.optimizer.lr</td>
<td>学习率衰减相关API，包括 NoamDecay 、 StepDecay 、 PiecewiseDecay 等。</td>
</tr>
<tr>
<td align="center">paddle.regularizer</td>
<td>正则化相关API，包括 L1Decay、L2Decay 等。</td>
</tr>
<tr>
<td align="center">paddle.static</td>
<td>静态图下基础框架相关API，包括 Variable, Program, Executor等</td>
</tr>
<tr>
<td align="center">paddle.static.nn</td>
<td>静态图下组网专用API，包括 全连接层 fc 、控制流 while_loop&#x2F;cond 。</td>
</tr>
<tr>
<td align="center">paddle.text</td>
<td>NLP领域API，包括NLP领域相关的数据集， 如 Imdb 、 Movielens 。</td>
</tr>
<tr>
<td align="center">paddle.utils</td>
<td>工具类相关API，包括 CppExtension、CUDAExtension 等。</td>
</tr>
</tbody></table>
<p><a href="">121</a></p>
<h2 id="飞桨产业级深度学习开源开放平台"><a href="#飞桨产业级深度学习开源开放平台" class="headerlink" title="飞桨产业级深度学习开源开放平台"></a>飞桨产业级深度学习开源开放平台</h2><p>飞桨（PaddlePaddle）以百度多年的深度学习技术研究和业务应用为基础，集深度学习核心训练和推理框架、基础模型库、端到端开发套件、丰富的工具组件于一体，是中国首个自主研发、功能丰富、开源开放的产业级深度学习平台。飞桨于2016 年正式开源，是主流深度学习框架中一款完全国产化的产品。相比国内其他产品，飞桨是一个功能完整的深度学习平台，也是唯一成熟稳定、具备大规模推广条件的深度学习开源开放平台。根据国际权威调查机构IDC报告显示，2021年飞桨已位居中国深度学习平台市场综合份额第一。</p>
<p>目前，飞桨已凝聚477万开发者，基于飞桨开源深度学习平台创建56万个模型，服务了18万家企事业单位。飞桨助力开发者快速实现AI想法，创新AI应用，作为基础平台支撑越来越多行业实现产业智能化升级，并已广泛应用于智慧城市、智能制造、智慧金融、泛交通、泛互联网、智慧农业等领域，如 <strong>图1</strong> 所示。</p>
<p>飞桨产业级深度学习开源开放平台包含核心框架、基础模型库、端到端开发套件与工具组件几个部分，各组件使用场景如 <strong>图2</strong> 所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/e040a31c13924cd5ae004308f73ce50c6979239be95f4f3ebb8c92b1311d77ab" srcset="/img/loading.gif" lazyload width="1000" ></center>
<center><br>图2：飞桨开源组件使用场景概览 </br></center>
概览图上半部分是从开发、训练到部署的全流程工具；下半部分是预训练模型、封装工具、各领域的开发套件和模型库等模型资源，支持深度学习模型从训练到部署的全流程。

<h2 id="模型开发和训练组件"><a href="#模型开发和训练组件" class="headerlink" title="模型开发和训练组件"></a>模型开发和训练组件</h2><p><strong>飞桨核心框架Paddle</strong>支持用户完成基础的模型编写和单机训练功能。除核心框架之外，飞桨还提供了<strong>分布式训练框架FleetAPI</strong>、<strong>云上任务提交工具PaddleCloud</strong>和<strong>多任务学习框架PALM</strong>。</p>
<h3 id="模型部署组件"><a href="#模型部署组件" class="headerlink" title="模型部署组件"></a>模型部署组件</h3><p>针对不同硬件环境，飞桨提供了丰富的支持方案：</p>
<ul>
<li><p><strong>Paddle Inference</strong>：飞桨原生推理库，用于服务器端模型部署，支持Python、C、C++、Go等语言，可将模型融入业务系统。</p>
</li>
<li><p><strong>Paddle Serving</strong>：飞桨服务化部署框架，用于云端服务化部署，可将模型作为单独的Web服务。</p>
</li>
<li><p><strong>Paddle Lite</strong>：飞桨轻量化推理引擎，用于Mobile、IoT等场景的部署，有着广泛的硬件支持。</p>
</li>
<li><p><strong>Paddle.js</strong>：使用JavaScript（Web）语言部署模型，用于在浏览器、小程序等环境快速部署模型。</p>
</li>
<li><p><strong>PaddleSlim</strong>：模型压缩工具，获得更小体积的模型和更快的执行性能，通常在模型部署前使用。</p>
</li>
<li><p><strong>X2Paddle</strong>：飞桨模型转换工具，将其他框架模型转换成Paddle模型，转换格式后可以方便的使用上述5个工具。</p>
</li>
</ul>
<h3 id="其他全研发流程的辅助工具组件"><a href="#其他全研发流程的辅助工具组件" class="headerlink" title="其他全研发流程的辅助工具组件"></a>其他全研发流程的辅助工具组件</h3><ul>
<li><p><strong>AutoDL</strong>：飞桨自动化深度学习工具，自动搜索最优的网络结构与超参数，实现网络结构设计。免去用户在诸多网络结构中选择困难的烦恼和人工调参的繁琐工作。</p>
</li>
<li><p><strong>VisualDL</strong>：飞桨可视化分析工具，以丰富的图表呈现训练参数变化趋势、模型结构、数据样本、高维数据分布、精度召回曲线等模型关键信息，帮助用户清晰直观地理解深度学习模型训练过程及模型结构，启发优化思路。</p>
</li>
<li><p><strong>PaddleFL</strong>：飞桨联邦学习框架，通过PaddleFL复制和比较不同的联邦学习算法，实现大规模分布式集群部署，并且提供丰富的横向和纵向联邦学习策略及其在计算机视觉、自然语言处理、推荐算法等领域的应用。</p>
</li>
</ul>
<h3 id="产业级开源模型库"><a href="#产业级开源模型库" class="headerlink" title="产业级开源模型库"></a>产业级开源模型库</h3><p>飞桨提供了产业级开源模型库，覆盖计算机视觉(PaddleCV)、自然语言处理(PaddleNLP)、推荐(PaddleRec)、语音(PaddleSpeech)四大应用领域，包含经过产业实践长期打磨的主流模型以及在国际竞赛中的夺冠模型。同时，飞桨将主流模型按照领域组织成端到端开发套件，助力快速的产业应用。</p>
<p><strong>1）预训练模型和封装工具</strong>：通过低代码形式，支持企业POC快速验证、快速实现深度学习算法开发及产业部署。</p>
<ul>
<li><p><strong>PaddleHub</strong>：飞桨预训练模型应用工具，提供超过350个预训练模型，覆盖文本、图像、视频、语音四大领域。模型即软件，通过Python API或者命令行工具，一行代码完成预训练模型的预测。结合Fine-tune API，10行代码完成迁移学习，是进行原型验证（POC）的首选。</p>
</li>
<li><p><strong>PaddleX</strong>：飞桨全流程开发工具，以低代码的形式支持开发者快速实现深度学习算法开发及产业部署。提供极简Python API和可视化界面Demo两种开发模式，可一键安装。提供CPU、GPU、树莓派等通用硬件高性能部署方案，并通过Maufacture SDK支持用户流程化串联部署任务，极大降低部署成本。</p>
</li>
</ul>
<p><strong>2）开发套件</strong>：针对具体的应用场景提供了全套的研发工具，例如：在图像检测场景不仅提供了预训练模型，还提供了数据增强等工具。开发套件覆盖计算机视觉、自然语言处理、语音、推荐四大主流领域，甚至还包括图神经网络和增强学习。开发套件可以提供一个领域极致优化（State Of The Art）的实现方案，曾有国内团队使用飞桨的开发套件获得了国际建模竞赛的大奖。</p>
<ul>
<li><strong>PaddleClas</strong>：飞桨图像分类开发套件，提供通用图像识别系统PP-ShiTu，可高效实现高精度车辆、商品等多种识别任务；同时提供37个系列213个高性能图像分类预训练模型，其中包括10万分类预训练模型、PP-LCNet等明星模型；以及SSLD知识蒸馏等先进算法优化策略，可被广泛应用于高阶视觉任务，辅助产业及科研领域快速解决多类别、高相似度、小样本等业界难点。</li>
<li><strong>PaddleDetection</strong>：飞桨目标检测开发套件，内置190个主流目标检测、实例分割、跟踪、关键点检测算法，其中包括服务器端和移动端产业级SOTA模型、冠军方案和学术前沿算法，并提供配置化的网络模块组件、十余种数据增强策略和损失函数等高阶优化支持和多种部署方案，在打通数据处理、模型开发、训练、压缩、部署全流程的基础上，提供丰富的案例及教程，加速算法产业落地应用。</li>
<li><strong>PaddleSeg</strong>：飞桨图像分割套件PaddleSeg，提供语义分割、交互式分割、全景分割、Matting四大图像分割能力，涵盖30+主流分割网络，80+高质量预训练模型。通过模块化的设计，提供了配置化驱动和API调用等两种应用方式，帮助开发者更便捷地完成从训练到部署的全流程图像分割应用，被广泛应用在自动驾驶、遥感、医疗、质检、巡检、互联网娱乐等行业。</li>
<li><strong>PaddleOCR：</strong> 飞桨文字识别开发套件，旨在打造一套丰富、领先且实用的OCR工具库，开源了基于PP-OCRv2的实用超轻量中英文OCR模型、通用中英文OCR模型，以及德法日韩等80多种多语言OCR模型，并提供上述模型训练方法和多种预测部署方式。同时开源文本风格数据合成工具Style-Text和半自动文本图像标注工具PPOCRLabel，目前已经成为全球知名的OCR开源项目。</li>
<li><strong>PaddleGAN</strong>：飞桨生成对抗网络开发套件，提供图像生成、风格迁移、超分辨率、影像上色、人脸属性编辑、人脸融合、动作迁移等前沿算法，其模块化设计，便于开发者进行二次研发，同时提供30+预训练模型，助力开发者快速开发丰富的应用。</li>
<li><strong>PaddleVideo</strong>：飞桨视频模型开发套件，具有高指标的模型算法、全流程可部署、更快训练速度和丰富的应用案例、保姆级教程并在体育、安防、互联网、媒体等行业有广泛应用，如：足球&#x2F;蓝球动作检测、乒乓球动作识别、花样滑冰动作识别、知识增强的大规模视频分类打标签、智慧安防、内容分析等产业实践案例。</li>
<li><strong>ERNIEKit</strong>：飞桨语义理解套件，基于持续学习的知识增强语义理解框架实现，内置业界领先的系列ERNE预训练模型，该套件全面升级飞桨框架v2.2，同时支持动态图和静态图，兼顾了开发的便利性与部署的高性能需求。同时还能够支持各类NLP算法任务Fine-tuning,包含保证极速推理的Fast-inference API，灵活部署的ERNIE Service和轻量化解决方案ERNIE Slim，训练过程所见即所得，支持动态debug同时方便二次开发。</li>
<li><strong>PLSC</strong>：飞桨海量类别分类套件，为用户提供了大规模分类任务从训练到部署的全流程解决方案。提供简洁易用的高层API，通过数行代码即可实现千万类别分类模型的训练，并提供快速部署模型的能力。</li>
<li><strong>ElasticCTR</strong>：飞桨个性化推荐开发套件，可以实现分布式训练CTR预估任务和基于PaddleServing的在线个性化推荐服务。PaddleServing服务化部署框架具有良好的易用性、灵活性和高性能，可以提供端到端的CTR训练和部署解决方案。ElasticCTR具备产业实践基础、弹性调度能力、高性能和工业级部署等特点。</li>
<li><strong>Parakeet</strong>：飞桨语音合成套件，提供了灵活、高效、先进的文本到语音合成工具，帮助开发者更便捷高效地完成语音合成模型的开发和应用。</li>
<li><strong>PGL</strong>：飞桨图学习框架，业界首个提出通用消息并行传递机制，支持万亿级巨图的工业级图学习框架。PGL 原生支持异构图，支持分布式图存储及分布式学习算法，支持 GNNAutoScale实现单卡深度图卷积，覆盖 30+ 图学习模型，并内置 KDDCup 2021 PGL 冠军算法。内置图推荐算法套件 Graph4Rec 以及高效知识表示套件 Graph4KG。历经大量真实工业应用验证，能够灵活、高效地搭建前沿的大规模图学习算法。</li>
<li><strong>PARL</strong>：飞桨深度强化学习框架，夺得NeurIPS强化学习挑战赛三连冠。具有高灵活性、可扩展性和高性能的特点，可支持实现数千台CPU和GPU的高性能并行，实现了数十种主流强化学习算法的示例，覆盖了从单智能体到多智能体，离散决策到连续控制，离线学习到在线学习等多样化的强化学习支持。此外，飞桨还发布了业界首个通用元智能体训练环境MetaGym，提升算法在不同配置智能体和多种环境中的适应能力，目前包含四轴飞行器、电梯调度、四足机器狗、3D迷宫等多个仿真训练环境。</li>
<li><strong>Paddle Quantum</strong>：量桨，基于飞桨的量子机器学习工具集，提供组合优化、量子化学等前沿功能，常用量子电路模型，以及丰富的量子机器学习案例，帮助开发者便捷地搭建量子神经网络，开发量子人工智能应用。</li>
<li><strong>PaddleHelix</strong>：飞桨螺旋桨生物计算平台，面向新药研发、疫苗设计、精准医疗等场景提供AI能力。在新药研发上，提供基于大规模数据预训练的分子表征和蛋白表征模型，助力分子生成、药物筛选、化合物合成等任务，同时提供从分子生成到药物筛选到全流程pipeline。在疫苗设计上，Linear系列算法相比传统方法在RNA折叠上提升了几百上干倍的效率，在mRNA序列设计上其结构紧密性、稳定性、细胞内蛋白表达水平以及动物免疫原性方面超过标准算法设计的基准序列。在精准医疗上,PaddleHelix提供了利用组学信息精准定位药物，进行双药联用提升治愈率的高性能模型。</li>
</ul>
<p>开发套件中的大量模型，既可以通过调整配置文件直接使用的模式，也可以定位到模型的源代码文件进行二次研发。</p>
<blockquote>
<p>比较几种模型工具，PaddleHub的使用最为简易，二次研发模型源代码的灵活性最好。读者可以参考“使用PaddleHub-&gt;基于配置文件使用各领域的开发套件-&gt;二次研发原始模型代码”的顺序来使用飞桨产业级模型库，在此基础上根据业务需求进行优化，即可达到事半功倍的效果。</p>
</blockquote>
<h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h2><ol>
<li><p>模型假设</p>
</li>
<li><p>优化目标</p>
</li>
<li><p>寻解算法</p>
<ul>
<li><p>公式法：</p>
</li>
<li><p>梯度下降法:  </p>
<ol>
<li><p>方向</p>
</li>
<li><p>步长</p>
</li>
<li><p>特征缩放</p>
</li>
<li><p>梯度决定步长</p>
</li>
</ol>
</li>
</ul>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220912142745507.png" srcset="/img/loading.gif" lazyload alt="image-20220912142745507"></p>
<p>激活函数(sigmoid)：</p>
<p>$$<br>y&#x3D;\frac{1}{1+e^{-x}}<br>$$</p>
<p>激活函数(RELU):</p>
<p>$$<br>y&#x3D;<br>\begin{cases}<br>0 &amp; x&lt;0\<br>kx &amp; x&gt;&#x3D;0<br>\end{cases}<br>$$<br><strong>卷积核</strong>就是<strong>提取的特征</strong>，训练过程中通过<strong>反向传播</strong>不断<strong>改变卷积核</strong>，最终得到的卷积核就是识别目标的特征。</p>
<p>卷积核与输入的卷积结果为特征层</p>
<p>优化器，学习率，学习策略，模型，预处理方法</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220902190129721.png" srcset="/img/loading.gif" lazyload style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220902190439588.png" srcset="/img/loading.gif" lazyload style="zoom: 50%;" />

<h2 id="常用的库"><a href="#常用的库" class="headerlink" title="常用的库"></a>常用的库</h2><ol>
<li><p><strong>numpy</strong>是Python科学计算库的基础。包含了强大的N维数组对象和向量运算。</p>
</li>
<li><p><strong>pandas</strong>是建立在numpy基础上的高效数据分析处理库，是Python的重要数据分析库。</p>
</li>
<li><p><strong>Matplotlib</strong>是一个主要用于绘制二维图形的Python库。用途：绘图、可视化</p>
</li>
<li><p><strong>PIL</strong>库是一个具有强大图像处理能力的第三方库。用途：图像处理</p>
</li>
</ol>
<hr>
<h3 id="Numpy库"><a href="#Numpy库" class="headerlink" title="Numpy库"></a>Numpy库</h3><ul>
<li>可以使用array函数从常规Python<strong>列表或元组</strong>中创建数组。得到的数组的类型是从Python列表中元素的类型推导出来的</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment">#将列表转换为二维数组</span><br>array = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>                 [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br><span class="hljs-built_in">print</span>(array)<br><span class="hljs-comment">#将元组转换为二维数组</span><br>array = np.array(((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),<br>                 (<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>)))<br><span class="hljs-built_in">print</span>(array)<br><span class="hljs-comment">#将列表或元组转为一维数组</span><br>a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]) <br>b = np.array((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(a,b)<br></code></pre></td></tr></table></figure>

<h4 id="1-常用函数"><a href="#1-常用函数" class="headerlink" title="1.常用函数"></a>1.常用函数</h4><ul>
<li><p>zeros():可以创建指定长度或者形状的全0数组</p>
</li>
<li><p>ones():可以创建指定长度或者形状的全1数组</p>
</li>
<li><p>empty():创建一个数组，其初始内容是随机的,取决于内存的状态</p>
</li>
<li><p>arange():创建一个指定处末位置和步长的数字数组</p>
</li>
<li><p>random.random((m,n))：生成0~1之间的m行n列的数组</p>
</li>
<li><p>random.randint(a, b, (m,n )):生成a~b之间左开右闭区间m行n列的随机整数</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">zeroarray = np.zeros((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),dtype=<span class="hljs-string">&#x27;int64&#x27;</span>)<br><span class="hljs-built_in">print</span>(zeroarray)<br>onearray = np.ones((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>),dtype=<span class="hljs-string">&#x27;int64&#x27;</span>)<br><span class="hljs-built_in">print</span>(onearray)<br>emptyarray = np.empty((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(emptyarray)<br>array = np.arange( <span class="hljs-number">10</span>, <span class="hljs-number">31</span>,<span class="hljs-number">5</span> )<br><span class="hljs-built_in">print</span>(array)<br>randarray = np.random.random((m,n))<br>randint = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">9</span>, (<span class="hljs-number">1</span>, )).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>输出数组的一些信息，如维度、形状、元素个数、元素类型等</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">array = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>],[<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>]])<br><span class="hljs-built_in">print</span>(array)<br><span class="hljs-comment">#数组维度</span><br><span class="hljs-built_in">print</span>(array.ndim)<br><span class="hljs-comment">#数组形状</span><br><span class="hljs-built_in">print</span>(array.shape)<br><span class="hljs-comment">#数组元素个数</span><br><span class="hljs-built_in">print</span>(array.size)<br><span class="hljs-comment">#数组元素类型</span><br><span class="hljs-built_in">print</span>(array.dtype)<br></code></pre></td></tr></table></figure>

<ul>
<li>reshape([m,n])重新定义数字的形状为m行n列。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">array1 = np.arange(<span class="hljs-number">6</span>).reshape([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<span class="hljs-comment">#重塑为2行3列</span><br><span class="hljs-built_in">print</span>(array1)<br><br><br>array2 = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]],dtype=np.int64).reshape([<span class="hljs-number">3</span>,<span class="hljs-number">2</span>])<span class="hljs-comment">#重塑为3行2列</span><br><span class="hljs-built_in">print</span>(array2)<br><br></code></pre></td></tr></table></figure>

<h4 id="2-数组计算"><a href="#2-数组计算" class="headerlink" title="2.数组计算"></a>2.数组计算</h4><p><strong>注:大小相等的数组之间的任何算术运算都会将运算应用到元素级。同样，数组与标量的算术运算也会将那个标量值传播到各个元素.</strong></p>
<ol>
<li><p>矩阵基础运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">arr1 = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br>arr2 = np.ones([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=np.int64)<br><br><span class="hljs-built_in">print</span>(arr1 + arr2)<br><span class="hljs-built_in">print</span>(arr1 - arr2)<br><span class="hljs-built_in">print</span>(arr1 * arr2)<br><span class="hljs-built_in">print</span>(arr1 / arr2)<br><span class="hljs-built_in">print</span>(arr1 ** <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>矩阵乘法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">arr3 = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br>arr4 = np.ones([<span class="hljs-number">3</span>,<span class="hljs-number">2</span>],dtype=np.int64)<br><span class="hljs-built_in">print</span>(arr3)<br><span class="hljs-built_in">print</span>(arr4)<br><span class="hljs-built_in">print</span>(np.dot(arr3,arr4))<br></code></pre></td></tr></table></figure>
</li>
<li><p>矩阵其他运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">sum</span>(arr3,axis=<span class="hljs-number">1</span>)) <span class="hljs-comment">#axis=1,每一行求和 axie=0,每一列求和</span><br><span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">max</span>(arr3))<br><span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">min</span>(arr3))<br><span class="hljs-built_in">print</span>(np.mean(arr3))<br><span class="hljs-built_in">print</span>(np.argmax(arr3),axis=<span class="hljs-number">0</span>/<span class="hljs-number">1</span>)<span class="hljs-comment">#axis=1,每一行求最大值的索引 axie=0,每一列求最大值索引</span><br><span class="hljs-built_in">print</span>(np.argmin(arr3),axis=<span class="hljs-number">0</span>/<span class="hljs-number">1</span>)<span class="hljs-comment">#axis=1,每一行求最小值的索引 axie=0,每一列求最小值索引</span><br><span class="hljs-built_in">print</span>(arr3.transpose())<span class="hljs-comment">#求数组的转置矩阵</span><br><span class="hljs-built_in">print</span>(arr3.flatten())<span class="hljs-comment">#将数组降为一维</span><br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-数组的索引与切片"><a href="#3-数组的索引与切片" class="headerlink" title="3.数组的索引与切片"></a>3.数组的索引与切片</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">arr5 = np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">6</span>).reshape([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(arr5)<br><span class="hljs-built_in">print</span>(arr5[<span class="hljs-number">1</span>])<span class="hljs-comment">#索引第一行</span><br><span class="hljs-built_in">print</span>(arr5[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>])<span class="hljs-comment">#索引第一行第二列</span><br><span class="hljs-built_in">print</span>(arr5[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<span class="hljs-comment">#索引第一行第二列</span><br><span class="hljs-built_in">print</span>(arr5[<span class="hljs-number">1</span>,:])<span class="hljs-comment">#切片第一行</span><br><span class="hljs-built_in">print</span>(arr5[:,<span class="hljs-number">1</span>])<span class="hljs-comment">#切片第一列</span><br><span class="hljs-built_in">print</span>(arr5[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>:<span class="hljs-number">2</span>])<span class="hljs-comment">#切片第一行第零列开始向后的两个元素，返回一个列表</span><br></code></pre></td></tr></table></figure>

<hr>
<h4 id="4-线性代数常用库函数"><a href="#4-线性代数常用库函数" class="headerlink" title="4.线性代数常用库函数"></a>4.线性代数常用库函数</h4><center><img src=https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193010646.png></center>



<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 矩阵相乘</span><br>a = np.arange(<span class="hljs-number">12</span>)<br>b = a.reshape([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>c = a.reshape([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>])<br><span class="hljs-comment"># 矩阵b的第二维大小，必须等于矩阵c的第一维大小</span><br>d = b.dot(c) <span class="hljs-comment"># 等价于 np.dot(b, c)</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;a: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;b: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(b))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;c: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(c))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;d: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(d))<br><br><span class="hljs-comment"># numpy.linalg  中有一组标准的矩阵分解运算以及诸如求逆和行列式之类的东西</span><br><span class="hljs-comment"># np.linalg.diag 以一维数组的形式返回方阵的对角线（或非对角线）元素，</span><br><span class="hljs-comment"># 或将一维数组转换为方阵（非对角线元素为0）</span><br>e = np.diag(d)<br>f = np.diag(e)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;d: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(d))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;e: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(e))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;f: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(f))<br><br><span class="hljs-comment"># trace, 计算对角线元素的和</span><br>g = np.trace(d)<br>g<br><br><span class="hljs-comment"># det，计算行列式</span><br>h = np.linalg.det(d)<br>h<br><br><span class="hljs-comment"># eig，计算特征值和特征向量</span><br>i = np.linalg.eig(d)<br>i<br><br><span class="hljs-comment"># inv，计算方阵的逆</span><br>tmp = np.random.rand(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>j = np.linalg.inv(tmp)<br>j<br></code></pre></td></tr></table></figure>

<h4 id="5-文件写入和读取"><a href="#5-文件写入和读取" class="headerlink" title="5.文件写入和读取"></a>5.文件写入和读取</h4><ol>
<li>tofile()和fromfile()</li>
</ol>
<ul>
<li><p>tofile()将数组中的数据以二进制格式写进文件</p>
</li>
<li><p>tofile()输出的数据不保存数组形状和元素类型等信息</p>
</li>
<li><p>fromfile()函数读回数据时需要用户指定元素类型，并对数组的形状进行适当的修改</p>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 使用np.fromfile从文本文件&#x27;housing.data&#x27;读入数据</span><br><span class="hljs-comment"># 这里要设置参数sep = &#x27; &#x27;，表示使用空白字符来分隔数据</span><br><span class="hljs-comment"># 空格或者回车都属于空白字符，读入的数据被转化成1维数组</span><br>d = np.fromfile(<span class="hljs-string">&#x27;./work/housing.data&#x27;</span>, sep = <span class="hljs-string">&#x27; &#x27;</span>)<br>d<br></code></pre></td></tr></table></figure>

<ol start="2">
<li>save() 和 load(),savez()</li>
</ol>
<p><u>NumPy专用的二进制格式保存数据，它们会自动处理元素类型和形状等信息</u></p>
<ul>
<li><p>如果想将多个数组保存到一个文件中，可以使用savez()，savez()的第一个参数是文件名，其后的参数都是需要保存的数组，也可以使用关键字参数为数组起名非关键字参数传递的数组会自动起名为arr_0、arr_1、…。savez()输出的是一个扩展名为npz的压缩文件，其中每个文件都是一个save()保存的npy文件，文件名和数组名相同</p>
</li>
<li><p>load()自动识别npz文件，并且返回一个类似于字典的对象，可以通过数组名作为键获取数组的内容</p>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 产生随机数组a</span><br>a = np.random.rand(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br>np.save(<span class="hljs-string">&#x27;a.npy&#x27;</span>, a)<br><br><span class="hljs-comment"># 从磁盘文件&#x27;a.npy&#x27;读入数组</span><br>b = np.load(<span class="hljs-string">&#x27;a.npy&#x27;</span>)<br><br><span class="hljs-comment"># 检查a和b的数值是否一样</span><br>check = (a == b).<span class="hljs-built_in">all</span>()<br>check<br></code></pre></td></tr></table></figure>



<h4 id="6-统计函数"><a href="#6-统计函数" class="headerlink" title="6.统计函数"></a>6.统计函数</h4><p>可以通过数组上的一组数学函数对整个数组或某个轴向的数据进行统计计算。主要包括如下统计方法：</p>
<ul>
<li><code>mean</code>：计算算术平均数，零长度数组的mean为NaN。</li>
<li><code>std</code>和<code>var</code>：计算标准差和方差，自由度可调（默认为n）。</li>
<li><code>sum</code> ：对数组中全部或某轴向的元素求和，零长度数组的sum为0。</li>
<li><code>max</code>和<code>min</code>：计算最大值和最小值。</li>
<li><code>argmin</code>和<code>argmax</code>：分别为最大和最小元素的索引。</li>
<li><code>cumsum</code>：计算所有元素的累加。</li>
<li><code>cumprod</code>：计算所有元素的累积。</li>
</ul>
<hr>
<p><strong>说明：</strong></p>
<p>sum、mean以及标准差std等聚合计算既可以当做数组的实例方法调用，也可以当做NumPy函数使用。</p>
<hr>
<h4 id="7-随机数np-random"><a href="#7-随机数np-random" class="headerlink" title="7.随机数np.random"></a>7.随机数np.random</h4><p>主要介绍创建ndarray随机数组以及随机打乱顺序、随机选取元素等相关操作的方法。</p>
<h5 id="7-1-创建随机ndarray数组"><a href="#7-1-创建随机ndarray数组" class="headerlink" title="7.1 创建随机ndarray数组"></a>7.1 创建随机ndarray数组</h5><p>创建随机ndarray数组主要包含设置随机种子、均匀分布和正态分布三部分内容，具体代码如下所示。</p>
<ul>
<li><p><strong>设置随机数种子</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 可以多次运行，观察程序输出结果是否一致</span><br><span class="hljs-comment"># 如果不设置随机数种子，观察多次运行输出结果是否一致</span><br>np.random.seed(<span class="hljs-number">10</span>)<br>a = np.random.rand(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>a<br></code></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p><strong>均匀分布</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成均匀分布随机数，随机数取值范围在[0, 1)之间</span><br>a = np.random.rand(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>a<br><span class="hljs-comment"># 生成均匀分布随机数，指定随机数取值范围和数组形状</span><br>a = np.random.uniform(low = -<span class="hljs-number">1.0</span>, high = <span class="hljs-number">1.0</span>, size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br>a<br></code></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><strong>正态分布</strong></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 生成标准正态分布随机数</span><br>a = np.random.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>a<br><span class="hljs-comment"># 生成正态分布随机数，指定均值loc和方差scale</span><br>a = np.random.normal(loc = <span class="hljs-number">1.0</span>, scale = <span class="hljs-number">1.0</span>, size = (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>a<br></code></pre></td></tr></table></figure>

<h5 id="7-2-随机打乱ndarray数组顺序"><a href="#7-2-随机打乱ndarray数组顺序" class="headerlink" title="7.2 随机打乱ndarray数组顺序"></a>7.2 随机打乱ndarray数组顺序</h5><ul>
<li>随机打乱1维ndarray数组顺序，发现所有元素位置都被打乱了，代码如下所示。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 生成一维数组</span><br>a = np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;before random shuffle: &#x27;</span>, a)<br><span class="hljs-comment"># 打乱一维数组顺序</span><br>np.random.shuffle(a)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;after random shuffle: &#x27;</span>, a)<br><span class="hljs-comment"># 生成一维数组</span><br>a = np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>)<br><span class="hljs-comment"># 将一维数组转化成2维数组</span><br>a = a.reshape(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;before random shuffle: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a))<br><span class="hljs-comment"># 打乱一维数组顺序</span><br>np.random.shuffle(a)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;after random shuffle: \n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a))<br></code></pre></td></tr></table></figure>

<h5 id="7-3-随机选取元素"><a href="#7-3-随机选取元素" class="headerlink" title="7.3 随机选取元素"></a>7.3 随机选取元素</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 随机选取部分元素</span><br>a = np.arange(<span class="hljs-number">30</span>)<br>b = np.random.choice(a, size=<span class="hljs-number">5</span>)<br>b<br></code></pre></td></tr></table></figure>

<h4 id="8-NumPy应用举例"><a href="#8-NumPy应用举例" class="headerlink" title="8. NumPy应用举例"></a>8. NumPy应用举例</h4><h5 id="5-1-计算激活函数Sigmoid和ReLU"><a href="#5-1-计算激活函数Sigmoid和ReLU" class="headerlink" title="5.1 计算激活函数Sigmoid和ReLU"></a>5.1 计算激活函数Sigmoid和ReLU</h5><p>使用ndarray数组可以很方便的构建数学函数，并利用其底层的矢量计算能力快速实现计算。下面以神经网络中比较常用激活函数Sigmoid和ReLU为例，介绍代码实现过程。</p>
<ul>
<li><strong>计算Sigmoid激活函数</strong></li>
</ul>
<p>$$<br>y &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p>
<ul>
<li><strong>计算ReLU激活函数</strong></li>
</ul>
<p>$$<br>y&#x3D;\left{<br>\begin{aligned}<br>0 &amp; , &amp; (x&lt;0) \<br>x &amp; , &amp; (x\ge 0)<br>\end{aligned}<br>\right.<br>$$</p>
<p>使用Numpy计算激活函数Sigmoid和ReLU的值，使用matplotlib画出图形，代码如下所示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># ReLU和Sigmoid激活函数示意图</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>%matplotlib inline<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib.patches <span class="hljs-keyword">as</span> patches<br><br><span class="hljs-comment">#设置图片大小</span><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">3</span>))<br><br><span class="hljs-comment"># x是1维数组，数组大小是从-10. 到10.的实数，每隔0.1取一个点</span><br>x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 计算 Sigmoid函数</span><br>s = <span class="hljs-number">1.0</span> / (<span class="hljs-number">1</span> + np.exp(- x))<br><br><span class="hljs-comment"># 计算ReLU函数</span><br>y = np.clip(x, a_min = <span class="hljs-number">0.</span>, a_max = <span class="hljs-literal">None</span>)<br><br><span class="hljs-comment">#########################################################</span><br><span class="hljs-comment"># 以下部分为画图程序</span><br><br><span class="hljs-comment"># 设置两个子图窗口，将Sigmoid的函数图像画在左边</span><br>f = plt.subplot(<span class="hljs-number">121</span>)<br><span class="hljs-comment"># 画出函数曲线</span><br>plt.plot(x, s, color=<span class="hljs-string">&#x27;r&#x27;</span>)<br><span class="hljs-comment"># 添加文字说明</span><br>plt.text(-<span class="hljs-number">5.</span>, <span class="hljs-number">0.9</span>, <span class="hljs-string">r&#x27;$y=\sigma(x)$&#x27;</span>, fontsize=<span class="hljs-number">13</span>)<br><span class="hljs-comment"># 设置坐标轴格式</span><br>currentAxis=plt.gca()<br>currentAxis.xaxis.set_label_text(<span class="hljs-string">&#x27;x&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br>currentAxis.yaxis.set_label_text(<span class="hljs-string">&#x27;y&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br><br><span class="hljs-comment"># 将ReLU的函数图像画在右边</span><br>f = plt.subplot(<span class="hljs-number">122</span>)<br><span class="hljs-comment"># 画出函数曲线</span><br>plt.plot(x, y, color=<span class="hljs-string">&#x27;g&#x27;</span>)<br><span class="hljs-comment"># 添加文字说明</span><br>plt.text(-<span class="hljs-number">3.0</span>, <span class="hljs-number">9</span>, <span class="hljs-string">r&#x27;$y=ReLU(x)$&#x27;</span>, fontsize=<span class="hljs-number">13</span>)<br><span class="hljs-comment"># 设置坐标轴格式</span><br>currentAxis=plt.gca()<br>currentAxis.xaxis.set_label_text(<span class="hljs-string">&#x27;x&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br>currentAxis.yaxis.set_label_text(<span class="hljs-string">&#x27;y&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure>

<h5 id="5-2-图像翻转和裁剪"><a href="#5-2-图像翻转和裁剪" class="headerlink" title="5.2 图像翻转和裁剪"></a>5.2 图像翻转和裁剪</h5><p>图像是由像素点构成的矩阵，其数值可以用ndarray来表示。将上述介绍的操作用在图像数据对应的ndarray上，可以很轻松的实现图片的翻转、裁剪和亮度调整，具体代码和效果如下所示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 导入需要的包</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># 读入图片</span><br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./work/images/000000001584.jpg&#x27;</span>)<br>image = np.array(image)<br><span class="hljs-comment"># 查看数据形状，其形状是[H, W, 3]，</span><br><span class="hljs-comment"># 其中H代表高度， W是宽度，3代表RGB三个通道</span><br>image.shape<br><span class="hljs-comment"># 原始图片</span><br>plt.imshow(image)<br><span class="hljs-comment"># 垂直方向翻转</span><br><span class="hljs-comment"># 这里使用数组切片的方式来完成，</span><br><span class="hljs-comment"># 相当于将图片最后一行挪到第一行，</span><br><span class="hljs-comment"># 倒数第二行挪到第二行，..., </span><br><span class="hljs-comment"># 第一行挪到倒数第一行</span><br><span class="hljs-comment"># 对于行指标，使用::-1来表示切片，</span><br><span class="hljs-comment"># 负数步长表示以最后一个元素为起点，向左走寻找下一个点</span><br><span class="hljs-comment"># 对于列指标和RGB通道，仅使用:表示该维度不改变</span><br>image2 = image[::-<span class="hljs-number">1</span>, :, :]<br>plt.imshow(image2)<br><span class="hljs-comment"># 水平方向翻转</span><br>image3 = image[:, ::-<span class="hljs-number">1</span>, :]<br>plt.imshow(image3)<br><span class="hljs-comment"># 保存图片</span><br>im3 = Image.fromarray(image3)<br>im3.save(<span class="hljs-string">&#x27;im3.jpg&#x27;</span>)<br><span class="hljs-comment">#  高度方向裁剪</span><br>H, W = image.shape[<span class="hljs-number">0</span>], image.shape[<span class="hljs-number">1</span>]<br><span class="hljs-comment"># 注意此处用整除，H_start必须为整数</span><br>H1 = H // <span class="hljs-number">2</span> <br>H2 = H<br>image4 = image[H1:H2, :, :]<br>plt.imshow(image4)<br><span class="hljs-comment">#  宽度方向裁剪</span><br>W1 = W//<span class="hljs-number">6</span><br>W2 = W//<span class="hljs-number">3</span> * <span class="hljs-number">2</span><br>image5 = image[:, W1:W2, :]<br>plt.imshow(image5)<br><span class="hljs-comment"># 两个方向同时裁剪</span><br>image5 = image[H1:H2, \<br>               W1:W2, :]<br>plt.imshow(image5)<br><span class="hljs-comment"># 调整亮度</span><br>image6 = image * <span class="hljs-number">0.5</span><br>plt.imshow(image6.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>))<br><span class="hljs-comment"># 调整亮度</span><br>image7 = image * <span class="hljs-number">2.0</span><br><span class="hljs-comment"># 由于图片的RGB像素值必须在0-255之间，</span><br><span class="hljs-comment"># 此处使用np.clip进行数值裁剪</span><br>image7 = np.clip(image7, \<br>        a_min=<span class="hljs-literal">None</span>, a_max=<span class="hljs-number">255.</span>)<br>plt.imshow(image7.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>))<br><span class="hljs-comment">#高度方向每隔一行取像素点</span><br>image8 = image[::<span class="hljs-number">2</span>, :, :]<br>plt.imshow(image8)<br><span class="hljs-comment">#宽度方向每隔一列取像素点</span><br>image9 = image[:, ::<span class="hljs-number">2</span>, :]<br>plt.imshow(image9)<br><span class="hljs-comment">#间隔行列采样，图像尺寸会减半，清晰度变差</span><br>image10 = image[::<span class="hljs-number">2</span>, ::<span class="hljs-number">2</span>, :]<br>plt.imshow(image10)<br>image10.shape<br></code></pre></td></tr></table></figure>

<h4 id="9-Paddle-Tensor"><a href="#9-Paddle-Tensor" class="headerlink" title="9. Paddle.Tensor"></a>9. Paddle.Tensor</h4><p>飞桨使用Tensor数据结构来表示数据，在神经网络中传递的数据均为Tensor。Tensor可以将其理解为多维数组，其可以具有任意多的维度，不同Tensor可以有不同的数据类型 (dtype) 和形状 (shape)。同一Tensor的中所有元素的数据类型均相同。如果你对 Numpy 熟悉，Tensor是类似于Numpy数组（array）的概念。</p>
<p>飞桨的Tensor高度兼容Numpy数组（array），在基础数据结构和方法上，增加了很多适用于深度学习任务的参数和方法，如：反向计算梯度，更灵活的指定运行硬件等。</p>
<p>如下述代码声明了两个Tensor类型的向量$x$和$y$，指定CPU为计算运行硬件，要自动反向求导。两个向量除了可以与Numpy类似的做相乘的操作之外，还可以直接获取到每个变量的导数值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle<br>x = paddle.to_tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], dtype=<span class="hljs-string">&#x27;float32&#x27;</span>, place=paddle.CPUPlace(), stop_gradient=<span class="hljs-literal">False</span>)<br>y = paddle.to_tensor([<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>], dtype=<span class="hljs-string">&#x27;float32&#x27;</span>, place=paddle.CPUPlace(), stop_gradient=<span class="hljs-literal">False</span>)<br>z = x * y<br>z.backward()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;tensor&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(x.grad))<br></code></pre></td></tr></table></figure>

<blockquote>
<p>此外，飞桨Tensor还可以与Numpy的数组方便的互转，具体方法如下。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>tensor_to_convert = paddle.to_tensor([<span class="hljs-number">1.</span>,<span class="hljs-number">2.</span>])<br><br><span class="hljs-comment">#通过 Tensor.numpy() 方法，将 Tensor 转化为 Numpy数组</span><br>tensor_to_convert.numpy()<br><br><span class="hljs-comment">#通过paddle.to_tensor() 方法，将 Numpy数组 转化为 Tensor</span><br>tensor_temp = paddle.to_tensor(np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>]))<br></code></pre></td></tr></table></figure>

<h5 id="推荐优先使用Paddle-Tensor的场景"><a href="#推荐优先使用Paddle-Tensor的场景" class="headerlink" title="推荐优先使用Paddle.Tensor的场景"></a>推荐优先使用Paddle.Tensor的场景</h5><p>虽然Paddle的Tensor可以与Numpy的数组方便的互相转换，但在实际中两者频繁转换会性能消耗。飞桨的Tensor支持的操作已经基本覆盖Numpy并有所加强，所以推荐用户在程序中优先使用飞桨的Tensor完成各种数据处理和组网操作。具体分为如下两种场景：</p>
<ul>
<li>场景一：在组网程序中，对网络中向量的处理，务必使用Tensor，而不建议转成Numpy的数组。如果在组网过程中转成Numpy的数组，并使用Numpy的函数会拖慢整体性能；</li>
<li>场景二：在数据处理和模型后处理等场景，建议优先使用Tensor，主要是飞桨为AI硬件做了大量的适配和性能优化工作，部分情况下会获得更好的使用体验和性能。</li>
</ul>
<h3 id="Pandas库"><a href="#Pandas库" class="headerlink" title="Pandas库"></a>Pandas库</h3><p>注：提供高性能易用数据类型和分析工具；</p>
<p>​		pandas基于numpy实现，常与numpy和matplotlib一同使用。</p>
<p><strong>Pandas核心数据结构：</strong></p>
<center>
<img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/a8c80653f39b479dab9f6867a638b64c405e79d6540c4307a22f43c4b0e228bc" srcset="/img/loading.gif" lazyload alt="img" style="zoom: 67%;" />
</center>
<center>
<center>
<img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/c8f06f423acc488fb391bca5dcf8f2b02d7444ef526f41599b6b430ae24659c1" srcset="/img/loading.gif" lazyload alt="img" style="zoom: 50%;" />
</center>

<h4 id="1-Series"><a href="#1-Series" class="headerlink" title="1. Series"></a>1. Series</h4><p>注：Series是一种类似于一维数组的对象，它由一维数组（各种numpy数据类    型）以及一组与之相关的数据标签（即索引）组成.可理解为带标签的一维数组，可存储整数、浮点数、字符串、Python 对象等类型的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>s = pd.Series([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>])<span class="hljs-comment">#将列表转为series对象</span><br><span class="hljs-built_in">print</span>(s)<br></code></pre></td></tr></table></figure>

<p>注：Series中可以使用index设置索引列表，与字典不同的是，Series允许索引重复。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#与字典不同的是：Series允许索引重复</span><br>s = pd.Series([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>],index=[<span class="hljs-number">100</span>,<span class="hljs-number">200</span>,<span class="hljs-number">100</span>,<span class="hljs-number">400</span>,<span class="hljs-number">500</span>])<br><span class="hljs-built_in">print</span>(s)<br>d = &#123;<span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">2</span>&#125;<br>pd.Series(d)<br>d = &#123;<span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">2</span>&#125;<span class="hljs-comment">#将字典实例化</span><br>pd.Series(d)<br><span class="hljs-built_in">print</span>(s.values)<span class="hljs-comment">#取键值</span><br><span class="hljs-built_in">print</span>(s.index)<span class="hljs-comment">#取键</span><br><span class="hljs-comment">#与普通numpy数组相比，可以通过索引的方式选取Series中的单个或一组值</span><br><span class="hljs-built_in">print</span>(s[<span class="hljs-number">100</span>])<br><span class="hljs-built_in">print</span>(s[[<span class="hljs-number">400</span>, <span class="hljs-number">500</span>]])<br><span class="hljs-comment">#对应元素求和</span><br><span class="hljs-built_in">print</span>(s+s)<br><span class="hljs-comment">#对应元素乘</span><br><span class="hljs-built_in">print</span>(s*<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure>

<p>注：Series中最重要的一个功能是它会在算术运算中基于标签自动对齐不同索引的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">obj1 = pd.Series(&#123;<span class="hljs-string">&quot;Ohio&quot;</span>: <span class="hljs-number">35000</span>, <span class="hljs-string">&quot;Oregon&quot;</span>: <span class="hljs-number">16000</span>, <span class="hljs-string">&quot;Texas&quot;</span>: <span class="hljs-number">71000</span>, <span class="hljs-string">&quot;Utah&quot;</span>: <span class="hljs-number">5000</span>&#125;)<br><span class="hljs-built_in">print</span>(obj1)<br>obj2 = pd.Series(&#123;<span class="hljs-string">&quot;California&quot;</span>: np.nan, <span class="hljs-string">&quot;Ohio&quot;</span>: <span class="hljs-number">35000</span>, <span class="hljs-string">&quot;Oregon&quot;</span>: <span class="hljs-number">16000</span>, <span class="hljs-string">&quot;Texas&quot;</span>: <span class="hljs-number">71000</span>&#125;)<br><span class="hljs-built_in">print</span>(obj2)<br><span class="hljs-built_in">print</span>(obj1 + obj2)<br><br><span class="hljs-comment">#可以切片，基础运算时没有匹配的键值会被设为NaN</span><br>s = pd.Series(np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]), index=[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>])<br><br><span class="hljs-built_in">print</span>(s[<span class="hljs-string">&#x27;a&#x27;</span>])<br><span class="hljs-built_in">print</span>(s[<span class="hljs-number">1</span>:])<br><br><span class="hljs-built_in">print</span>(s[:-<span class="hljs-number">1</span>])<br><br><span class="hljs-built_in">print</span>(s[<span class="hljs-number">1</span>:] + s[:-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>

<h4 id="2-DataFrame"><a href="#2-DataFrame" class="headerlink" title="2. DataFrame"></a>2. DataFrame</h4><p>注：</p>
<ul>
<li>DataFrame是一个表格型的数据结构，类似于Excel或sql表</li>
</ul>
<p>它含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔值等）</p>
<p>DataFrame既有行索引也有列索引，它可以被看做由Series组成的字典（共用同一个索引）。</p>
<ul>
<li><strong>用多维数组字典、列表字典生成 DataFrame。</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#如果指定了列顺序，则DataFrame的列就会按照指定顺序进行排列, 跟原Series一样，如果传入的列在数据中找不到，就会产生NAN值.</span><br>data = &#123;<span class="hljs-string">&#x27;state&#x27;</span>: [<span class="hljs-string">&#x27;Ohio&#x27;</span>, <span class="hljs-string">&#x27;Ohio&#x27;</span>, <span class="hljs-string">&#x27;Ohio&#x27;</span>, <span class="hljs-string">&#x27;Nevada&#x27;</span>, <span class="hljs-string">&#x27;Nevada&#x27;</span>], <span class="hljs-string">&#x27;year&#x27;</span>: [<span class="hljs-number">2000</span>, <span class="hljs-number">2001</span>, <span class="hljs-number">2002</span>, <span class="hljs-number">2001</span>, <span class="hljs-number">2002</span>], <span class="hljs-string">&#x27;pop&#x27;</span>: [<span class="hljs-number">1.5</span>, <span class="hljs-number">1.7</span>, <span class="hljs-number">3.6</span>, <span class="hljs-number">2.4</span>, <span class="hljs-number">2.9</span>]&#125;<br>frame = pd.DataFrame(data,columns=[<span class="hljs-string">&#x27;year&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;pop&#x27;</span>, <span class="hljs-string">&#x27;debt&#x27;</span>])<br><span class="hljs-built_in">print</span>(frame)<br></code></pre></td></tr></table></figure>

<img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220910232915712.png" srcset="/img/loading.gif" lazyload alt="image-20220910232915712"  />

<ul>
<li><strong>用 Series 字典或字典生成 DataFrame, 即Series可以作为DataFrame的子集。</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">d = &#123;<span class="hljs-string">&#x27;one&#x27;</span>: pd.Series([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], index=[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>]),<br>     <span class="hljs-string">&#x27;two&#x27;</span>: pd.Series([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>], index=[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>])&#125;<br><span class="hljs-built_in">print</span>(pd.DataFrame(d))<br><br><span class="hljs-comment">#通过类似字典标记的方式或属性的方式，可以将DataFrame的列获取为一个Series,返回的Series拥有原DataFrame相同的索引</span><br><span class="hljs-built_in">print</span>(frame2[<span class="hljs-string">&#x27;state&#x27;</span>])<br><br><span class="hljs-comment">#列可以通过赋值的方式进行修改,例如，给那个空的“delt”列赋上一个标量值或一组值</span><br>frame2[<span class="hljs-string">&#x27;debt&#x27;</span>] = <span class="hljs-number">16.5</span><br><span class="hljs-built_in">print</span>(frame2)<br><br><span class="hljs-comment">#新增一个Series</span><br>frame2[<span class="hljs-string">&#x27;new&#x27;</span>] = frame2[<span class="hljs-string">&#x27;debt&#x27;</span> ]* frame2[<span class="hljs-string">&#x27;pop&#x27;</span>] <br><span class="hljs-built_in">print</span>(frame2)<br><br><span class="hljs-comment">#用numpy数组赋值</span><br>frame2[<span class="hljs-string">&#x27;debt&#x27;</span>] = np.arange(<span class="hljs-number">5.</span>)<br><span class="hljs-built_in">print</span>(frame2)<br></code></pre></td></tr></table></figure>

<h4 id="3-索引对象常用方法"><a href="#3-索引对象常用方法" class="headerlink" title="3. 索引对象常用方法"></a>3. 索引对象常用方法</h4><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193234679.png" srcset="/img/loading.gif" lazyload alt="image-20220911193234679" style="zoom:50%;" />

<h4 id="4-常用方法"><a href="#4-常用方法" class="headerlink" title="4.常用方法"></a>4.常用方法</h4><ul>
<li><p>data.<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=shape&spm=1001.2101.3001.7020">shape</a>返回的是元组（data必须是浮点数类型）</p>
<ul>
<li><p>data.shape[0]是行数</p>
</li>
<li><p>data.shape[1]是列数</p>
</li>
</ul>
</li>
</ul>
<h3 id="PIL库"><a href="#PIL库" class="headerlink" title="PIL库"></a>PIL库</h3><p>注：</p>
<ul>
<li><p>PIL库是一个具有强大图像处理能力的第三方库。</p>
</li>
<li><p>在命令行下的安装方法: pip install pillow。</p>
</li>
<li><p>在使用过程中的弓|入方法: from PIL import Image。</p>
</li>
<li><p>图像的组成：由RGB三原色组成,RGB图像中，一种彩色由R、G、B三原色按照比例混合而成。0-255区分不同亮度的颜色。图像的数组表示：图像是一个由像素组成的矩阵，每个元素是一个RGB值。</p>
</li>
<li><p>Image 是 PIL 库中代表一个图像的类（对象）。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment">#显示matplotlib生成的图形</span><br>%matplotlib inline<br><br><span class="hljs-comment">#读取图片</span><br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/home/aistudio/work/yushuxin.jpg&#x27;</span>) <br><br><span class="hljs-comment">#显示图片</span><br><span class="hljs-comment">#img.show() #自动调用计算机上显示图片的工具</span><br>plt.imshow(img)  <br>plt.show(img)   <br><br><span class="hljs-comment">#获得图像的模式和大小</span><br>img_mode = img.mode<br><span class="hljs-built_in">print</span>(img_mode)<br>width,height = img.size<br><span class="hljs-built_in">print</span>(width,height)<br><br><span class="hljs-comment">#将图片旋转45度</span><br>img_rotate = img.rotate(<span class="hljs-number">45</span>) <br><br><span class="hljs-comment">#左右镜像</span><br>img3_lr = img3.transpose(Image.FLIP_LEFT_RIGHT)<br><span class="hljs-comment">#上下镜像</span><br>img3_bt = img3.transpose(Image.FLIP_TOP_BOTTOM)<br><br><span class="hljs-comment">#缩放</span><br>width,height = img2.size<br>img2_resize_result = img2.resize((<span class="hljs-built_in">int</span>(width*<span class="hljs-number">0.6</span>),<span class="hljs-built_in">int</span>(height*<span class="hljs-number">0.6</span>)),Image.ANTIALIAS)<br><br><span class="hljs-comment">#剪切 crop()四个参数分别是：(左上角点的x坐标，左上角点的y坐标，右下角点的x坐标，右下角点的y坐标)</span><br>img1_crop_result = img1.crop((<span class="hljs-number">126</span>,<span class="hljs-number">0</span>,<span class="hljs-number">381</span>,<span class="hljs-number">300</span>))<br><span class="hljs-comment">#保存图片</span><br>img1_crop_result.save(<span class="hljs-string">&#x27;path&#x27;</span>)<br></code></pre></td></tr></table></figure>

<hr>
<h3 id="Matplotlib库"><a href="#Matplotlib库" class="headerlink" title="Matplotlib库"></a>Matplotlib库</h3><p>注：</p>
<ul>
<li>Matplotlib库由各种可视化类构成，内部结构复杂。</li>
<li>matplotlib.pylot是绘制各类可视化图形的命令字库。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><br><span class="hljs-comment">#显示matplotlib生成的图形</span><br>%matplotlib inline<br><br>x = np.linspace(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">50</span>) <span class="hljs-comment">#等差数列，从-1到1生成50个等间距的数的数组</span><br>y1 = <span class="hljs-number">2</span>*x + <span class="hljs-number">1</span><br>y2 = x**<span class="hljs-number">2</span><br><br><span class="hljs-comment">#传入x,y,通过plot()绘制出折线图 </span><br>plt.figure(figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">5</span>))<span class="hljs-comment">#保持图像</span><br>plt.plot(x,y1,color=<span class="hljs-string">&#x27;red&#x27;</span>,linewidth=<span class="hljs-number">1</span>)<span class="hljs-comment">#连续图</span><br>plt.plot(x,y2,color=<span class="hljs-string">&#x27;blue&#x27;</span>,linewidth=<span class="hljs-number">5</span>)<br>plt.legend(handles=[l1,l2],labels=[<span class="hljs-string">&#x27;aa&#x27;</span>,<span class="hljs-string">&#x27;bb&#x27;</span>],loc=<span class="hljs-string">&#x27;best&#x27;</span>)<span class="hljs-comment">#图例</span><br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">1</span>))  <span class="hljs-comment">#x轴只截取一段进行显示</span><br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">1</span>))  <span class="hljs-comment">#y轴只截取一段进行显示</span><br>plt.show()<span class="hljs-comment">#显示图形</span><br><br><span class="hljs-comment">#绘制离散图</span><br>dots1 =np.random.rand(<span class="hljs-number">50</span>)<br>dots2 =np.random.rand(<span class="hljs-number">50</span>)<br>plt.scatter(dots1,dots2,c=<span class="hljs-string">&#x27;red&#x27;</span>,alpha=<span class="hljs-number">0.5</span>) <span class="hljs-comment">#c表示颜色，alpha表示透明度</span><br>plt.show()<br><br><span class="hljs-comment">#绘制直方图（柱状图）</span><br>x = np.arange(<span class="hljs-number">10</span>)<br>y = <span class="hljs-number">2</span>**x+<span class="hljs-number">10</span><br>plt.bar(x,y,facecolor=<span class="hljs-string">&#x27;#9999ff&#x27;</span>,edgecolor=<span class="hljs-string">&#x27;white&#x27;</span>)<br><span class="hljs-keyword">for</span> ax,ay <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(x,y):<span class="hljs-comment">#顶部居中标注纵坐标</span><br>    plt.text(ax,ay,<span class="hljs-string">&#x27;%.1f&#x27;</span> % ay,ha=<span class="hljs-string">&#x27;center&#x27;</span>,va=<span class="hljs-string">&#x27;bottom&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<h4 id="基础图标函数"><a href="#基础图标函数" class="headerlink" title="基础图标函数"></a>基础图标函数</h4><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193426778.png" srcset="/img/loading.gif" lazyload alt="image-20220911193426778" style="zoom:67%;" />

<p><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193512661.png" srcset="/img/loading.gif" lazyload alt="image-20220911193512661"></p>
<p><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911193512661.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>简介：深层神经网络一般都需要大量的训练数据才能获得比较理想的结果。在数据量有限的情况下，可以通过数据增强( Data Augmentation )来增加训练样本的多样性，提高模型鲁棒性。<br>目的：</p>
<ul>
<li><p>增加数据量</p>
</li>
<li><p>采集更多的图像特征</p>
</li>
<li><p>使网络可见更多的数据变化</p>
</li>
<li><p>提高模型的泛化能力</p>
</li>
</ul>
<p>增强方式：</p>
<img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911194420932.png" srcset="/img/loading.gif" lazyload alt="image-20220911194420932" style="zoom: 80%;" />

<p>下图所示为一些基础的图像增强方法，如果我们发现数据集中的猫均是标准姿势，而真实场景中的猫时常有倾斜身姿的情况，那么在原始图片数据的基础上采用旋转的方法造一批数据加入到数据集会有助于提升模型效果。类似的，如果数据集中均是高清图片，而真实场景中经常有拍照模糊或曝光异常的情况，则采用降采样和调整饱和度的方式造一批数据，有助于提升模型的效果。<br><img src="https://ai-studio-static-online.cdn.bcebos.com/54adf1fa9d7e4dc9a6a23cdf42417fbf624a0b3406564b9990b7852ce8fac9c7" srcset="/img/loading.gif" lazyload><br>基础的图像增强方法</p>
<p>下图展示了一些高阶的图像增强方法，裁剪和拼接分别适合于“数据集中物体完整，但实际场景中物体存在遮挡”，以及“数据集中物体背景单一，而实际场景中物体的背景多变”的两种情况。<br><img src="https://ai-studio-static-online.cdn.bcebos.com/16edd4f2b23d48ae8ba699e4cd00d65a75a43e7a8b7e4f8a98513588031f1e32" srcset="/img/loading.gif" lazyload><br>高阶的数据增强方法</p>
<p>下图展示了专门针对文本识别的数据增强方法TIA（Text Image augmentation），对应到“数据集中字体多是平面，而真实场景中的字体往往会在曲面上扭曲的情况，比如拿着相机对一张凸凹不平摆放的纸面拍摄的文字就会存在这种情况”。<br><img src="https://ai-studio-static-online.cdn.bcebos.com/3dbcf420c25e4d62b21500fa1278bb17e1c219ebcd96457a8fc5cf10e4c4e360" srcset="/img/loading.gif" lazyload><br>TIA（Text Image augmentation）：针对文本识别数据增强方法</p>
<p>下图展示了一种新颖的数据增强技巧，用于很多现实中的文字检测，要面临复杂多样的背景，比如店铺牌匾上的文字，周围的背景可能是非常多样的。将部分文本区域剪辑出来，随机摆放到图片的各种位置来生成新的训练数据。这样的数据会大大提高模型在复杂背景中，检测到文字内容的能力。<br><img src="https://ai-studio-static-online.cdn.bcebos.com/7dd04e92862540b390b11246b2375a828df2237b79a64aed9a698a2272fa7b46" srcset="/img/loading.gif" lazyload><br>CopyPaste：一种新颖的数据增强技巧</p>
<h3 id="1-随机旋转"><a href="#1-随机旋转" class="headerlink" title="1. 随机旋转"></a>1. 随机旋转</h3><p>注：<strong>使用numpy+ PIL库进行图像的随机旋转</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">rotate_image</span>(<span class="hljs-params"> img </span>): <br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    图像增强，增加随机旋转角度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    angle = np.random.randint( -<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)<br>    img = img.rotate(angle)<br>    <span class="hljs-keyword">return</span> img<br><br></code></pre></td></tr></table></figure>

<h3 id="2-随机亮度调整"><a href="#2-随机亮度调整" class="headerlink" title="2. 随机亮度调整"></a>2. 随机亮度调整</h3><p>注：使用numpy+ PIL库进行图像的随机亮度调整</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_brightness</span>(<span class="hljs-params">img</span>): <br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    图像增强，亮度调整</span><br><span class="hljs-string">    :param img:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    prob = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> prob &lt; train_parameters[<span class="hljs-string">&#x27;image_enhance_strategy&#x27;</span>][<span class="hljs-string">&#x27;brightness_prob&#x27;</span>]:<br>        brightness_delta = train_parameters[<span class="hljs-string">&#x27;image_enhance_strategy&#x27;</span>][<span class="hljs-string">&#x27;brightness_delta&#x27;</span>] <br>        delta = np.random.uniform(-brightness_delta, brightness_delta) + <span class="hljs-number">1</span><br>        img = ImageEnhance.Brightness(img).enhance(delta)<br>    <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure>

<h3 id="3-训练过程可视化"><a href="#3-训练过程可视化" class="headerlink" title="3. 训练过程可视化"></a>3. 训练过程可视化</h3><p>注：使用Matplotlib库绘制深度学习训练过程中,随着数据的增加，误差与准确率的变化趋势,从而对模型效果进行评估。观察到模型的误差相对较低,而准确率较高，接下来可以使用该模型进行预测。</p>
<p><img src="https://raw.githubusercontent.com/Alleyf/PictureMap/main/web_icons/image-20220911195508553.png" srcset="/img/loading.gif" lazyload alt="image-20220911195508553"></p>
<h1 id="入门实战深度学习"><a href="#入门实战深度学习" class="headerlink" title="入门实战深度学习"></a>入门实战深度学习</h1><h2 id="1-深度学习模型的基本步骤"><a href="#1-深度学习模型的基本步骤" class="headerlink" title="1.深度学习模型的基本步骤"></a>1.深度学习模型的基本步骤</h2><p><img src="https://ai-studio-static-online.cdn.bcebos.com/9f7cc7174c6f482b9b0d3a1f9bdc1195cf9bf0bc24d140da87aceba2dde4ea5d" srcset="/img/loading.gif" lazyload alt="step"></p>
<h2 id="2-实例一“波士顿房价预测”"><a href="#2-实例一“波士顿房价预测”" class="headerlink" title="2.实例一“波士顿房价预测”"></a>2.实例一“波士顿房价预测”</h2><h3 id="2-1-数据处理"><a href="#2-1-数据处理" class="headerlink" title="2.1 数据处理"></a>2.1 数据处理</h3><p>数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装<code>load data</code>函数。数据预处理后，才能被模型调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入需要用到的package</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> json<br><span class="hljs-comment"># 读入训练数据</span><br>datafile = <span class="hljs-string">&#x27;./work/housing.data&#x27;</span><br>data = np.fromfile(datafile, sep=<span class="hljs-string">&#x27; &#x27;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="2-1-2-数据形状变换"><a href="#2-1-2-数据形状变换" class="headerlink" title="2.1.2 数据形状变换"></a>2.1.2 数据形状变换</h4><p>由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个XX<em>X</em>（影响房价的特征）和一个YY<em>Y</em>（该类型房屋的均价）.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... </span><br><span class="hljs-comment"># 这里对原始数据做reshape，变成N x 14的形式</span><br>feature_names = [ <span class="hljs-string">&#x27;CRIM&#x27;</span>, <span class="hljs-string">&#x27;ZN&#x27;</span>, <span class="hljs-string">&#x27;INDUS&#x27;</span>, <span class="hljs-string">&#x27;CHAS&#x27;</span>, <span class="hljs-string">&#x27;NOX&#x27;</span>, <span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;AGE&#x27;</span>,<span class="hljs-string">&#x27;DIS&#x27;</span>, <br>                 <span class="hljs-string">&#x27;RAD&#x27;</span>, <span class="hljs-string">&#x27;TAX&#x27;</span>, <span class="hljs-string">&#x27;PTRATIO&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>, <span class="hljs-string">&#x27;MEDV&#x27;</span> ]<br>feature_num = <span class="hljs-built_in">len</span>(feature_names)<br>data = data.reshape([data.shape[<span class="hljs-number">0</span>] // feature_num, feature_num])<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-comment"># 查看数据</span><br>x = data[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(x.shape)<br><span class="hljs-built_in">print</span>(x)<br></code></pre></td></tr></table></figure>

<h4 id="2-1-3-数据集划分"><a href="#2-1-3-数据集划分" class="headerlink" title="2.1.3 数据集划分"></a>2.1.3 数据集划分</h4><p>将数据集划分成<strong>训练集和测试集</strong>，其中训练集用于确定模型的参数，测试集用于评判模型的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#将80%的数据用作训练集，20%用作测试集</span><br>ratio = <span class="hljs-number">0.8</span><br>offset = <span class="hljs-built_in">int</span>(data.shape[<span class="hljs-number">0</span>] * ratio)<br>training_data = data[:offset]<br>training_data.shape<br></code></pre></td></tr></table></figure>

<h4 id="2-1-4-数据归一化处理"><a href="#2-1-4-数据归一化处理" class="headerlink" title="2.1.4 数据归一化处理"></a>2.1.4 数据归一化处理</h4><p>对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效，在本节的后半部分会详细说明；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算train数据集的最大值，最小值，平均值</span><br>maximums, minimums, avgs = \<br>                     training_data.<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">0</span>), \<span class="hljs-comment">#按行求各行最大值</span><br>                     training_data.<span class="hljs-built_in">min</span>(axis=<span class="hljs-number">0</span>), \<br>     training_data.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>) / training_data.shape[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># 对数据进行归一化处理</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(feature_num):<br>    <span class="hljs-comment">#print(maximums[i], minimums[i], avgs[i])</span><br>    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])<br></code></pre></td></tr></table></figure>

<h4 id="2-1-5-封装成load-data函数"><a href="#2-1-5-封装成load-data函数" class="headerlink" title="2.1.5 封装成load data函数"></a>2.1.5 封装成load data函数</h4><p>将上述几个数据处理操作封装成<code>load data</code>函数，以便下一步模型的调用，实现方法如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>    <span class="hljs-comment"># 从文件导入数据</span><br>    datafile = <span class="hljs-string">&#x27;./work/housing.data&#x27;</span><br>    data = np.fromfile(datafile, sep=<span class="hljs-string">&#x27; &#x27;</span>)<br><br>    <span class="hljs-comment"># 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数</span><br>    feature_names = [ <span class="hljs-string">&#x27;CRIM&#x27;</span>, <span class="hljs-string">&#x27;ZN&#x27;</span>, <span class="hljs-string">&#x27;INDUS&#x27;</span>, <span class="hljs-string">&#x27;CHAS&#x27;</span>, <span class="hljs-string">&#x27;NOX&#x27;</span>, <span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;AGE&#x27;</span>, \<br>                      <span class="hljs-string">&#x27;DIS&#x27;</span>, <span class="hljs-string">&#x27;RAD&#x27;</span>, <span class="hljs-string">&#x27;TAX&#x27;</span>, <span class="hljs-string">&#x27;PTRATIO&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>, <span class="hljs-string">&#x27;MEDV&#x27;</span> ]<br>    feature_num = <span class="hljs-built_in">len</span>(feature_names)<br><br>    <span class="hljs-comment"># 将原始数据进行Reshape，变成[N, 14]这样的形状</span><br>    data = data.reshape([data.shape[<span class="hljs-number">0</span>] // feature_num, feature_num])<br><br>    <span class="hljs-comment"># 将原数据集拆分成训练集和测试集</span><br>    <span class="hljs-comment"># 这里使用80%的数据做训练，20%的数据做测试</span><br>    <span class="hljs-comment"># 测试集和训练集必须是没有交集的</span><br>    ratio = <span class="hljs-number">0.8</span><br>    offset = <span class="hljs-built_in">int</span>(data.shape[<span class="hljs-number">0</span>] * ratio)<br>    training_data = data[:offset]<br><br>    <span class="hljs-comment"># 计算训练集的最大值，最小值，平均值</span><br>    maximums, minimums, avgs = training_data.<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">0</span>), training_data.<span class="hljs-built_in">min</span>(axis=<span class="hljs-number">0</span>), \<br>                                 training_data.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>) / training_data.shape[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-comment"># 对数据进行归一化处理</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(feature_num):<br>        <span class="hljs-comment">#print(maximums[i], minimums[i], avgs[i])</span><br>        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])<br><br>    <span class="hljs-comment"># 训练集和测试集的划分比例</span><br>    training_data = data[:offset]<br>    test_data = data[offset:]<br><br>    <span class="hljs-keyword">return</span> training_data, test_data<br><span class="hljs-comment"># 获取数据</span><br>training_data, test_data = load_data()<br>x = training_data[:, :-<span class="hljs-number">1</span>]<br>y = training_data[:, -<span class="hljs-number">1</span>:]<br><span class="hljs-comment"># 查看第一个样本的数据</span><br><span class="hljs-built_in">print</span>(x[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(y[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure>

<h3 id="2-2-模型设计"><a href="#2-2-模型设计" class="headerlink" title="2.2 模型设计"></a>2.2 模型设计</h3><p>模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。</p>
<p>如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个分量，$y$有1个分量，那么参数权重的形状（shape）是$13 \times 1$。假设我们以如下任意数字赋值参数做初始化：</p>
<p>$w&#x3D;[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,−0.1,−0.2,−0.3,−0.4,0.0]$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, -<span class="hljs-number">0.1</span>, -<span class="hljs-number">0.2</span>, -<span class="hljs-number">0.3</span>, -<span class="hljs-number">0.4</span>, <span class="hljs-number">0.0</span>]<br>w = np.array(w).reshape([<span class="hljs-number">13</span>, <span class="hljs-number">1</span>])<span class="hljs-comment">#参数转为列向量进行矩阵乘法</span><br></code></pre></td></tr></table></figure>

<p>取出第1条样本数据，观察样本的特征向量与参数向量相乘的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x1=x[<span class="hljs-number">0</span>]<br>t = np.dot(x1, w)<br><span class="hljs-built_in">print</span>(t)<br></code></pre></td></tr></table></figure>

<p>完整的线性回归公式，还需要初始化偏移量$b$，同样随意赋初值-0.2。那么，线性回归模型的完整输出是$z&#x3D;t+b$，这个从特征和参数计算输出值的过程称为“前向计算”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b = -<span class="hljs-number">0.2</span><br>z = t + b<br><span class="hljs-built_in">print</span>(z)<br></code></pre></td></tr></table></figure>

<p>将上述计算预测输出的过程以“类和对象”的方式来描述，类成员变量有参数$w$和$b$。通过写一个<code>forward</code>函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程，代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_of_weights</span>):<br>        <span class="hljs-comment"># 随机产生w的初始值</span><br>        <span class="hljs-comment"># 为了保持程序每次运行结果的一致性，</span><br>        <span class="hljs-comment"># 此处设置固定的随机数种子</span><br>        np.random.seed(<span class="hljs-number">0</span>)<br>        self.w = np.random.randn(num_of_weights, <span class="hljs-number">1</span>)<br>        self.b = <span class="hljs-number">0.</span><span class="hljs-comment">#初始偏移量为0.0</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        z = np.dot(x, self.w) + self.b<br>        <span class="hljs-keyword">return</span> z<br></code></pre></td></tr></table></figure>

<p>基于Network类的定义，模型的计算过程如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">net = Network(<span class="hljs-number">13</span>)<br>x1 = x[<span class="hljs-number">0</span>]<br>y1 = y[<span class="hljs-number">0</span>]<br>z = net.forward(x1)<br><span class="hljs-built_in">print</span>(z)<br></code></pre></td></tr></table></figure>

<p>从上述前向计算的过程可见，线性回归也可以表示成一种简单的神经网络（只有一个神经元，且激活函数为恒等式）。这也是机器学习模型普遍为深度学习模型替代的原因：由于深度学习网络强大的表示能力，很多传统机器学习模型的学习能力等同于相对简单的深度学习模型。</p>
<h3 id="2-3-训练配置"><a href="#2-3-训练配置" class="headerlink" title="2.3 训练配置"></a>2.3 训练配置</h3><p>模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。</p>
<p>通过模型计算$x_{1}$表示的影响因素所对应的房价应该是$z$ 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：</p>
<p>$$<br>Loss &#x3D; (y - z)^2<br>$$<br>上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型好坏的指标。读者可能会奇怪：如果要衡量预测房价和真实房价之间的差距，是否将每一个样本的差距的绝对值加和即可？差距绝对值加和是更加直观和朴素的思路，为何要平方加和？ 损失函数的设计不仅要考虑准确衡量问题的“合理性”，通常还要考虑“易于优化求解”。至于这个问题的答案，在介绍完优化算法后再揭示。</p>
<p>在回归问题中，<strong>均方误差</strong>是一种比较常见的形式，分类问题中通常会采用<strong>交叉熵</strong>作为损失函数，在后续的章节中会更详细的介绍。对一个样本计算损失函数值的实现如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Loss = (y1 - z)*(y1 - z)<br><span class="hljs-built_in">print</span>(Loss)<br></code></pre></td></tr></table></figure>

<p>因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。<br>$$<br>L&#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(y_i - z_i)^2}<br>$$</p>
<p>在Network类下面添加损失函数的计算过程如下。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_of_weights</span>):<br>        <span class="hljs-comment"># 随机产生w的初始值</span><br>        <span class="hljs-comment"># 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子</span><br>        np.random.seed(<span class="hljs-number">0</span>)<br>        self.w = np.random.randn(num_of_weights, <span class="hljs-number">1</span>)<br>        self.b = <span class="hljs-number">0.</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        z = np.dot(x, self.w) + self.b<br>        <span class="hljs-keyword">return</span> z<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, z, y</span>):<br>        error = z - y<br>        cost = error * error<br>        cost = np.mean(cost)<br>        <span class="hljs-keyword">return</span> cost<br></code></pre></td></tr></table></figure>

<p>使用定义的Network类，可以方便的计算预测值和损失函数。需要注意的是，类中的变量$x$, $w$，$b$, $z$, $error$等均是向量。以变量$x$为例，共有两个维度，一个代表特征数量（值为13），一个代表样本数量，代码如下所示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs py">net = Network(<span class="hljs-number">13</span>)<br><span class="hljs-comment"># 此处可以一次性计算多个样本的预测值和损失函数</span><br>x1 = x[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>]<br>y1 = y[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;actuality: &#x27;</span>,y1)<br>z = net.forward(x1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;predict: &#x27;</span>, z)<br>loss = net.loss(z, y1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss:&#x27;</span>, loss)<br></code></pre></td></tr></table></figure>

<h3 id="2-4-训练过程"><a href="#2-4-训练过程" class="headerlink" title="2.4 训练过程"></a>2.4 训练过程</h3><p>上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数$Loss$尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。</p>
<p>我们先做一个小测试：如 <strong>图5</strong> 所示，基于微积分知识，求一条曲线在某个点的斜率等于函数在该点的导数值。那么大家思考下，当处于曲线的极值点时，该点的斜率是多少？</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/94f0437e6a454a0682f3b831c96a62bdaf40898af25145ec9b5b50bc80391f5c" srcset="/img/loading.gif" lazyload width="300" hegiht="" ></center>
<center><br>图5：曲线斜率等于导数值</br></center>
这个问题并不难回答，处于曲线极值点时的斜率为0，即函数在极值点的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：
$$
\frac{\partial{L}}{\partial{\boldsymbol{w}}}=0
$$

<p>$$<br>\frac{\partial{L}}{\partial{b}}&#x3D;0<br>$$</p>
<p>其中$L$表示的是损失函数的值，$\boldsymbol{w}$为模型权重，$b$为偏置项。$\boldsymbol{w}$和$b$均为要学习的模型参数。</p>
<p>把损失函数表示成矩阵的形式为</p>
<p>$$<br>L&#x3D;\frac{1}{N}||\boldsymbol{y}-(\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b})||^2<br>$$</p>
<p>($||$为范数，表示向量之间的距离，**$||x||_p &#x3D; (|x_1|^{p} + |x_2|^{p} + ··· + |x_n|^{p}) ^\frac{1}{p}$**)</p>
<p>其中$\boldsymbol{y}$为$N$个样本的标签值构成的向量，形状为$N\times 1$；$\boldsymbol{X}$为$N$个样本特征向量构成的矩阵，形状为$N\times D$，$D$为数据特征长度；$\boldsymbol{w}$为权重向量，形状为$D\times 1$；$\boldsymbol{b}$为所有元素都为$b$的向量，形状为$N\times 1$。</p>
<p>计算公式7对参数$b$的偏导数<br>$$<br>\frac{\partial L}{\partial b} &#x3D; \boldsymbol{1}^T(\boldsymbol{y}-(\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}))<br>$$<br>请注意，上述公式忽略了系数$\frac{2}{N}$，并不影响最后结果。其中$\boldsymbol{1}$为$N$维的全1向量。</p>
<p>令公式8等于0，得到<br>$$<br>b^* &#x3D; \boldsymbol{\bar{x}}^T\boldsymbol{w}-\bar{y}<br>$$<br>其中$\bar{y}&#x3D;\frac{1}{N}\boldsymbol{1}^T\boldsymbol{y}$为所有标签的平均值，$\boldsymbol{\bar{x}}&#x3D;\frac{1}{N}(\boldsymbol{1}^T\boldsymbol{X})^T$为所有特征向量的平均值。将$b^*$带入公式7中并对参数$\boldsymbol{w}$求偏导得到</p>
<p>$$<br>\frac{\partial L}{\partial \boldsymbol{w}} &#x3D; (\boldsymbol{X}-\boldsymbol{\bar{x}}^T)^T((\boldsymbol{y}-\bar{y})-(\boldsymbol{X}-\boldsymbol{\bar{x}}^T)\boldsymbol{w})<br>$$</p>
<p>令公式10等于0，得到最优参数</p>
<p>$$<br>\boldsymbol{w}^*&#x3D;((\boldsymbol{X}-\boldsymbol{\bar{x}}^T)^T(\boldsymbol{X}-\boldsymbol{\bar{x}}^T))^{-1}(\boldsymbol{X}-\boldsymbol{\bar{x}}^T)^T(\boldsymbol{y}-\bar{y}) \<br>b^* &#x3D; \boldsymbol{\bar{x}}^T\boldsymbol{w}^*-\bar{y}<br>$$</p>
<p>将样本数据$(x, y)$带入上面的公式11和公式12中即可求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的任务有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单的形式，则很难通过上式求解。为了解决这个问题，下面我们将引入更加普适的数值求解方法：梯度下降法。</p>
<h4 id="2-4-1-梯度下降法"><a href="#2-4-1-梯度下降法" class="headerlink" title="2.4.1 梯度下降法"></a>2.4.1 梯度下降法</h4><p>在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知$x$，求$y$很容易)，但是即使获取到密码锁系统，也无法破解出正确得密钥（已知$y$，求$x$很难）。</p>
<p>这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出$Loss$导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这种方法笔者称它为“盲人下坡法”。哦不，有个更正式的说法“梯度下降法”。</p>
<p>训练的关键是找到一组$(w, b)$，使得损失函数$L$取极小值。我们先看一下损失函数$L$只随两个参数$w_5$、$w_9$变化时的简单情形，启发下寻解的思路。<br>$$L&#x3D;L(w_5, w_9) (公式13)$$<br>这里将$w_0, w_1, …, w_{12}$中除$w_5, w_9$之外的参数和$b$都固定下来，可以用图画出$L(w_5, w_9)$的形式，并在三维空间中画出损失函数随参数变化的曲面图。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs py">net = Network(<span class="hljs-number">13</span>)<br>losses = []<br><span class="hljs-comment">#只画出参数w5和w9在区间[-160, 160]的曲线部分，以及包含损失函数的极值</span><br>w5 = np.arange(-<span class="hljs-number">160.0</span>, <span class="hljs-number">160.0</span>, <span class="hljs-number">1.0</span>)<br>w9 = np.arange(-<span class="hljs-number">160.0</span>, <span class="hljs-number">160.0</span>, <span class="hljs-number">1.0</span>)<br>losses = np.zeros([<span class="hljs-built_in">len</span>(w5), <span class="hljs-built_in">len</span>(w9)])<br><br><span class="hljs-comment">#计算设定区域内每个参数取值所对应的Loss</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(w5)):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(w9)):<br>        net.w[<span class="hljs-number">5</span>] = w5[i]<br>        net.w[<span class="hljs-number">9</span>] = w9[j]<br>        z = net.forward(x)<br>        loss = net.loss(z, y)<br>        losses[i, j] = loss<br><br><span class="hljs-comment">#使用matplotlib将两个变量和对应的Loss作3D图</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D<br>fig = plt.figure()<br>ax = Axes3D(fig)<br><br>w5, w9 = np.meshgrid(w5, w9)<br><br>ax.plot_surface(w5, w9, losses, rstride=<span class="hljs-number">1</span>, cstride=<span class="hljs-number">1</span>, cmap=<span class="hljs-string">&#x27;rainbow&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p>从图中可以明显观察到有些区域的函数值比周围的点小。需要说明的是：为什么选择$w_5$和$w_9$来画图呢？这是因为选择这两个参数的时候，可比较直观的从损失函数的曲面图上发现极值点的存在。其他参数组合，从图形上观测损失函数的极值点不够直观。</p>
<p>观察上述曲线呈现出“圆滑”的坡度，这正是我们选择以均方误差作为损失函数的原因之一。<strong>图6</strong> 呈现了只有一个参数维度时，均方误差和绝对值误差（只将每个样本的误差累加，不做平方处理）的损失函数曲线图。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/99487dca6520441db5073d1c154b5d2fb1174b5cf4d946c29f9d80a209bc2687" srcset="/img/loading.gif" lazyload width="700" hegiht="40" ></center>
<center><br>图6：均方误差和绝对值误差损失函数曲线图</br></center>
由此可见，均方误差表现的“圆滑”的坡度有两个好处：

<ul>
<li>曲线的最低点是可导的。</li>
<li>越接近最低点，曲线的坡度逐渐放缓，有助于通过当前的梯度来判断接近最低点的程度（是否逐渐减少步长，以免错过最低点）。</li>
</ul>
<p>而绝对值误差是不具备这两个特性的，这也是损失函数的设计不仅仅要考虑“合理性”，还要追求“易解性”的原因。</p>
<p>现在我们要找出一组$[w_5, w_9]$的值，使得损失函数最小，实现梯度下降法的方案如下：</p>
<ul>
<li>步骤1：随机的选一组初始值，例如：$[w_5, w_9] &#x3D; [-100.0, -100.0]$</li>
<li>步骤2：选取下一个点$[w_5^{‘} , w_9^{‘}]$，使得$L(w_5^{‘} , w_9^{‘}) &lt; L(w_5, w_9)$</li>
<li>步骤3：重复步骤2，直到损失函数几乎不再下降。</li>
</ul>
<p>如何选择$[w_5^{‘} , w_9^{‘}]$是至关重要的，第一要保证$L$是下降的，第二要使得下降的趋势尽可能的快。微积分的基础知识告诉我们：沿着梯度的反方向，是函数值下降最快的方向，如 <strong>图7</strong> 所示。简单理解，函数在某一个点的梯度方向是曲线斜率最大的方向，但梯度方向是向上的，所以下降最快的是梯度的反方向。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/5f8322f6172542dab0f78684b70efe45d819895332af4cabb7c536217ab0bb26" srcset="/img/loading.gif" lazyload width="400" hegiht="40" ></center>
<center><br>图7：梯度下降方向示意图</br></center>
#### 2.4.2 梯度计算

<p>上文已经介绍了损失函数的计算方法，这里稍微改写。为了使梯度计算更加简洁，引入因子$\frac{1}{2}$，定义损失函数如下：</p>
<p>$$<br>L&#x3D; \frac{1}{2N}\sum_{i&#x3D;1}^N{(y_i - z_i)^2}<br>$$<br>其中$z_i$是网络对第$i$个样本的预测值：</p>
<p>$$<br>z_i &#x3D; \sum_{j&#x3D;0}^{12}{x_i^{j}\cdot w_j} + b<br>$$<br>梯度的定义：</p>
<p>$$<br>𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 &#x3D; (\frac{\partial{L}}{\partial{w_0}},\frac{\partial{L}}{\partial{w_1}}, … ,\frac{\partial{L}}{\partial{w_{12}}} ,\frac{\partial{L}}{\partial{b}})<br>$$<br>可以计算出$L$对$w$和$b$的偏导数：</p>
<p>$$<br>\frac{\partial{L}}{\partial{w_j}} &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{w_j}}} &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(z_i - y_i)x_i^{j}}<br>$$</p>
<p>$$<br>\frac{\partial{L}}{\partial{b}} &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{b}}} &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(z_i - y_i)}<br>$$</p>
<p>从导数的计算过程可以看出，因子$\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因。</p>
<p>下面我们考虑只有一个样本的情况下，计算梯度：</p>
<p>$$<br>L&#x3D; \frac{1}{2}{(y_i - z_i)^2}<br>$$</p>
<p>$$<br>z_1 &#x3D; {x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + …  + {x_1^{12}\cdot w_{12}} + b<br>$$</p>
<p>可以计算出：</p>
<p>$$<br>L&#x3D; \frac{1}{2}{({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + …  + {x_1^{12}\cdot w_{12}} + b - y_1)^2}<br>$$<br>可以计算出$L$对$w$和$b$的偏导数：</p>
<p>$$<br>\frac{\partial{L}}{\partial{w_0}} &#x3D; ({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + …  + {x_1^{12}\cdot w_12} + b - y_1)\cdot x_1^{0}&#x3D;({z_1} - {y_1})\cdot x_1^{0}<br>$$</p>
<p>$$<br>\frac{\partial{L}}{\partial{b}} &#x3D; ({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + …  + {x_1^{12}\cdot w_{12}} + b - y_1)\cdot 1 &#x3D; ({z_1} - {y_1}) (公式23)<br>$$</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#查看每个变量的数据和维度</span><br>x1 = x[<span class="hljs-number">0</span>]<br>y1 = y[<span class="hljs-number">0</span>]<br>z1 = net.forward(x1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;x1 &#123;&#125;, shape &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(x1, x1.shape))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;y1 &#123;&#125;, shape &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(y1, y1.shape))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;z1 &#123;&#125;, shape &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(z1, z1.shape))<br><span class="hljs-comment">#当只有一个样本时，可以计算某个wj，比如w0的梯度</span><br>gradient_w0 = (z1 - y1) * x1[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient_w0 &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(gradient_w0))<br></code></pre></td></tr></table></figure>

<h4 id="2-4-3-使用NumPy进行梯度计算"><a href="#2-4-3-使用NumPy进行梯度计算" class="headerlink" title="2.4.3 使用NumPy进行梯度计算"></a>2.4.3 使用NumPy进行梯度计算</h4><p>基于NumPy广播机制（对向量和矩阵计算如同对1个单一变量计算一样），可以更快速的实现梯度计算。计算梯度的代码中直接用$(z_1 - y_1) \cdot x_1$，得到的是一个13维的向量，每个分量分别代表该维度的梯度。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#计算一个样本的所有梯度</span><br>gradient_w = (z1 - y1) * x1<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient_w_by_sample1 &#123;&#125;, gradient.shape &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(gradient_w, gradient_w.shape))<br></code></pre></td></tr></table></figure>

<p>此处可见，计算梯度<code>gradient_w</code>的维度是$3 \times 13$，并且其第1行与上面第1个样本计算的梯度gradient_w_by_sample1一致，第2行与上面第2个样本计算的梯度gradient_w_by_sample2一致，第3行与上面第3个样本计算的梯度gradient_w_by_sample3一致。这里使用矩阵操作，可以更加方便的对3个样本分别计算各自对梯度的贡献。</p>
<p>那么对于有N个样本的情形，我们可以直接使用如下方式计算出所有样本对梯度的贡献，这就是使用NumPy库广播功能带来的便捷。<br>小结一下这里使用NumPy库的广播功能：</p>
<ul>
<li>一方面可以扩展参数的维度，代替for循环来计算1个样本对从$w_0$到$w_{12}$的所有参数的梯度。<code>列表征特征维度</code></li>
<li>另一方面可以扩展样本的维度，代替for循环来计算样本0到样本403对参数的梯度。<code>行表征样本维度</code></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs py">z = net.forward(x)<br>gradient_w = (z - y) * x<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient_w shape &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(gradient_w.shape))<br><span class="hljs-built_in">print</span>(gradient_w)<br></code></pre></td></tr></table></figure>

<p>上面gradient_w的每一行代表了一个样本对梯度的贡献。根据梯度的计算公式，总梯度是对每个样本对梯度贡献的平均值。</p>
<p>$$<br>\frac{\partial{L}}{\partial{w_j}} &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{w_j}}} &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N{(z_i - y_i)x_i^{j}}<br>$$<br>可以使用NumPy的均值函数来完成此过程，代码实现如下。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># axis = 0 表示把每一列做相加然后再除以总的行数</span><br>gradient_w = np.mean(gradient_w, axis=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient_w &#x27;</span>, gradient_w.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w &#x27;</span>, net.w.shape)<br><span class="hljs-built_in">print</span>(gradient_w)<br><span class="hljs-built_in">print</span>(net.w)<br></code></pre></td></tr></table></figure>

<p>使用NumPy的矩阵操作方便地完成了gradient的计算，但引入了一个问题，<code>gradient_w</code>的形状是(13,)，而$w$的维度是(13, 1)。导致该问题的原因是使用<code>np.mean</code>函数时消除了第0维。为了加减乘除等计算方便，<code>gradient_w</code>和$w$必须保持一致的形状。因此我们将<code>gradient_w</code>的维度也设置为(13,1)，代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">gradient_w = gradient_w[:, np.newaxis]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient_w shape&#x27;</span>, gradient_w.shape)<br><span class="hljs-built_in">print</span>(gradient_w)<br></code></pre></td></tr></table></figure>

<p>梯度计算综合代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py">z = net.forward(x)<br>gradient_w = (z - y) * x<br>gradient_w = np.mean(gradient_w, axis=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># gradient_w = gradient_w.reshape(13,1)</span><br>gradient_w = gradient_w[:, np.newaxis]<br>gradient_w<br></code></pre></td></tr></table></figure>

<p>偏置计算综合代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs py">gradient_b = (z - y)<br>gradient_b = np.mean(gradient_b)<br><span class="hljs-comment"># 此处b是一个数值，所以可以直接用np.mean得到一个标量</span><br>gradient_b<br></code></pre></td></tr></table></figure>

<p>总结为OOP的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_of_weights</span>):<br>        <span class="hljs-comment"># 随机产生w的初始值</span><br>        <span class="hljs-comment"># 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子</span><br>        np.random.seed(<span class="hljs-number">0</span>)<br>        self.w = np.random.randn(num_of_weights, <span class="hljs-number">1</span>)<br>        self.b = <span class="hljs-number">0.</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        z = np.dot(x, self.w) + self.b<br>        <span class="hljs-keyword">return</span> z<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, z, y</span>):<br>        error = z - y<br>        num_samples = error.shape[<span class="hljs-number">0</span>]<br>        cost = error * error<br>        cost = np.<span class="hljs-built_in">sum</span>(cost) / num_samples<br>        <span class="hljs-keyword">return</span> cost<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">self, x, y</span>):<br>        z = self.forward(x)<br>        gradient_w = (z-y)*x<br>        gradient_w = np.mean(gradient_w, axis=<span class="hljs-number">0</span>)<br>        gradient_w = gradient_w[:, np.newaxis]<br>        gradient_b = (z - y)<br>        gradient_b = np.mean(gradient_b)<br>        <br>        <span class="hljs-keyword">return</span> gradient_w, gradient_b<br><span class="hljs-comment">#主函数调用上述方法计算梯度</span><br><span class="hljs-comment"># 调用上面定义的gradient函数，计算梯度</span><br><span class="hljs-comment"># 初始化网络</span><br>net = Network(<span class="hljs-number">13</span>)<br><span class="hljs-comment"># 设置[w5, w9] = [-100., -100.]</span><br>net.w[<span class="hljs-number">5</span>] = -<span class="hljs-number">100.0</span><br>net.w[<span class="hljs-number">9</span>] = -<span class="hljs-number">100.0</span><br><br>z = net.forward(x)<br>loss = net.loss(z, y)<br>gradient_w, gradient_b = net.gradient(x, y)<br>gradient_w5 = gradient_w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>]<br>gradient_w9 = gradient_w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;point &#123;&#125;, loss &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>([net.w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>], net.w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]], loss))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>([gradient_w5, gradient_w9]))<br><br></code></pre></td></tr></table></figure>

<h4 id="2-4-4-梯度更新"><a href="#2-4-4-梯度更新" class="headerlink" title="2.4.4 梯度更新"></a>2.4.4 梯度更新</h4><p>下面研究更新梯度的方法，确定损失函数更小的点。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1</span><br><span class="hljs-comment"># 定义移动步长 eta</span><br>eta = <span class="hljs-number">0.1</span><br><span class="hljs-comment"># 更新参数w5和w9</span><br>net.w[<span class="hljs-number">5</span>] = net.w[<span class="hljs-number">5</span>] - eta * gradient_w5<br>net.w[<span class="hljs-number">9</span>] = net.w[<span class="hljs-number">9</span>] - eta * gradient_w9<br><span class="hljs-comment"># 重新计算z和loss</span><br>z = net.forward(x)<br>loss = net.loss(z, y)<br>gradient_w, gradient_b = net.gradient(x, y)<br>gradient_w5 = gradient_w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>]<br>gradient_w9 = gradient_w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;point &#123;&#125;, loss &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>([net.w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>], net.w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]], loss))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gradient &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>([gradient_w5, gradient_w9]))<br></code></pre></td></tr></table></figure>

<p>运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。感兴趣的话，大家可以尝试不停的点击上面的代码块，观察损失函数是否一直在变小。</p>
<p>在上述代码中，每次更新参数使用的语句：<br><code>net.w[5] = net.w[5] - eta * gradient_w5</code></p>
<ul>
<li>相减：参数需要向梯度的反方向移动。</li>
<li>eta：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长，又称为学习率。</li>
</ul>
<p>大家可以思考下，为什么之前我们要做输入特征的归一化，保持尺度一致？这是为了让统一的步长更加合适，使训练更加高效。</p>
<p>如 <strong>图8</strong> 所示，特征输入归一化后，不同参数输出的Loss是一个比较规整的曲线，学习率可以设置成统一的值 ；特征输入未归一化时，不同特征对应的参数所需的步长不一致，尺度较大的参数需要大步长，尺寸较小的参数需要小步长，导致无法设置统一的学习率。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/903f552bc55b4a5eba71caa7dd86fd2d7b71b8ebb6cb4500a5f5711f465707f3" srcset="/img/loading.gif" lazyload width="300" hegiht="40" ></center>
<center><br>图8：未归一化的特征，会导致不同特征维度的理想步长不同</br></center>
#### 2.4.5 封装Train函数

<p>将上面的循环计算过程封装在<code>train</code>和<code>update</code>函数中，实现方法如下所示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_of_weights</span>):<br>        <span class="hljs-comment"># 随机产生w的初始值</span><br>        <span class="hljs-comment"># 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子</span><br>        np.random.seed(<span class="hljs-number">0</span>)<br>        self.w = np.random.randn(num_of_weights,<span class="hljs-number">1</span>)<br>        self.w[<span class="hljs-number">5</span>] = -<span class="hljs-number">100.</span><br>        self.w[<span class="hljs-number">9</span>] = -<span class="hljs-number">100.</span><br>        self.b = <span class="hljs-number">0.</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        z = np.dot(x, self.w) + self.b<br>        <span class="hljs-keyword">return</span> z<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, z, y</span>):<br>        error = z - y<br>        num_samples = error.shape[<span class="hljs-number">0</span>]<br>        cost = error * error<br>        cost = np.<span class="hljs-built_in">sum</span>(cost) / num_samples<br>        <span class="hljs-keyword">return</span> cost<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">self, x, y</span>):<br>        z = self.forward(x)<br>        gradient_w = (z-y)*x<br>        gradient_w = np.mean(gradient_w, axis=<span class="hljs-number">0</span>)<br>        gradient_w = gradient_w[:, np.newaxis]<br>        gradient_b = (z - y)<br>        gradient_b = np.mean(gradient_b)        <br>        <span class="hljs-keyword">return</span> gradient_w, gradient_b<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, gradient_w5, gradient_w9, eta=<span class="hljs-number">0.01</span></span>):<br>        net.w[<span class="hljs-number">5</span>] = net.w[<span class="hljs-number">5</span>] - eta * gradient_w5<br>        net.w[<span class="hljs-number">9</span>] = net.w[<span class="hljs-number">9</span>] - eta * gradient_w9<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, x, y, iterations=<span class="hljs-number">100</span>, eta=<span class="hljs-number">0.01</span></span>):<br>        points = []<br>        losses = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iterations):<br>            points.append([net.w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>], net.w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]])<br>            z = self.forward(x)<br>            L = self.loss(z, y)<br>            gradient_w, gradient_b = self.gradient(x, y)<br>            gradient_w5 = gradient_w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>]<br>            gradient_w9 = gradient_w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]<br>            self.update(gradient_w5, gradient_w9, eta)<br>            losses.append(L)<br>            <span class="hljs-keyword">if</span> i % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iter &#123;&#125;, point &#123;&#125;, loss &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i, [net.w[<span class="hljs-number">5</span>][<span class="hljs-number">0</span>], net.w[<span class="hljs-number">9</span>][<span class="hljs-number">0</span>]], L))<br>        <span class="hljs-keyword">return</span> points, losses<br><br><span class="hljs-comment"># 获取数据</span><br>train_data, test_data = load_data()<br>x = train_data[:, :-<span class="hljs-number">1</span>]<br>y = train_data[:, -<span class="hljs-number">1</span>:]<br><span class="hljs-comment"># 创建网络</span><br>net = Network(<span class="hljs-number">13</span>)<br>num_iterations=<span class="hljs-number">2000</span><br><span class="hljs-comment"># 启动训练</span><br>points, losses = net.train(x, y, iterations=num_iterations, eta=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># 画出损失函数的变化趋势</span><br>plot_x = np.arange(num_iterations)<br>plot_y = np.array(losses)<br>plt.plot(plot_x, plot_y)<br>plt.show()<br></code></pre></td></tr></table></figure>

<h4 id="2-4-6-训练过程扩展到全部参数"><a href="#2-4-6-训练过程扩展到全部参数" class="headerlink" title="2.4.6 训练过程扩展到全部参数"></a>2.4.6 训练过程扩展到全部参数</h4><p>为了能给读者直观的感受，上文演示的梯度下降的过程仅包含$w_5$和$w_9$两个参数。但房价预测的模型必须要对所有参数$w$和$b$进行求解，这需要将Network中的<code>update</code>和<code>train</code>函数进行修改。由于不再限定参与计算的参数（所有参数均参与计算），修改之后的代码反而更加简洁。</p>
<p>实现逻辑：“前向计算输出、根据输出和真实值计算Loss、基于Loss和输入计算梯度、根据梯度更新参数值”四个部分反复执行，直到到损失函数最小。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_of_weights</span>):<br>        <span class="hljs-comment"># 随机产生w的初始值</span><br>        <span class="hljs-comment"># 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子</span><br>        np.random.seed(<span class="hljs-number">0</span>)<br>        self.w = np.random.randn(num_of_weights, <span class="hljs-number">1</span>)<br>        self.b = <span class="hljs-number">0.</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        z = np.dot(x, self.w) + self.b<br>        <span class="hljs-keyword">return</span> z<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, z, y</span>):<br>        error = z - y<br>        num_samples = error.shape[<span class="hljs-number">0</span>]<br>        cost = error * error<br>        cost = np.<span class="hljs-built_in">sum</span>(cost) / num_samples<br>        <span class="hljs-keyword">return</span> cost<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">self, x, y</span>):<br>        z = self.forward(x)<br>        gradient_w = (z-y)*x<br>        gradient_w = np.mean(gradient_w, axis=<span class="hljs-number">0</span>)<br>        gradient_w = gradient_w[:, np.newaxis]<br>        gradient_b = (z - y)<br>        gradient_b = np.mean(gradient_b)        <br>        <span class="hljs-keyword">return</span> gradient_w, gradient_b<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, gradient_w, gradient_b, eta = <span class="hljs-number">0.01</span></span>):<br>        self.w = self.w - eta * gradient_w<br>        self.b = self.b - eta * gradient_b<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, x, y, iterations=<span class="hljs-number">100</span>, eta=<span class="hljs-number">0.01</span></span>):<br>        losses = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iterations):<br>            z = self.forward(x)<br>            L = self.loss(z, y)<br>            gradient_w, gradient_b = self.gradient(x, y)<br>            self.update(gradient_w, gradient_b, eta)<br>            losses.append(L)<br>            <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iter &#123;&#125;, loss &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i, L))<br>        <span class="hljs-keyword">return</span> losses<br><br><span class="hljs-comment"># 获取数据</span><br>train_data, test_data = load_data()<br>x = train_data[:, :-<span class="hljs-number">1</span>]<br>y = train_data[:, -<span class="hljs-number">1</span>:]<br><span class="hljs-comment"># 创建网络</span><br>net = Network(<span class="hljs-number">13</span>)<br>num_iterations=<span class="hljs-number">1000</span><br><span class="hljs-comment"># 启动训练</span><br>losses = net.train(x,y, iterations=num_iterations, eta=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># 画出损失函数的变化趋势</span><br>plot_x = np.arange(num_iterations)<br>plot_y = np.array(losses)<br>plt.plot(plot_x, plot_y)<br>plt.show()<br></code></pre></td></tr></table></figure>

<h4 id="2-4-7-随机梯度下降法（-Stochastic-Gradient-Descent）"><a href="#2-4-7-随机梯度下降法（-Stochastic-Gradient-Descent）" class="headerlink" title="2.4.7  随机梯度下降法（ Stochastic Gradient Descent）"></a>2.4.7  随机梯度下降法（ Stochastic Gradient Descent）</h4><p>在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD），核心概念如下：</p>
<ul>
<li>mini-batch：每次迭代时抽取出来的一批数据被称为一个mini-batch。</li>
<li>batch_size：一个mini-batch所包含的样本数目称为batch_size。</li>
<li>epoch：当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。</li>
</ul>
<p>下面结合程序介绍具体的实现过程，涉及到数据处理和训练过程两部分代码的修改。</p>
<ul>
<li><strong>数据处理代码修改</strong></li>
</ul>
<p>数据处理需要实现拆分数据批次和样本乱序（为了实现随机抽样的效果）两个功能。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_of_weights</span>):<br>        <span class="hljs-comment"># 随机产生w的初始值</span><br>        <span class="hljs-comment"># 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子</span><br>        <span class="hljs-comment">#np.random.seed(0)</span><br>        self.w = np.random.randn(num_of_weights, <span class="hljs-number">1</span>)<br>        self.b = <span class="hljs-number">0.</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        z = np.dot(x, self.w) + self.b<br>        <span class="hljs-keyword">return</span> z<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, z, y</span>):<br>        error = z - y<br>        num_samples = error.shape[<span class="hljs-number">0</span>]<br>        cost = error * error<br>        cost = np.<span class="hljs-built_in">sum</span>(cost) / num_samples<br>        <span class="hljs-keyword">return</span> cost<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">self, x, y</span>):<br>        z = self.forward(x)<br>        N = x.shape[<span class="hljs-number">0</span>]<br>        gradient_w = <span class="hljs-number">1.</span> / N * np.<span class="hljs-built_in">sum</span>((z-y) * x, axis=<span class="hljs-number">0</span>)<br>        gradient_w = gradient_w[:, np.newaxis]<br>        gradient_b = <span class="hljs-number">1.</span> / N * np.<span class="hljs-built_in">sum</span>(z-y)<br>        <span class="hljs-keyword">return</span> gradient_w, gradient_b<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, gradient_w, gradient_b, eta = <span class="hljs-number">0.01</span></span>):<br>        self.w = self.w - eta * gradient_w<br>        self.b = self.b - eta * gradient_b<br>            <br>                <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, training_data, num_epochs, batch_size=<span class="hljs-number">10</span>, eta=<span class="hljs-number">0.01</span></span>):<br>        n = <span class="hljs-built_in">len</span>(training_data)<br>        losses = []<br>        <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            <span class="hljs-comment"># 在每轮迭代开始之前，将训练数据的顺序随机打乱</span><br>            <span class="hljs-comment"># 然后再按每次取batch_size条数据的方式取出</span><br>            np.random.shuffle(training_data)<br>            <span class="hljs-comment"># 将训练数据进行拆分，每个mini_batch包含batch_size条的数据</span><br>            mini_batches = [training_data[k:k+batch_size] <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n, batch_size)]<br>            <span class="hljs-keyword">for</span> iter_id, mini_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(mini_batches):<br>                <span class="hljs-comment">#print(self.w.shape)</span><br>                <span class="hljs-comment">#print(self.b)</span><br>                x = mini_batch[:, :-<span class="hljs-number">1</span>]<br>                y = mini_batch[:, -<span class="hljs-number">1</span>:]<br>                a = self.forward(x)<br>                loss = self.loss(a, y)<br>                gradient_w, gradient_b = self.gradient(x, y)<br>                self.update(gradient_w, gradient_b, eta)<br>                losses.append(loss)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch &#123;:3d&#125; / iter &#123;:3d&#125;, loss = &#123;:.4f&#125;&#x27;</span>.<br>                                 <span class="hljs-built_in">format</span>(epoch_id, iter_id, loss))<br>        <br>        <span class="hljs-keyword">return</span> losses<br><br><span class="hljs-comment"># 获取数据</span><br>train_data, test_data = load_data()<br><br><span class="hljs-comment"># 创建网络</span><br>net = Network(<span class="hljs-number">13</span>)<br><span class="hljs-comment"># 启动训练</span><br>losses = net.train(train_data, num_epochs=<span class="hljs-number">50</span>, batch_size=<span class="hljs-number">100</span>, eta=<span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 画出损失函数的变化趋势</span><br>plot_x = np.arange(<span class="hljs-built_in">len</span>(losses))<br>plot_y = np.array(losses)<br>plt.plot(plot_x, plot_y)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p>观察上述Loss的变化，随机梯度下降加快了训练过程，但由于每次仅基于少量样本更新参数和计算损失，所以损失下降曲线会出现震荡。</p>
<hr>
<p><strong>说明：</strong></p>
<p>由于房价预测的数据量过少，所以难以感受到随机梯度下降带来的性能提升。</p>
<hr>
<h3 id="2-5-模型保存"><a href="#2-5-模型保存" class="headerlink" title="2.5 模型保存"></a>2.5 模型保存</h3><p>Numpy提供了save接口，可直接将模型权重数组保存为.npy格式的文件。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs py">np.save(<span class="hljs-string">&#x27;w.npy&#x27;</span>, net.w)<br>np.save(<span class="hljs-string">&#x27;b.npy&#x27;</span>, net.b)<br></code></pre></td></tr></table></figure>

<h3 id="2-5-小结"><a href="#2-5-小结" class="headerlink" title="2.5 小结"></a>2.5 小结</h3><p>本节我们详细介绍了如何使用NumPy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：</p>
<ul>
<li><p>构建网络，初始化参数$w$和$b$，定义预测和损失函数的计算方法。</p>
</li>
<li><p>随机选择初始点，建立梯度的计算方法和参数更新方式。</p>
</li>
<li><p>从总的数据集中抽取部分数据作为一个mini_batch，计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。</p>
</li>
</ul>
<h3 id="2-7-框架实现"><a href="#2-7-框架实现" class="headerlink" title="2.7 框架实现"></a>2.7 框架实现</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#加载飞桨、NumPy和相关类库</span><br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.fluid <span class="hljs-keyword">as</span> fluid<br><span class="hljs-keyword">import</span> paddle.fluid.dygraph <span class="hljs-keyword">as</span> dygraph<br><span class="hljs-keyword">from</span> paddle.fluid.dygraph <span class="hljs-keyword">import</span> Linear<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>    <span class="hljs-comment"># 从文件导入数据</span><br>    datafile = <span class="hljs-string">&#x27;housing.data&#x27;</span><br>    data = np.fromfile(datafile, sep=<span class="hljs-string">&#x27; &#x27;</span>, dtype=np.float32)<br><br>    <span class="hljs-comment"># 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数</span><br>    feature_names = [ <span class="hljs-string">&#x27;CRIM&#x27;</span>, <span class="hljs-string">&#x27;ZN&#x27;</span>, <span class="hljs-string">&#x27;INDUS&#x27;</span>, <span class="hljs-string">&#x27;CHAS&#x27;</span>, <span class="hljs-string">&#x27;NOX&#x27;</span>, <span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;AGE&#x27;</span>, \<br>                      <span class="hljs-string">&#x27;DIS&#x27;</span>, <span class="hljs-string">&#x27;RAD&#x27;</span>, <span class="hljs-string">&#x27;TAX&#x27;</span>, <span class="hljs-string">&#x27;PTRATIO&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>, <span class="hljs-string">&#x27;MEDV&#x27;</span> ]<br>    feature_num = <span class="hljs-built_in">len</span>(feature_names)<br><br>    <span class="hljs-comment"># 将原始数据进行Reshape，变成[N, 14]这样的形状</span><br>    data = data.reshape([data.shape[<span class="hljs-number">0</span>] // feature_num, feature_num])<br><br>    <span class="hljs-comment"># 将原数据集拆分成训练集和测试集</span><br>    <span class="hljs-comment"># 这里使用80%的数据做训练，20%的数据做测试</span><br>    <span class="hljs-comment"># 测试集和训练集必须是没有交集的</span><br>    ratio = <span class="hljs-number">0.8</span><br>    offset = <span class="hljs-built_in">int</span>(data.shape[<span class="hljs-number">0</span>] * ratio)<br>    training_data = data[:offset]<br><br>    <span class="hljs-comment"># 计算train数据集的最大值，最小值</span><br>    maximums, minimums, avgs = training_data.<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">0</span>), training_data.<span class="hljs-built_in">min</span>(axis=<span class="hljs-number">0</span>), training_data.mean(axis=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># 记录数据的归一化参数，在预测时对数据做归一化</span><br>    <span class="hljs-keyword">global</span> max_values<br>    <span class="hljs-keyword">global</span> min_values<br>    <span class="hljs-keyword">global</span> avg_values<br>   <br>    max_values = maximums<br>    min_values = minimums<br>    avg_values = avgs<br>    <br>    <span class="hljs-comment"># 对数据进行归一化处理</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(feature_num):<br>        data[:, i] = (data[:, i] - avg_values[i]) / (maximums[i] - minimums[i])<br><br>    <span class="hljs-comment"># 训练集和测试集的划分比例</span><br>    training_data = data[:offset]<br>    test_data = data[offset:]<br>    <span class="hljs-keyword">return</span> training_data, test_data<br>    <br>    <span class="hljs-comment"># 验证数据集读取程序的正确性</span><br>training_dataverify, test_dataverify = load_data()<br><span class="hljs-built_in">print</span>(training_dataverify.shape)<br><span class="hljs-built_in">print</span>(training_dataverify[<span class="hljs-number">0</span>,:])<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Regressor</span>(fluid.dygraph.Layer):<br><br>    <span class="hljs-comment"># self代表类的实例自身</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,name_scope</span>):<br>        <span class="hljs-comment"># 初始化父类中的一些参数</span><br>        <span class="hljs-built_in">super</span>(Regressor, self).__init__(name_scope)<br>        name_scope = self.full_name()<br>        <br>        <span class="hljs-comment"># 定义一层全连接层，输入维度是13，输出维度是1</span><br>        self.fc = Linear(input_dim=<span class="hljs-number">13</span>, output_dim=<span class="hljs-number">1</span>,act=<span class="hljs-literal">None</span>)<br>    <br>    <span class="hljs-comment"># 网络的前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        x = self.fc(inputs)<br>        <span class="hljs-keyword">return</span> x<br>        <br>        <span class="hljs-comment">#定义飞桨动态图的工作环境</span><br><span class="hljs-keyword">with</span> fluid.dygraph.guard():<br>    <span class="hljs-comment">#声明定义好的线性回归模型</span><br>    model = Regressor(<span class="hljs-string">&quot;Regressor&quot;</span>)<br>    <span class="hljs-comment">#开启模型训练模式</span><br>    model.train()<br>    <span class="hljs-comment">#加载数据</span><br>    training_data, test_data = load_data()<br>    <span class="hljs-comment">#定义优化算法，这里使用随机梯度下降-SGD</span><br>    <span class="hljs-comment">#学习率设置为0.01</span><br>    opt = fluid.optimizer.SGD(learning_rate=<span class="hljs-number">0.01</span>, parameter_list=model.parameters())<br>    <br>    <span class="hljs-keyword">with</span> dygraph.guard(fluid.CPUPlace()):<br>    EPOCH_NUM = <span class="hljs-number">10</span> <span class="hljs-comment">#设置外层循环次数，即数据集遍历次数</span><br>    BATCH_SIZE = <span class="hljs-number">10</span> <span class="hljs-comment">#设置一个批次的数据大小</span><br>    <br>    <span class="hljs-comment">#定义外层循环</span><br>    <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br><span class="hljs-comment">#         在每轮训练之前，将训练数据打乱</span><br>        np.random.shuffle(training_data)<br><span class="hljs-comment">#         将训练数据拆分，每个batch含10个数据（样本）</span><br>        mini_batches = [training_data[k:k+BATCH_SIZE] <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(training_data), BATCH_SIZE)]<br><span class="hljs-comment">#         定义内部循环</span><br>        <span class="hljs-keyword">for</span> iter_id, mini_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(mini_batches):<br>            x = np.array(mini_batch[:,:-<span class="hljs-number">1</span>]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            y = np.array(mini_batch[:,-<span class="hljs-number">1</span>:]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-comment">#             将numpy数据转换为飞浆动态图variable格式</span><br>            house_features = dygraph.to_variable(x)<br>            prices = dygraph.to_variable(y)<br>        <span class="hljs-comment">#     调用模型前向计算</span><br>            predicts = model(house_features)<br><br>        <span class="hljs-comment">#     计算损失</span><br>            loss = fluid.layers.square_error_cost(predicts, label=prices)<br>            avg_loss = fluid.layers.mean(loss)<br>            <span class="hljs-keyword">if</span> iter_id%<span class="hljs-number">20</span>==<span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch:&#123;&#125;,iter_id:&#123;&#125;,loss is:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch_id, iter_id, avg_loss.numpy()))<br>        <span class="hljs-comment">#     反向传播</span><br>            avg_loss.backward()<br>        <span class="hljs-comment">#     最小化loss，更新参数</span><br>            opt.minimize(avg_loss)<br>        <span class="hljs-comment">#     清除梯度</span><br>            model.clear_gradients()<br><span class="hljs-comment">#     保存模型</span><br>fluid.save_dygraph(model.state_dict(), <span class="hljs-string">&#x27;LRmodel&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_one_example</span>(<span class="hljs-params">data_dir</span>):<br>    f = <span class="hljs-built_in">open</span>(data_dir,<span class="hljs-string">&quot;r&quot;</span>)<br>    datas = f.readlines()<br><span class="hljs-comment">#     选取倒数第10个数据进行预测</span><br>    tmp = datas[-<span class="hljs-number">9</span>]<br>    tmp = tmp.strip().split()<br>    one_data = [<span class="hljs-built_in">float</span>(v) <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> tmp]<br>    <br><span class="hljs-comment">#     对数据进行归一化处理</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(one_data)-<span class="hljs-number">1</span>):<br>        one_data[i] = (one_data[i]-avg_values[i])/(max_values[i]-min_values[i])<br>        <br>    data = np.reshape(np.array(one_data[:-<span class="hljs-number">1</span>]),[<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]).astype(np.float32)<br>    label = one_data[-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> data, label<br><span class="hljs-keyword">with</span> dygraph.guard():<br>    model_dict, _ = fluid.load_dygraph(<span class="hljs-string">&#x27;LR_model&#x27;</span>)<br>    model.load_dict(model_dict)<br>    model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-comment">#     参数为测试数据集的文件地址</span><br>    test_data, label = load_one_example(<span class="hljs-string">&#x27;housing.data&#x27;</span>)<br><span class="hljs-comment">#     将数据转为variable格式</span><br>    test_data = dygraph.to_variable(test_data)<br>    results = model(test_data)<br>    <br><span class="hljs-comment">#     对结果进行反归一化</span><br>    results = results * (max_values[-<span class="hljs-number">1</span>]-min_values[-<span class="hljs-number">1</span>]) + avg_values[-<span class="hljs-number">1</span>]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Inference result is &#123;&#125;, the corresponding label is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(results.numpy(), label))<br></code></pre></td></tr></table></figure>

<h2 id="3-实例二“手写文字识别”"><a href="#3-实例二“手写文字识别”" class="headerlink" title="3.实例二“手写文字识别”"></a>3.实例二“手写文字识别”</h2><h3 id="1-单层网络多元逻辑回归模型"><a href="#1-单层网络多元逻辑回归模型" class="headerlink" title="1.单层网络多元逻辑回归模型"></a>1.单层网络多元逻辑回归模型</h3><h4 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h4><p>在房价预测深度学习任务中，我们使用了单层且没有非线性变换的模型，取得了理想的预测效果。在手写数字识别中，我们依然使用这个模型预测输入的图形数字值。其中，模型的输入为784维（28×28）数据，输出为1维数据，如 <strong>图5</strong> 所示。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/9c146e7d9c4a4119a8cd09f7c8b5ee61f2ac1820a221429a80430291728b9c4a" srcset="/img/loading.gif" lazyload width="500" hegiht="" ></center>
<center><br>图5：手写数字识别网络模型</br></center>

<p>输入像素的位置排布信息对理解图像内容非常重要（如将原始尺寸为28×28图像的像素按照7×112的尺寸排布，那么其中的数字将不可识别），因此网络的输入设计为28×28的尺寸，而不是1×784，以便于模型能够正确处理像素之间的空间信息。</p>
<hr>
<p><strong>说明：</strong></p>
<p>事实上，采用只有一层的简单网络（对输入求加权和）时并没有处理位置关系信息，因此可以猜测出此模型的预测效果可能有限。在后续优化环节介绍的卷积神经网络则更好的考虑了这种位置关系信息，模型的预测效果也会有显著提升。</p>
<hr>
<p>下面以类的方式组建手写数字识别的网络，实现方法如下所示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 定义mnist数据识别网络结构，同房价预测网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MNIST</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MNIST, self).__init__()<br>        <br>        <span class="hljs-comment"># 定义一层全连接层，输出维度是1</span><br>        self.fc = paddle.nn.Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">1</span>)<br>        <br>    <span class="hljs-comment"># 定义网络结构的前向计算过程</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc(inputs)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>

<h4 id="训练配置"><a href="#训练配置" class="headerlink" title="训练配置"></a>训练配置</h4><p>训练配置需要先生成模型实例（设为“训练”状态），再设置优化算法和学习率（使用随机梯度下降SGD，学习率设置为0.001），实现方法如下所示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 声明网络结构</span><br>model = MNIST()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-comment"># 启动训练模式</span><br>    model.train()<br>    <span class="hljs-comment"># 加载训练集 batch_size 设为 16</span><br>    train_loader = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="hljs-string">&#x27;train&#x27;</span>), <br>                                        batch_size=<span class="hljs-number">16</span>, <br>                                        shuffle=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 定义优化器，使用随机梯度下降SGD优化器，学习率设置为0.001</span><br>    opt = paddle.optimizer.SGD(learning_rate=<span class="hljs-number">0.001</span>, parameters=model.parameters())<br></code></pre></td></tr></table></figure>

<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>训练过程采用二层循环嵌套方式，训练完成后需要保存模型参数，以便后续使用。</p>
<ul>
<li>内层循环：负责整个数据集的一次遍历，遍历数据集采用分批次（batch）方式。</li>
<li>外层循环：定义遍历数据集的次数，本次训练中外层循环10次，通过参数EPOCH_NUM设置。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 图像归一化函数，将数据范围为[0, 255]的图像归一化到[0, 1]</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">norm_img</span>(<span class="hljs-params">img</span>):<br>    <span class="hljs-comment"># 验证传入数据格式是否正确，img的shape为[batch_size, 28, 28]</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(img.shape) == <span class="hljs-number">3</span><br>    batch_size, img_h, img_w = img.shape[<span class="hljs-number">0</span>], img.shape[<span class="hljs-number">1</span>], img.shape[<span class="hljs-number">2</span>]<br>    <span class="hljs-comment"># 归一化图像数据</span><br>    img = img / <span class="hljs-number">255</span><br>    <span class="hljs-comment"># 将图像形式reshape为[batch_size, 784]</span><br>    img = paddle.reshape(img, [batch_size, img_h*img_w])<br>    <br>    <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-comment"># 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型</span><br>paddle.vision.set_image_backend(<span class="hljs-string">&#x27;cv2&#x27;</span>)<br><br><span class="hljs-comment"># 声明网络结构</span><br>model = MNIST()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-comment"># 启动训练模式</span><br>    model.train()<br>    <span class="hljs-comment"># 加载训练集 batch_size 设为 16</span><br>    train_loader = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="hljs-string">&#x27;train&#x27;</span>), <br>                                        batch_size=<span class="hljs-number">16</span>, <br>                                        shuffle=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 定义优化器，使用随机梯度下降SGD优化器，学习率设置为0.001</span><br>    opt = paddle.optimizer.SGD(learning_rate=<span class="hljs-number">0.001</span>, parameters=model.parameters())<br>    EPOCH_NUM = <span class="hljs-number">10</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>            images = norm_img(data[<span class="hljs-number">0</span>]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            labels = data[<span class="hljs-number">1</span>].astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            <br>            <span class="hljs-comment">#前向计算的过程</span><br>            predicts = model(images)<br>            <br>            <span class="hljs-comment"># 计算损失</span><br>            loss = F.square_error_cost(predicts, labels)<br>            avg_loss = paddle.mean(loss)<br>            <br>            <span class="hljs-comment">#每训练了1000批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch_id: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, batch_id, avg_loss.numpy()))<br>            <br>            <span class="hljs-comment">#后向传播，更新参数的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br>            <br><br>train(model)<br>paddle.save(model.state_dict(), <span class="hljs-string">&#x27;./mnist.pdparams&#x27;</span>)<br></code></pre></td></tr></table></figure>

<blockquote>
<p>另外，从训练过程中损失所发生的变化可以发现，虽然损失整体上在降低，但到训练的最后一轮，损失函数值依然较高。可以猜测手写数字识别完全复用房价预测的代码，训练效果并不好。接下来我们通过模型测试，获取模型训练的真实效果。</p>
</blockquote>
<h4 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h4><p>模型测试的主要目的是验证训练好的模型是否能正确识别出数字，包括如下四步：</p>
<ul>
<li>声明实例</li>
<li>加载模型：加载训练过程中保存的模型参数，</li>
<li>灌入数据：将测试样本传入模型，模型的状态设置为校验状态（eval），显式告诉框架我们接下来只会使用前向计算的流程，不会计算梯度和梯度反向传播。</li>
<li>获取预测结果，取整后作为预测标签输出。</li>
</ul>
<p>在模型测试之前，需要先从’.&#x2F;work&#x2F;example_0.png’文件中读取样例图片，并进行归一化处理。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 导入图像读取第三方库</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br>train_loader = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="hljs-string">&#x27;train&#x27;</span>), <br>                                    batch_size=<span class="hljs-number">16</span>, <br>                                    shuffle=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># print(data[0][0].shape)</span><br><span class="hljs-comment"># plt.imshow(norm_img(train_data0))</span><br><span class="hljs-comment"># plt.show()</span><br>img_path = <span class="hljs-string">&#x27;./data/data17152/example_0.png&#x27;</span><br><span class="hljs-comment"># 读取原始图像并显示</span><br>im = Image.<span class="hljs-built_in">open</span>(img_path)<br>plt.imshow(im)<br>plt.show()<br><span class="hljs-comment"># 将原始图像转为灰度图</span><br>im = im.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;原始图像shape: &#x27;</span>, np.array(im).shape)<br><span class="hljs-comment"># 使用Image.ANTIALIAS方式采样原始图片</span><br>im = im.resize((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), Image.ANTIALIAS)<br>plt.imshow(im)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;采样后图片shape: &quot;</span>, np.array(im).shape)<br></code></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 读取一张本地的样例图片，转变成模型输入的格式</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_image</span>(<span class="hljs-params">img_path</span>):<br>    <span class="hljs-comment"># 从img_path中读取图像，并转为灰度图</span><br>    im = Image.<span class="hljs-built_in">open</span>(img_path).convert(<span class="hljs-string">&#x27;L&#x27;</span>)<br>    im = im.resize((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), Image.ANTIALIAS)<br>    im = np.array(im).reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).astype(np.float32)<br><br>    <span class="hljs-comment"># 图像归一化，保持和数据集的数据范围一致</span><br>    im =  <span class="hljs-number">1</span> - im / <span class="hljs-number">255</span><br>    <span class="hljs-keyword">return</span> im<br><br><span class="hljs-comment"># 定义预测过程</span><br>model = MNIST()<br>params_file_path = <span class="hljs-string">&#x27;mnist.pdparams&#x27;</span><br>img_path = <span class="hljs-string">&#x27;./data/data17152/example_0.png&#x27;</span><br><span class="hljs-comment"># 加载模型参数</span><br>param_dict = paddle.load(params_file_path)<br><span class="hljs-comment"># 将模型参数加载到模型里</span><br>model.load_dict(param_dict)<br><span class="hljs-comment"># 灌入数据</span><br>model.<span class="hljs-built_in">eval</span>()<br>tensor_img = load_image(img_path)<br>result = model(paddle.to_tensor(tensor_img))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;result&#x27;</span>,result)<br><span class="hljs-comment">#  预测输出取整，即为预测的数字，打印结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;本次预测的数字是&quot;</span>, result.numpy().astype(<span class="hljs-string">&#x27;int32&#x27;</span>))<br></code></pre></td></tr></table></figure>

<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p><strong><u>灰度图：0是黑，255是白</u></strong></p>
<p><strong>重要函数：</strong></p>
<ul>
<li><code>enumerate(train_loader())</code>将迭代器进行迭代</li>
<li><code>np.reshape(1, -1)</code>将ndtype数组转为1维行向量，**-1为占位符**，列数看有多少列就多少列</li>
</ul>
<h3 id="2-多层网络改进模型"><a href="#2-多层网络改进模型" class="headerlink" title="2.多层网络改进模型"></a>2.多层网络改进模型</h3><h4 id="数据同步读取与训练"><a href="#数据同步读取与训练" class="headerlink" title="数据同步读取与训练"></a>数据同步读取与训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#数据处理部分之前的代码，加入部分数据处理的库</span><br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">from</span> paddle.nn <span class="hljs-keyword">import</span> Linear<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> gzip<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">mode=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>    datafile = <span class="hljs-string">&#x27;./work/mnist.json.gz&#x27;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loading mnist dataset from &#123;&#125; ......&#x27;</span>.<span class="hljs-built_in">format</span>(datafile))<br>    <span class="hljs-comment"># 加载json数据文件</span><br>    data = json.load(gzip.<span class="hljs-built_in">open</span>(datafile))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mnist dataset load done&#x27;</span>)<br>   <br>    <span class="hljs-comment"># 读取到的数据区分训练集，验证集，测试集</span><br>    train_set, val_set, eval_set = data<br>    <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;train&#x27;</span>:<br>        <span class="hljs-comment"># 获得训练数据集</span><br>        imgs, labels = train_set[<span class="hljs-number">0</span>], train_set[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&#x27;valid&#x27;</span>:<br>        <span class="hljs-comment"># 获得验证数据集</span><br>        imgs, labels = val_set[<span class="hljs-number">0</span>], val_set[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&#x27;eval&#x27;</span>:<br>        <span class="hljs-comment"># 获得测试数据集</span><br>        imgs, labels = eval_set[<span class="hljs-number">0</span>], eval_set[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">else</span>:<span class="hljs-comment">#抛出异常</span><br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;mode can only be one of [&#x27;train&#x27;, &#x27;valid&#x27;, &#x27;eval&#x27;]&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练数据集数量: &quot;</span>, <span class="hljs-built_in">len</span>(imgs))<br>    <br>    <span class="hljs-comment"># 校验数据</span><br>    imgs_length = <span class="hljs-built_in">len</span>(imgs)<br><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(imgs) == <span class="hljs-built_in">len</span>(labels), \<br>          <span class="hljs-string">&quot;length of train_imgs(&#123;&#125;) should be the same as train_labels(&#123;&#125;)&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(imgs), <span class="hljs-built_in">len</span>(labels))<br>    <br>    <span class="hljs-comment"># 获得数据集长度</span><br>    imgs_length = <span class="hljs-built_in">len</span>(imgs)<br>    <br>    <span class="hljs-comment"># 定义数据集每个数据的序号，根据序号读取数据</span><br>    index_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(imgs_length))<br>    <span class="hljs-comment"># 读入数据时用到的批次大小</span><br>    BATCHSIZE = <span class="hljs-number">100</span><br>    <br>    <span class="hljs-comment"># 定义数据生成器</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generator</span>():<br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            <span class="hljs-comment"># 训练模式下打乱数据</span><br>            random.shuffle(index_list)<br>        imgs_list = []<br>        labels_list = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index_list:<br>            <span class="hljs-comment"># 将数据处理成希望的类型</span><br>            img = np.array(imgs[i]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            label = np.array(labels[i]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            imgs_list.append(img) <br>            labels_list.append(label)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(imgs_list) == BATCHSIZE:<br>                <span class="hljs-comment"># 每获得一个batchsize的数据，就返回</span><br>                <span class="hljs-keyword">yield</span> np.array(imgs_list), np.array(labels_list)<br>                <span class="hljs-comment"># 清空数据读取列表</span><br>                imgs_list = []<br>                labels_list = []<br>    <br>        <span class="hljs-comment"># 如果剩余数据的数目小于BATCHSIZE，</span><br>        <span class="hljs-comment"># 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(imgs_list) &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">yield</span> np.array(imgs_list), np.array(labels_list)<br>    <span class="hljs-keyword">return</span> data_generator<span class="hljs-comment">#返回迭代器</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#数据处理部分之后的代码，数据读取的部分调用Load_data函数</span><br><span class="hljs-comment"># 定义网络结构，同上一节所使用的网络结构</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MNIST</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MNIST, self).__init__()<br>        <span class="hljs-comment"># 定义一层全连接层，输出维度是1</span><br>        self.fc = paddle.nn.Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc(inputs)<br>        <span class="hljs-keyword">return</span> outputs<br><br><span class="hljs-comment"># 训练配置，并启动训练过程</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    model = MNIST()<br>    model.train()<br>    <span class="hljs-comment">#调用加载数据的函数</span><br>    train_loader = load_data(<span class="hljs-string">&#x27;train&#x27;</span>)<br>    opt = paddle.optimizer.SGD(learning_rate=<span class="hljs-number">0.001</span>, parameters=model.parameters())<br>    EPOCH_NUM = <span class="hljs-number">10</span><br>    <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>            <span class="hljs-comment">#准备数据，变得更加简洁</span><br>            images, labels = data<br>            <span class="hljs-comment">#将ndtype数据转换为模型接受的tensor类型进行输入训练</span><br>            images = paddle.to_tensor(images)<br>            labels = paddle.to_tensor(labels) <br><br>            <span class="hljs-comment">#前向计算的过程</span><br>            predits = model(images)<br>            <br>            <span class="hljs-comment">#计算损失，取一个批次样本损失的平均值</span><br>            loss = F.square_error_cost(predits, labels)<br>            avg_loss = paddle.mean(loss)      <br>            <br>            <span class="hljs-comment">#每训练了200批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch_id, batch_id, avg_loss.numpy()))<br>            <br>            <span class="hljs-comment">#后向传播，更新参数的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br><br>    <span class="hljs-comment"># 保存模型</span><br>    paddle.save(model.state_dict(), <span class="hljs-string">&#x27;./mnist.pdparams&#x27;</span>)<br><span class="hljs-comment"># 创建模型           </span><br>model = MNIST()<br><span class="hljs-comment"># 启动训练过程</span><br>train(model)<br><br></code></pre></td></tr></table></figure>

<blockquote>
<p>上面提到的数据读取采用的是同步数据读取方式。对于样本量较大、数据读取较慢的场景，建议采用异步数据读取方式。异步读取数据时，数据读取和模型训练并行执行，从而加快了数据读取速度，牺牲一小部分内存换取数据读取效率的提升，二者关系如 <strong>图4</strong> 所示。</p>
</blockquote>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/a5fd990c5355426183a71b95aa28a59f979014f6905144ddb415c5a4fe647441" srcset="/img/loading.gif" lazyload width="500" ></center>
<center><br>图4：同步数据读取和异步数据读取示意图</br></center>
* **同步数据读取**：数据读取与模型训练串行。当模型需要数据时，才运行数据读取函数获得当前批次的数据。在读取数据期间，模型一直等待数据读取结束才进行训练，数据读取速度相对较慢。
* **异步数据读取**：数据读取和模型训练并行。读取到的数据不断的放入缓存区，无需等待模型训练就可以启动下一轮数据读取。当模型训练完一个批次后，不用等待数据读取过程，直接从缓存区获得下一批次数据进行训练，从而加快了数据读取速度。
* **异步队列**：数据读取和模型训练交互的仓库，二者均可以从仓库中读取数据，它的存在使得两者的工作节奏可以解耦。

<p>使用飞桨实现异步数据读取非常简单，只需要两个步骤：</p>
<ol>
<li>构建一个继承paddle.io.Dataset类的数据读取器。</li>
<li>通过paddle.io.DataLoader创建异步数据读取的迭代器。</li>
</ol>
<h4 id="数据异步读取与训练"><a href="#数据异步读取与训练" class="headerlink" title="数据异步读取与训练"></a>数据异步读取与训练</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> gzip<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 创建一个类MnistDataset，继承paddle.io.Dataset 这个类</span><br><span class="hljs-comment"># MnistDataset的作用和上面load_data()函数的作用相同，均是构建一个迭代器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MnistDataset</span>(paddle.io.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, mode</span>):<br>        datafile = <span class="hljs-string">&#x27;./work/mnist.json.gz&#x27;</span><br>        data = json.load(gzip.<span class="hljs-built_in">open</span>(datafile))<br>        <span class="hljs-comment"># 读取到的数据区分训练集，验证集，测试集</span><br>        train_set, val_set, eval_set = data<br>        <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;train&#x27;</span>:<br>            <span class="hljs-comment"># 获得训练数据集</span><br>            imgs, labels = train_set[<span class="hljs-number">0</span>], train_set[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&#x27;valid&#x27;</span>:<br>            <span class="hljs-comment"># 获得验证数据集</span><br>            imgs, labels = val_set[<span class="hljs-number">0</span>], val_set[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&#x27;eval&#x27;</span>:<br>            <span class="hljs-comment"># 获得测试数据集</span><br>            imgs, labels = eval_set[<span class="hljs-number">0</span>], eval_set[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;mode can only be one of [&#x27;train&#x27;, &#x27;valid&#x27;, &#x27;eval&#x27;]&quot;</span>)<br>        <br>        <span class="hljs-comment"># 校验数据</span><br>        imgs_length = <span class="hljs-built_in">len</span>(imgs)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(imgs) == <span class="hljs-built_in">len</span>(labels), \<br>            <span class="hljs-string">&quot;length of train_imgs(&#123;&#125;) should be the same as train_labels(&#123;&#125;)&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(imgs), <span class="hljs-built_in">len</span>(labels))<br>        <br>        self.imgs = imgs<br>        self.labels = labels<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        img = np.array(self.imgs[idx]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        label = np.array(self.labels[idx]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        <br>        <span class="hljs-keyword">return</span> img, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.imgs)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 声明数据加载函数，使用MnistDataset数据集</span><br>train_dataset = MnistDataset(mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-comment"># print(train_dataset[1][1])</span><br><span class="hljs-comment"># 使用paddle.io.DataLoader 定义DataLoader对象用于加载Python生成器产生的数据，</span><br><span class="hljs-comment"># DataLoader 返回的是一个批次数据迭代器，并且是异步的；</span><br>data_loader = paddle.io.DataLoader(train_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 迭代的读取数据并打印数据的形状</span><br><span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader()):<br>    images, labels = data<br>    <span class="hljs-built_in">print</span>(i, images.shape, labels.shape)<br>    <span class="hljs-keyword">if</span> i&gt;=<span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    model = MNIST()<br>    model.train()<br>    opt = paddle.optimizer.SGD(learning_rate=<span class="hljs-number">0.001</span>, parameters=model.parameters())<br>    EPOCH_NUM = <span class="hljs-number">10</span><br>    <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader()):<br>            images, labels = data<br>            images = paddle.to_tensor(images)<br>            labels = paddle.to_tensor(labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            <br>            <span class="hljs-comment">#前向计算的过程  </span><br>            predicts = model(images)<br><br>            <span class="hljs-comment">#计算损失，取一个批次样本损失的平均值</span><br>            loss = F.square_error_cost(predicts, labels)<br>            avg_loss = paddle.mean(loss)       <br>            <br>            <span class="hljs-comment">#每训练了200批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch_id, batch_id, avg_loss.numpy()))<br>            <br>            <span class="hljs-comment">#后向传播，更新参数的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br><br>    <span class="hljs-comment">#保存模型参数</span><br>    paddle.save(model.state_dict(), <span class="hljs-string">&#x27;mnist&#x27;</span>)<br><br><span class="hljs-comment">#创建模型</span><br>model = MNIST()<br><span class="hljs-comment">#启动训练过程</span><br>train(model)<br></code></pre></td></tr></table></figure>









<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><ol>
<li><a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E8%BF%AD%E4%BB%A3%E5%99%A8&spm=1001.2101.3001.7020">迭代器</a>迭代器有两个基本的方法：创建迭代器<strong>iter()</strong> 和 访问迭代器**next()**。</li>
</ol>
<p>可以直接作用于for循环的对象统称为可迭代对象：Iterable</p>
<ul>
<li><p>一类是集合数据类型，如list、tuple、dict、set、str等；</p>
</li>
<li><p>一类是generator，包括生成器和带yield的generator function</p>
</li>
</ul>
<p>迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。</p>
<ol start="2">
<li>生成器</li>
</ol>
<p>如果列表元素可以按照某种算法推算出来，可以在循环的过程中不断推算出后续的元素，这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。</p>
<ol start="3">
<li>yield：是一个生成器函数，返回的是一个迭代器</li>
</ol>
<p>yield的作用：返回一个可以用来迭代(for循环)的生成器，它的应用场景通常为一个需要返回一系列值的，含有循环的函数中。</p>
<ol start="4">
<li>assert：<code>assert a &gt; 0,&quot;a超出范围&quot;</code><u>如果a&gt;0则顺序执行，否则抛出异常提示a超出范围</u></li>
</ol>
<h3 id="3-模型设计之损失函数"><a href="#3-模型设计之损失函数" class="headerlink" title="3.模型设计之损失函数"></a>3.模型设计之损失函数</h3><p>在之前的方案中，我们复用了房价预测模型的损失函数-均方误差。从预测效果来看，虽然损失不断下降，模型的预测值逐渐逼近真实值，但模型的最终效果不够理想。究其根本，不同的深度学习任务需要有各自适宜的损失函数。我们以房价预测和手写数字识别两个任务为例，详细剖析其中的缘由如下：</p>
<ol>
<li>房价预测是回归任务，而手写数字识别是分类任务，使用均方误差作为分类任务的损失函数存在逻辑和效果上的缺欠。</li>
<li>房价可以是大于0的任何浮点数，而手写数字识别的输出只可能是0~9之间的10个整数，相当于一种标签。</li>
<li>在房价预测的案例中，由于房价本身是一个连续的实数值，因此以模型输出的数值和真实房价差距作为损失函数（Loss）是符合道理的。但对于分类问题，真实结果是分类标签，而模型输出是实数值，导致以两者相减作为损失不具备物理含义。</li>
</ol>
<h4 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h4><p>如果模型能输出10个标签的概率，对应真实标签的概率输出尽可能接近100%，而其他标签的概率输出尽可能接近0%，且所有输出概率之和为1。这是一种更合理的假设！与此对应，真实的标签值可以转变成一个10维度的one-hot向量，在对应数字的位置上为1，其余位置为0，比如标签“6”可以转变成[0,0,0,0,0,0,1,0,0,0]。</p>
<p>为了实现上述思路，需要引入Softmax函数，它可以将原始输出转变成对应标签的概率，公式如下，其中$C$是标签类别个数。<br>$$<br>softmax(x_i) &#x3D; \frac {e^{x_i}}{\sum_{j&#x3D;0}^N{e^{x_j}}}, i&#x3D;0, …, C-1<br>$$<br>从公式的形式可见，每个输出的范围均在0~1之间，且所有输出之和等于1，这是这种变换后可被解释成概率的基本前提。对应到代码上，需要在前向计算中，对全连接网络的输出层增加一个Softmax运算，<code>outputs = F.softmax(outputs)</code>。</p>
<p><strong>图3</strong> 是一个三个标签的分类模型（三分类）使用的Softmax输出层，从中可见原始输出的三个数字3、1、-3，经过Softmax层后转变成加和为1的三个概率值0.88、0.12、0。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/ef129caf64254318821e9410bb71ab1f45fff20e4282482986081d44a1e3bcbb" srcset="/img/loading.gif" lazyload alt="img"></p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>在模型输出为分类标签的概率时，直接以标签和概率做比较也不够合理，人们更习惯使用交叉熵误差作为分类问题的损失衡量。</p>
<p>交叉熵损失函数的设计是基于最大似然思想：最大概率得到观察结果的假设是真的。如何理解呢？举个例子来说，如 <strong>图7</strong> 所示。有两个外形相同的盒子，甲盒中有99个白球，1个蓝球；乙盒中有99个蓝球，1个白球。一次试验取出了一个蓝球，请问这个球应该是从哪个盒子中取出的？</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/13a942e5ec7f4e91badb2f4613c6f71a00e51c8afb6a435e94a0b47cedac9515" srcset="/img/loading.gif" lazyload width="400" hegiht="" ></center>
<center><br>图7：体会最大似然的思想 </br></center>

<p>相信大家简单思考后均会得出更可能是从乙盒中取出的，因为从乙盒中取出一个蓝球的概率更高$（P(D|h)）$，所以观察到一个蓝球更可能是从乙盒中取出的$(P(h|D))$。$D$是观测的数据，即蓝球白球；$h$是模型，即甲盒乙盒。这就是贝叶斯公式所表达的思想：</p>
<p>$$P(h|D) ∝ P(h) \cdot P(D|h)$$</p>
<p>依据贝叶斯公式，某二分类模型“生成”$n$个训练样本的概率：</p>
<p>$$P(x_1)\cdot S(w^{T}x_1)\cdot P(x_2)\cdot(1-S(w^{T}x_2))\cdot … \cdot P(x_n)\cdot S(w^{T}x_n)$$</p>
<hr>
<p><strong>说明：</strong></p>
<p>对于二分类问题，模型为$S(w^{T}x_i)$，$S$为Sigmoid函数。当$y_i$&#x3D;1，概率为$S(w^{T}x_i)$；当$y_i$&#x3D;0，概率为$1-S(w^{T}x_i)$。</p>
<hr>
<p>经过公式推导，使得上述概率最大等价于最小化交叉熵，得到交叉熵的损失函数。交叉熵的公式如下：</p>
<p>$$ L &#x3D; -[\sum_{k&#x3D;1}^{n} t_k\log y_k +(1- t_k)\log(1-y_k)] $$</p>
<p>其中，$\log$表示以$e$为底数的自然对数。$y_k$代表模型输出，$t_k$代表各个标签。$t_k$中只有正确解的标签为1，其余均为0（one-hot表示）。</p>
<p>因此，交叉熵只计算对应着“正确解”标签的输出的自然对数。比如，假设正确标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差是$−\log 0.6 &#x3D; 0.51$；若“2”对应的输出是0.1，则交叉熵误差为$−\log 0.1 &#x3D; 2.30$。由此可见，交叉熵误差的值是由正确标签所对应的输出结果决定的。</p>
<p>自然对数的函数曲线可由如下代码实现。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x = np.arange(<span class="hljs-number">0.01</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0.01</span>)<br>y = np.log(x)<br>plt.title(<span class="hljs-string">&quot;y=log(x)&quot;</span>) <br>plt.xlabel(<span class="hljs-string">&quot;x&quot;</span>) <br>plt.ylabel(<span class="hljs-string">&quot;y&quot;</span>) <br>plt.plot(x,y)<br>plt.show()<br>plt.figure()<br></code></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote>
<p>分类任务</p>
<ol>
<li>数据处理部分：将输入的标签label数据类型改为int64型</li>
<li>网络定义部分：全连接层输出应该用SoftMax处理，将单一输出改为各类的输出概率，所有概率之和为1。</li>
<li>训练过程部分：损失函数改为交叉熵</li>
</ol>
</blockquote>
<h3 id="4-训练配置之优化器"><a href="#4-训练配置之优化器" class="headerlink" title="4.训练配置之优化器"></a>4.训练配置之优化器</h3><h4 id="设置学习率"><a href="#设置学习率" class="headerlink" title="设置学习率"></a>设置学习率</h4><p>在深度学习神经网络模型中，通常使用标准的随机梯度下降算法更新参数，学习率代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。学习率和深度学习任务类型有关，合适的学习率往往需要大量的实验和调参经验。探索学习率最优值时需要注意如下两点：</p>
<ul>
<li><strong>学习率不是越小越好</strong>。学习率越小，损失函数的变化速度越慢，意味着我们需要花费更长的时间进行收敛，如 <strong>图2</strong> 左图所示。</li>
<li><strong>学习率不是越大越好</strong>。只根据总样本集中的一个批次计算梯度，抽样误差会导致计算出的梯度不是全局最优的方向，且存在波动。在接近最优解时，过大的学习率会导致参数在最优解附近震荡，损失难以收敛，如 <strong>图2</strong> 右图所示。</li>
</ul>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/1e0f066dc9fa4e2bbc942447bdc0578c2ffc6afc15684154ae84bcf31b298d7b" srcset="/img/loading.gif" lazyload alt="img"></p>
<blockquote>
<p>在训练前，我们往往不清楚一个特定问题设置成怎样的学习率是合理的，因此在训练时可以尝试调小或调大，通过观察Loss下降的情况判断合理的学习率.</p>
</blockquote>
<h4 id="学习率的主流优化算法"><a href="#学习率的主流优化算法" class="headerlink" title="学习率的主流优化算法"></a>学习率的主流优化算法</h4><p>学习率是优化器的一个参数，调整学习率看似是一件非常麻烦的事情，需要不断的调整步长，观察训练时间和Loss的变化。经过研究员的不断的实验，当前已经形成了四种比较成熟的优化算法：SGD、Momentum、AdaGrad和Adam，效果如 <strong>图3</strong> 所示。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/f4cf80f95424411a85ad74998433317e721f56ddb4f64e6f8a28a27b6a1baa6b" srcset="/img/loading.gif" lazyload alt="img"></p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/SGD_cn.html">SGD：</a></strong> 随机梯度下降算法，每次训练少量数据，抽样偏差导致的参数收敛过程中震荡。</li>
<li><strong><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/Momentum_cn.html">Momentum：</a></strong> 引入物理“动量”的概念，累积速度，减少震荡，使参数更新的方向更稳定。</li>
</ul>
<p>每个批次的数据含有抽样误差，导致梯度更新的方向波动较大。如果我们引入物理动量的概念，给梯度下降的过程加入一定的“惯性”累积，就可以减少更新路径上的震荡，即每次更新的梯度由“历史多次梯度的累积方向”和“当次梯度”加权相加得到。历史多次梯度的累积方向往往是从全局视角更正确的方向，这与“惯性”的物理概念很像，也是为何其起名为“Momentum”的原因。类似不同品牌和材质的篮球有一定的重量差别，街头篮球队中的投手（擅长中远距离投篮）喜欢稍重篮球的比例较高。一个很重要的原因是，重的篮球惯性大，更不容易受到手势的小幅变形或风吹的影响。</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/AdagradOptimizer_cn.html">AdaGrad：</a></strong> 根据不同参数距离最优解的远近，动态调整学习率。学习率逐渐下降，依据各参数变化大小调整学习率。</li>
</ul>
<p>通过调整学习率的实验可以发现：当某个参数的现值距离最优解较远时（表现为梯度的绝对值较大），我们期望参数更新的步长大一些，以便更快收敛到最优解。当某个参数的现值距离最优解较近时（表现为梯度的绝对值较小），我们期望参数的更新步长小一些，以便更精细的逼近最优解。类似于打高尔夫球，专业运动员第一杆开球时，通常会大力打一个远球，让球尽量落在洞口附近。当第二杆面对离洞口较近的球时，他会更轻柔而细致的推杆，避免将球打飞。与此类似，参数更新的步长应该随着优化过程逐渐减少，减少的程度与当前梯度的大小有关。根据这个思想编写的优化算法称为“AdaGrad”，Ada是Adaptive的缩写，表示“适应环境而变化”的意思。<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/optimizer_cn/RMSPropOptimizer_cn.html#rmspropoptimizer">RMSProp</a>是在AdaGrad基础上的改进，学习率随着梯度变化而适应，解决AdaGrad学习率急剧下降的问题。</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/optimizer/Adam_cn.html">Adam：</a></strong> 由于动量和自适应学习率两个优化思路是正交的，因此可以将两个思路结合起来，这就是当前广泛应用的算法。</li>
</ul>
<h3 id="5-调试优化"><a href="#5-调试优化" class="headerlink" title="5.调试优化"></a>5.调试优化</h3><p>训练过程优化思路主要有如下五个关键环节：</p>
<p><strong>1. 计算分类准确率，观测模型训练效果。</strong></p>
<p>交叉熵损失函数只能作为优化目标，无法直接准确衡量模型的训练效果。准确率可以直接衡量训练效果，但由于其离散性质，不适合做为损失函数优化神经网络。</p>
<p><strong>2. 检查模型训练过程，识别潜在问题。</strong></p>
<p>如果模型的损失或者评估指标表现异常，通常需要打印模型每一层的输入和输出来定位问题，分析每一层的内容来获取错误的原因。</p>
<p><strong>3. 加入校验或测试，更好评价模型效果。</strong></p>
<p>理想的模型训练结果是在训练集和验证集上均有较高的准确率，如果训练集的准确率低于验证集，说明网络训练程度不够；如果训练集的准确率高于验证集，可能是发生了<strong>过拟合</strong>现象。通过在优化目标中加入正则化项的办法，解决过拟合的问题。</p>
<p><strong>4. 加入正则化项，避免模型过拟合。</strong></p>
<p>飞桨框架支持为整体参数加入正则化项，这是通常的做法。此外，飞桨框架也支持为某一层或某一部分的网络单独加入正则化项，以达到精细调整参数训练的效果。</p>
<p><strong>5. 可视化分析。</strong></p>
<p>用户不仅可以通过打印或使用matplotlib库作图，飞桨还提供了更专业的可视化分析工具VisualDL，提供便捷的可视化分析方法。</p>
<h4 id="计算模型的分类准确率"><a href="#计算模型的分类准确率" class="headerlink" title="计算模型的分类准确率"></a>计算模型的分类准确率</h4><p>准确率是一个直观衡量分类模型效果的指标，由于这个指标是离散的，因此不适合作为损失来优化。通常情况下，交叉熵损失越小的模型，分类的准确率也越高。基于分类准确率，我们可以公平地比较两种损失函数的优劣，例如在【手写数字识别】之损失函数章节中均方误差和交叉熵的比较。</p>
<p>使用飞桨提供的计算分类准确率API，可以直接计算准确率。</p>
<blockquote>
<p><em>class</em> paddle.metric.Accuracy</p>
</blockquote>
<p>该API的输入参数input为预测的分类结果predict，输入参数label为数据真实的label。飞桨还提供了更多衡量模型效果的计算指标，详细可以查看paddle.meric包下面的API。</p>
<p>在下述代码中，我们在模型前向计算过程forward函数中计算分类准确率，并在训练时打印每个批次样本的分类准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出</span><br>   <span class="hljs-comment"># 卷积层激活函数使用Relu，全连接层激活函数使用softmax</span><br>     <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, label</span>):<br>         x = self.conv1(inputs)<br>         x = F.relu(x)<br>         x = self.max_pool1(x)<br>         x = self.conv2(x)<br>         x = F.relu(x)<br>         x = self.max_pool2(x)<br>         x = paddle.reshape(x, [x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">980</span>])<br>         x = self.fc(x)<br>         <span class="hljs-keyword">if</span> label <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>             acc = paddle.metric.accuracy(<span class="hljs-built_in">input</span>=x, label=label)<br>             <span class="hljs-keyword">return</span> x, acc<br>         <span class="hljs-keyword">else</span>:<br>             <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    model = MNIST()<br>    model.train()<br>    <br>    <span class="hljs-comment">#四种优化算法的设置方案，可以逐一尝试效果</span><br>    <span class="hljs-comment"># opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())</span><br>    <span class="hljs-comment"># opt = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9, parameters=model.parameters())</span><br>    <span class="hljs-comment"># opt = paddle.optimizer.Adagrad(learning_rate=0.01, parameters=model.parameters())</span><br>    opt = paddle.optimizer.Adam(learning_rate=<span class="hljs-number">0.01</span>, parameters=model.parameters())<br>    <br>    EPOCH_NUM = <span class="hljs-number">2</span><br>    <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>            <span class="hljs-comment">#准备数据</span><br>            images, labels = data<br>            images = paddle.to_tensor(images)<br>            labels = paddle.to_tensor(labels)<br>            <br>            <span class="hljs-comment">#前向计算的过程</span><br>            predicts, acc = model(images, labels)<br>            <br>            <span class="hljs-comment">#计算损失，取一个批次样本损失的平均值</span><br>            loss = F.cross_entropy(predicts, labels)<br>            avg_loss = paddle.mean(loss)<br>            <br>            <span class="hljs-comment">#每训练了100批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;, acc is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))<br>                <br>            <span class="hljs-comment">#后向传播，更新参数，消除梯度的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br><br>    <span class="hljs-comment">#保存模型参数</span><br>    paddle.save(model.state_dict(), <span class="hljs-string">&#x27;mnist.pdparams&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="检查模型训练过程，识别潜在训练问题"><a href="#检查模型训练过程，识别潜在训练问题" class="headerlink" title="检查模型训练过程，识别潜在训练问题"></a>检查模型训练过程，识别潜在训练问题</h4><p>使用飞桨动态图编程可以方便的查看和调试训练的执行过程。在网络定义的Forward函数中，可以打印每一层输入输出的尺寸，以及每层网络的参数。通过查看这些信息，不仅可以更好地理解训练的执行过程，还可以发现潜在问题，或者启发继续优化的思路。</p>
<p>在下述程序中，使用<code>check_shape</code>变量控制是否打印“尺寸”，验证网络结构是否正确。使用<code>check_content</code>变量控制是否打印“内容值”，验证数据分布是否合理。假如在训练中发现中间层的部分输出持续为0，说明该部分的网络结构设计存在问题，没有充分利用。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#加入对每一层输入和输出的尺寸和数据内容的打印，根据check参数决策是否打印每层的参数和输出尺寸</span><br>     <span class="hljs-comment"># 卷积层激活函数使用Relu，全连接层激活函数使用softmax</span><br>     <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, label=<span class="hljs-literal">None</span>, check_shape=<span class="hljs-literal">False</span>, check_content=<span class="hljs-literal">False</span></span>):<br>         <span class="hljs-comment"># 给不同层的输出不同命名，方便调试</span><br>         outputs1 = self.conv1(inputs)<br>         outputs2 = F.relu(outputs1)<br>         outputs3 = self.max_pool1(outputs2)<br>         outputs4 = self.conv2(outputs3)<br>         outputs5 = F.relu(outputs4)<br>         outputs6 = self.max_pool2(outputs5)<br>         outputs6 = paddle.reshape(outputs6, [outputs6.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>])<br>         outputs7 = self.fc(outputs6)<br>         <br>         <span class="hljs-comment"># 选择是否打印神经网络每层的参数尺寸和输出尺寸，验证网络结构是否设置正确</span><br>         <span class="hljs-keyword">if</span> check_shape:<br>             <span class="hljs-comment"># 打印每层网络设置的超参数-卷积核尺寸，卷积步长，卷积padding，池化核尺寸</span><br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n########## print network layer&#x27;s superparams ##############&quot;</span>)<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;conv1-- kernel_size:&#123;&#125;, padding:&#123;&#125;, stride:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(self.conv1.weight.shape, self.conv1._padding, self.conv1._stride))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;conv2-- kernel_size:&#123;&#125;, padding:&#123;&#125;, stride:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(self.conv2.weight.shape, self.conv2._padding, self.conv2._stride))<br>             <span class="hljs-comment">#print(&quot;max_pool1-- kernel_size:&#123;&#125;, padding:&#123;&#125;, stride:&#123;&#125;&quot;.format(self.max_pool1.pool_size, self.max_pool1.pool_stride, self.max_pool1._stride))</span><br>             <span class="hljs-comment">#print(&quot;max_pool2-- kernel_size:&#123;&#125;, padding:&#123;&#125;, stride:&#123;&#125;&quot;.format(self.max_pool2.weight.shape, self.max_pool2._padding, self.max_pool2._stride))</span><br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;fc-- weight_size:&#123;&#125;, bias_size_&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(self.fc.weight.shape, self.fc.bias.shape))<br>             <br>             <span class="hljs-comment"># 打印每层的输出尺寸</span><br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n########## print shape of features of every layer ###############&quot;</span>)<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;inputs_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(inputs.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs1_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs1.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs2_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs2.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs3_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs3.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs4_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs4.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs5_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs5.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs6_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs6.shape))<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;outputs7_shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(outputs7.shape))<br>             <span class="hljs-comment"># print(&quot;outputs8_shape: &#123;&#125;&quot;.format(outputs8.shape))</span><br>             <br>         <span class="hljs-comment"># 选择是否打印训练过程中的参数和输出内容，可用于训练过程中的调试</span><br>         <span class="hljs-keyword">if</span> check_content:<br>            <span class="hljs-comment"># 打印卷积层的参数-卷积核权重，权重参数较多，此处只打印部分参数</span><br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n########## print convolution layer&#x27;s kernel ###############&quot;</span>)<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;conv1 params -- kernel weights:&quot;</span>, self.conv1.weight[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;conv2 params -- kernel weights:&quot;</span>, self.conv2.weight[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><br>             <span class="hljs-comment"># 创建随机数，随机打印某一个通道的输出值</span><br>             idx1 = np.random.randint(<span class="hljs-number">0</span>, outputs1.shape[<span class="hljs-number">1</span>])<br>             idx2 = np.random.randint(<span class="hljs-number">0</span>, outputs4.shape[<span class="hljs-number">1</span>])<br>             <span class="hljs-comment"># 打印卷积-池化后的结果，仅打印batch中第一个图像对应的特征</span><br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nThe &#123;&#125;th channel of conv1 layer: &quot;</span>.<span class="hljs-built_in">format</span>(idx1), outputs1[<span class="hljs-number">0</span>][idx1])<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The &#123;&#125;th channel of conv2 layer: &quot;</span>.<span class="hljs-built_in">format</span>(idx2), outputs4[<span class="hljs-number">0</span>][idx2])<br>             <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The output of last layer:&quot;</span>, outputs7[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;\n&#x27;</span>)<br>            <br>        <span class="hljs-comment"># 如果label不是None，则计算分类精度并返回</span><br>         <span class="hljs-keyword">if</span> label <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>             acc = paddle.metric.accuracy(<span class="hljs-built_in">input</span>=F.softmax(outputs7), label=label)<br>             <span class="hljs-keyword">return</span> outputs7, acc<br>         <span class="hljs-keyword">else</span>:<br>             <span class="hljs-keyword">return</span> outputs7<br></code></pre></td></tr></table></figure>

<h4 id="加入校验或测试，更好评价模型效果"><a href="#加入校验或测试，更好评价模型效果" class="headerlink" title="加入校验或测试，更好评价模型效果"></a>加入校验或测试，更好评价模型效果</h4><p>在训练过程中，我们会发现模型在训练样本集上的损失在不断减小。但这是否代表模型在未来的应用场景上依然有效？为了验证模型的有效性，通常将样本集合分成三份，训练集、校验集和测试集。</p>
<ul>
<li><strong>训练集</strong> ：用于训练模型的参数，即训练过程中主要完成的工作。</li>
<li><strong>校验集</strong> ：用于对模型超参数的选择，比如网络结构的调整、正则化项权重的选择等。</li>
<li><strong>测试集</strong> ：用于模拟模型在应用后的真实效果。因为测试集没有参与任何模型优化或参数训练的工作，所以它对模型来说是完全未知的样本。在不以校验数据优化网络结构或模型超参数时，校验数据和测试数据的效果是类似的，均更真实的反映模型效果。</li>
</ul>
<p>如下程序读取上一步训练保存的模型参数，读取校验数据集，并测试模型在校验数据集上的效果。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs"><br></code></pre></td></tr></table></figure>

<h4 id="加入正则化项，避免模型过拟合"><a href="#加入正则化项，避免模型过拟合" class="headerlink" title="加入正则化项，避免模型过拟合"></a>加入正则化项，避免模型过拟合</h4><h5 id="过拟合现象"><a href="#过拟合现象" class="headerlink" title="过拟合现象"></a>过拟合现象</h5><p>对于样本量有限、但需要使用强大模型的复杂任务，模型很容易出现过拟合的表现，即在训练集上的损失小，在验证集或测试集上的损失较大，如 <strong>图2</strong> 所示。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/99b879c21113494a9d7315eeda74bc4c8fea07f984824a03bf8411e946c75f1b" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>反之，如果模型在训练集和测试集上均损失较大，则称为欠拟合。过拟合表示模型过于敏感，学习到了训练数据中的一些误差，而这些误差并不是真实的泛化规律（可推广到测试集上的规律）。欠拟合表示模型还不够强大，还没有很好的拟合已知的训练样本，更别提测试样本了。因为欠拟合情况容易观察和解决，只要训练loss不够好，就不断使用更强大的模型即可，因此实际中我们更需要处理好过拟合的问题。</p>
<h5 id="导致过拟合原因"><a href="#导致过拟合原因" class="headerlink" title="导致过拟合原因"></a>导致过拟合原因</h5><p>造成过拟合的原因是模型过于敏感，而训练数据量太少或其中的噪音太多。</p>
<p>如<strong>图3</strong> 所示，理想的回归模型是一条坡度较缓的抛物线，欠拟合的模型只拟合出一条直线，显然没有捕捉到真实的规律，但过拟合的模型拟合出存在很多拐点的抛物线，显然是过于敏感，也没有正确表达真实规律。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/53c389bb3c824706bd2fbc05f83ab0c6dd6b5b2fdedb4150a17e16a1b64c243e" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>如<strong>图4</strong> 所示，理想的分类模型是一条半圆形的曲线，欠拟合用直线作为分类边界，显然没有捕捉到真实的边界，但过拟合的模型拟合出很扭曲的分类边界，虽然对所有的训练数据正确分类，但对一些较为个例的样本所做出的妥协，高概率不是真实的规律。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/b5a46f7e0fbe4f8686a71d9a2d330ed09f23bca565a44e0d941148729fd2f7d7" srcset="/img/loading.gif" lazyload alt="img"></p>
<h5 id="过拟合的成因与防控"><a href="#过拟合的成因与防控" class="headerlink" title="过拟合的成因与防控"></a>过拟合的成因与防控</h5><p>为了更好的理解过拟合的成因，可以参考侦探定位罪犯的案例逻辑，如 <strong>图5</strong> 所示。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/34de60a675b64468a2c3fee0844a168d53e891eaacf643fd8c1c9ba8e3812bcc" srcset="/img/loading.gif" lazyload alt="img"></p>
<p><strong>对于这个案例，假设侦探也会犯错，通过分析发现可能的原因：</strong></p>
<ol>
<li>情况1：罪犯证据存在错误，依据错误的证据寻找罪犯肯定是缘木求鱼。</li>
<li>情况2：搜索范围太大的同时证据太少，导致符合条件的候选（嫌疑人）太多，无法准确定位罪犯。</li>
</ol>
<p><strong>那么侦探解决这个问题的方法有两种：或者缩小搜索范围（比如假设该案件只能是熟人作案），或者寻找更多的证据。</strong></p>
<p><strong>归结到深度学习中，假设模型也会犯错，通过分析发现可能的原因：</strong></p>
<ol>
<li>情况1：训练数据存在噪音，导致模型学到了噪音，而不是真实规律。</li>
<li>情况2：使用强大模型（表示空间大）的同时训练数据太少，导致在训练数据上表现良好的候选假设太多，锁定了一个“虚假正确”的假设。</li>
</ol>
<p><strong>对于情况1，我们使用数据清洗和修正来解决。 对于情况2，我们或者限制模型表示能力，或者收集更多的训练数据。</strong></p>
<p>而清洗训练数据中的错误，或收集更多的训练数据往往是一句“正确的废话”，在任何时候我们都想获得更多更高质量的数据。在实际项目中，更快、更低成本可控制过拟合的方法，只有限制模型的表示能力。</p>
<h5 id="正则化项"><a href="#正则化项" class="headerlink" title="正则化项"></a>正则化项</h5><p>为了防止模型过拟合，在没有扩充样本量的可能下，只能降低模型的复杂度，可以通过限制参数的数量或可能取值（参数值尽量小）实现。</p>
<p>具体来说，在模型的优化目标（损失）中人为加入对参数规模的惩罚项。当参数越多或取值越大时，该惩罚项就越大。通过调整惩罚项的权重系数，可以使模型在“尽量减少训练损失”和“保持模型的泛化能力”之间取得平衡。泛化能力表示模型在没有见过的样本上依然有效。正则化项的存在，增加了模型在训练集上的损失。</p>
<p>飞桨支持为所有参数加上统一的正则化项，也支持为特定的参数添加正则化项。前者的实现如下代码所示，仅在优化器中设置<code>weight_decay</code>参数即可实现。使用参数<code>coeff</code>调节正则化项的权重，权重越大时，对模型复杂度的惩罚越高。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    model.train() <br><br>    <span class="hljs-comment">#各种优化算法均可以加入正则化项，避免过拟合，参数regularization_coeff调节正则化项的权重</span><br>    opt = paddle.optimizer.Adam(learning_rate=<span class="hljs-number">0.01</span>, weight_decay=paddle.regularizer.L2Decay(coeff=<span class="hljs-number">1e-5</span>), parameters=model.parameters())           <br><br>    EPOCH_NUM = <span class="hljs-number">5</span><br>    <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>            <span class="hljs-comment">#准备数据，变得更加简洁</span><br>            images, labels = data<br>            images = paddle.to_tensor(images)<br>            labels = paddle.to_tensor(labels)<br>            <br>            <span class="hljs-comment">#前向计算的过程，同时拿到模型输出值和分类准确率</span><br>            predicts, acc = model(images, labels)<br>            <span class="hljs-comment">#计算损失，取一个批次样本损失的平均值</span><br>            loss = F.cross_entropy(predicts, labels)<br>            avg_loss = paddle.mean(loss)<br>            <br>            <span class="hljs-comment">#每训练了100批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;, acc is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))<br>            <br>            <span class="hljs-comment">#后向传播，更新参数的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br><br>    <span class="hljs-comment">#保存模型参数</span><br>    paddle.save(model.state_dict(), <span class="hljs-string">&#x27;mnist_regul.pdparams&#x27;</span>)<br><br>model = MNIST()<br>train(model)<br></code></pre></td></tr></table></figure>

<h3 id="6-可视化分析"><a href="#6-可视化分析" class="headerlink" title="6.可视化分析"></a>6.可视化分析</h3><p>训练模型时，经常需要观察模型的评价指标，分析模型的优化过程，以确保训练是有效的。可选用这两种工具：Matplotlib库和VisualDL。</p>
<ul>
<li><strong>Matplotlib库</strong>：Matplotlib库是Python中使用的最多的2D图形绘图库，它有一套完全仿照MATLAB的函数形式的绘图接口，使用轻量级的PLT库（Matplotlib）作图是非常简单的。</li>
<li><strong>VisualDL</strong>：如果期望使用更加专业的作图工具，可以尝试VisualDL，飞桨可视化分析工具。VisualDL能够有效地展示飞桨在运行过程中的计算图、各种指标变化趋势和数据信息。</li>
</ul>
<h4 id="使用Matplotlib库绘制损失随训练下降的曲线图"><a href="#使用Matplotlib库绘制损失随训练下降的曲线图" class="headerlink" title="使用Matplotlib库绘制损失随训练下降的曲线图"></a>使用Matplotlib库绘制损失随训练下降的曲线图</h4><p>将训练的批次编号作为X轴坐标，该批次的训练损失作为Y轴坐标。</p>
<ol>
<li>训练开始前，声明两个列表变量存储对应的批次编号(iters&#x3D;[])和训练损失(losses&#x3D;[])。</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs py">iters=[]<br>losses=[]<br><span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>	<span class="hljs-string">&quot;&quot;&quot;start to training&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>

<ol>
<li>随着训练的进行，将iter和losses两个列表填满。</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><br>iters=[]<br>losses=[]<br><span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>	<span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>        images, labels = data<br>        predicts, acc = model(images, labels)<br>        loss = F.cross_entropy(predicts, label = labels.astype(<span class="hljs-string">&#x27;int64&#x27;</span>))<br>        avg_loss = paddle.mean(loss)<br>        <span class="hljs-comment"># 累计迭代次数和对应的loss</span><br>   	iters.append(batch_id + epoch_id*<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(train_loader()))<br>	losses.append(avg_loss)<br></code></pre></td></tr></table></figure>

<ol>
<li>训练结束后，将两份数据以参数形式导入PLT的横纵坐标。</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs py">plt.xlabel(<span class="hljs-string">&quot;iter&quot;</span>, fontsize=<span class="hljs-number">14</span>)，plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-number">14</span>)<br></code></pre></td></tr></table></figure>

<ol>
<li>最后，调用plt.plot()函数即可完成作图。</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs py">plt.plot(iters, losses,color=<span class="hljs-string">&#x27;red&#x27;</span>,label=<span class="hljs-string">&#x27;train loss&#x27;</span>) <br></code></pre></td></tr></table></figure>

<p>详细代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#引入matplotlib库</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    model.train()<br>    <br>    opt = paddle.optimizer.Adam(learning_rate=<span class="hljs-number">0.001</span>, parameters=model.parameters())<br>    <br>    EPOCH_NUM = <span class="hljs-number">10</span><br>    <span class="hljs-built_in">iter</span>=<span class="hljs-number">0</span><br>    iters=[]<br>    losses=[]<br>    <span class="hljs-keyword">for</span> epoch_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>            <span class="hljs-comment">#准备数据，变得更加简洁</span><br>            images, labels = data<br>            images = paddle.to_tensor(images)<br>            labels = paddle.to_tensor(labels)<br>            <br>            <span class="hljs-comment">#前向计算的过程，同时拿到模型输出值和分类准确率</span><br>            predicts, acc = model(images, labels)<br>            <span class="hljs-comment">#计算损失，取一个批次样本损失的平均值</span><br>            loss = F.cross_entropy(predicts, labels)<br>            avg_loss = paddle.mean(loss)<br>            <br>            <span class="hljs-comment">#每训练了100批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;, acc is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))<br>                iters.append(<span class="hljs-built_in">iter</span>)<br>                losses.append(avg_loss.numpy())<br>                <span class="hljs-built_in">iter</span> = <span class="hljs-built_in">iter</span> + <span class="hljs-number">100</span><br>            <br>            <span class="hljs-comment">#后向传播，更新参数的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br>            <br>    <span class="hljs-comment">#保存模型参数</span><br>    paddle.save(model.state_dict(), <span class="hljs-string">&#x27;mnist.pdparams&#x27;</span>)<br>    <br>    <span class="hljs-keyword">return</span> iters, losses<br>    <br>model = MNIST()<br>iters, losses = train(model)<br></code></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#画出训练过程中Loss的变化曲线</span><br>plt.figure()<br>plt.title(<span class="hljs-string">&quot;train loss&quot;</span>, fontsize=<span class="hljs-number">24</span>)<br>plt.xlabel(<span class="hljs-string">&quot;iter&quot;</span>, fontsize=<span class="hljs-number">14</span>)<br>plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-number">14</span>)<br>plt.plot(iters, losses,color=<span class="hljs-string">&#x27;red&#x27;</span>,label=<span class="hljs-string">&#x27;train loss&#x27;</span>) <br>plt.grid()<br>plt.show()<br></code></pre></td></tr></table></figure>

<h4 id="使用VisualDL可视化分析"><a href="#使用VisualDL可视化分析" class="headerlink" title="使用VisualDL可视化分析"></a>使用VisualDL可视化分析</h4><p>VisualDL是飞桨可视化分析工具，以丰富的图表呈现训练参数变化趋势、模型结构、数据样本、高维数据分布等。帮助用户清晰直观地理解深度学习模型训练过程及模型结构，进而实现高效的模型调优，具体代码实现如下。</p>
<ul>
<li>步骤1：引入VisualDL库，定义作图数据存储位置（供第3步使用），本案例的路径是“log”。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">from</span> visualdl <span class="hljs-keyword">import</span> LogWriter<br>log_writer = LogWriter(<span class="hljs-string">&quot;./log&quot;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>步骤2：在训练过程中插入作图语句。当每100个batch训练完成后，将当前损失作为一个新增的数据点(iter和acc的映射对)存储到第一步设置的文件中。使用变量iter记录下已经训练的批次数，作为作图的X轴坐标。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">log_writer.add_scalar(tag = <span class="hljs-string">&#x27;acc&#x27;</span>, step = <span class="hljs-built_in">iter</span>, value = avg_acc.numpy())<br>log_writer.add_scalar(tag = <span class="hljs-string">&#x27;loss&#x27;</span>, step = <span class="hljs-built_in">iter</span>, value = avg_loss.numpy())<br><span class="hljs-built_in">iter</span> = <span class="hljs-built_in">iter</span> + <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure>

<h3 id="7-模型加载及恢复训练"><a href="#7-模型加载及恢复训练" class="headerlink" title="7.模型加载及恢复训练"></a>7.模型加载及恢复训练</h3><p>在快速入门中，我们已经介绍了将训练好的模型保存到磁盘文件的方法。应用程序可以随时加载模型，完成预测任务。但是在日常训练工作中我们会遇到一些突发情况，导致训练过程主动或被动的中断。如果训练一个模型需要花费几天的训练时间，中断后从初始状态重新训练是不可接受的。</p>
<p>万幸的是，飞桨支持从上一次保存状态开始训练，只要我们随时保存训练过程中的模型状态，就不用从初始状态重新训练。</p>
<p>下面介绍恢复训练的实现方法，依然使用手写数字识别的案例，网络定义的部分保持不变。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#数据处理部分之前的代码，保持不变</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">import</span> gzip<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> warnings <br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> paddle.nn <span class="hljs-keyword">import</span> Conv2D, MaxPool2D, Linear<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># 创建一个类MnistDataset，继承paddle.io.Dataset 这个类</span><br><span class="hljs-comment"># MnistDataset的作用和上面load_data()函数的作用相同，均是构建一个迭代器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MnistDataset</span>(paddle.io.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, mode</span>):<br>        datafile = <span class="hljs-string">&#x27;./work/mnist.json.gz&#x27;</span><br>        data = json.load(gzip.<span class="hljs-built_in">open</span>(datafile))<br>        <span class="hljs-comment"># 读取到的数据区分训练集，验证集，测试集</span><br>        train_set, val_set, eval_set = data<br>        <br>        <span class="hljs-comment"># 数据集相关参数，图片高度IMG_ROWS, 图片宽度IMG_COLS</span><br>        self.IMG_ROWS = <span class="hljs-number">28</span><br>        self.IMG_COLS = <span class="hljs-number">28</span><br><br>        <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;train&#x27;</span>:<br>            <span class="hljs-comment"># 获得训练数据集</span><br>            imgs, labels = train_set[<span class="hljs-number">0</span>], train_set[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&#x27;valid&#x27;</span>:<br>            <span class="hljs-comment"># 获得验证数据集</span><br>            imgs, labels = val_set[<span class="hljs-number">0</span>], val_set[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&#x27;eval&#x27;</span>:<br>            <span class="hljs-comment"># 获得测试数据集</span><br>            imgs, labels = eval_set[<span class="hljs-number">0</span>], eval_set[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;mode can only be one of [&#x27;train&#x27;, &#x27;valid&#x27;, &#x27;eval&#x27;]&quot;</span>)<br>        <br>        <span class="hljs-comment"># 校验数据</span><br>        imgs_length = <span class="hljs-built_in">len</span>(imgs)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(imgs) == <span class="hljs-built_in">len</span>(labels), \<br>            <span class="hljs-string">&quot;length of train_imgs(&#123;&#125;) should be the same as train_labels(&#123;&#125;)&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(imgs), <span class="hljs-built_in">len</span>(labels))<br>        <br>        self.imgs = imgs<br>        self.labels = labels<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-comment"># MLP</span><br>        <span class="hljs-comment"># img = np.array(self.imgs[idx]).astype(&#x27;float32&#x27;)</span><br>        <span class="hljs-comment"># label = np.array(self.labels[idx]).astype(&#x27;int64&#x27;)</span><br>        <span class="hljs-comment"># CNN</span><br>        img = np.reshape(self.imgs[idx], [<span class="hljs-number">1</span>, self.IMG_ROWS, self.IMG_COLS]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        label = np.reshape(self.labels[idx], [<span class="hljs-number">1</span>]).astype(<span class="hljs-string">&#x27;int64&#x27;</span>)<br><br>        <span class="hljs-keyword">return</span> img, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.imgs)<br><br><span class="hljs-comment"># 定义模型结构</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MNIST</span>(paddle.nn.Layer):<br>     <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>         <span class="hljs-built_in">super</span>(MNIST, self).__init__()<br>         nn.initializer.set_global_initializer(nn.initializer.Uniform(), nn.initializer.Constant())<br>         <span class="hljs-comment"># 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2</span><br>         self.conv1 = Conv2D(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">20</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)<br>         <span class="hljs-comment"># 定义池化层，池化层卷积核kernel_size为2，池化步长为2</span><br>         self.max_pool1 = MaxPool2D(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>         <span class="hljs-comment"># 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2</span><br>         self.conv2 = Conv2D(in_channels=<span class="hljs-number">20</span>, out_channels=<span class="hljs-number">20</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)<br>         <span class="hljs-comment"># 定义池化层，池化层卷积核kernel_size为2，池化步长为2</span><br>         self.max_pool2 = MaxPool2D(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>         <span class="hljs-comment"># 定义一层全连接层，输出维度是10</span><br>         self.fc = Linear(in_features=<span class="hljs-number">980</span>, out_features=<span class="hljs-number">10</span>)<br>         <br>    <span class="hljs-comment"># 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出</span><br>     <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>         x = self.conv1(inputs)<br>         x = F.relu(x)<br>         x = self.max_pool1(x)<br>         x = self.conv2(x)<br>         x = F.relu(x)<br>         x = self.max_pool2(x)<br>         x = paddle.reshape(x, [x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">980</span>])<br>         x = self.fc(x)<br>        <span class="hljs-comment">#  x = F.softmax(x)</span><br>         <span class="hljs-keyword">return</span> x         <br></code></pre></td></tr></table></figure>

<p>定义训练Trainer，包含训练过程和模型保存</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path, model, optimizer</span>):<br>        self.model_path = model_path   <span class="hljs-comment"># 模型存放路径 </span><br>        self.model = model             <span class="hljs-comment"># 定义的模型</span><br>        self.optimizer = optimizer     <span class="hljs-comment"># 优化器</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 保存模型</span><br>        paddle.save(self.model.state_dict(), self.model_path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_step</span>(<span class="hljs-params">self, data</span>):<br>        images, labels = data<br>        <span class="hljs-comment"># 前向计算的过程</span><br>        predicts = self.model(images)<br>        <span class="hljs-comment"># 计算损失</span><br>        loss = F.cross_entropy(predicts, labels)<br>        avg_loss = paddle.mean(loss)<br>        <span class="hljs-comment"># 后向传播，更新参数的过程</span><br>        avg_loss.backward()<br>        self.optimizer.step()<br>        self.optimizer.clear_grad()<br>        <span class="hljs-keyword">return</span> avg_loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch</span>(<span class="hljs-params">self, datasets, epoch</span>):<br>        self.model.train()<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(datasets()):<br>            loss = self.train_step(data)<br>            <span class="hljs-comment"># 每训练了1000批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">500</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch_id: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, batch_id, loss.numpy()))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_datasets, start_epoch, end_epoch, save_path</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(save_path):<br>            os.makedirs(save_path)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_epoch, end_epoch):<br>            self.train_epoch(train_datasets, i)<br>            paddle.save(opt.state_dict(), <span class="hljs-string">&#x27;./&#123;&#125;/mnist_epoch&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(save_path,i)+<span class="hljs-string">&#x27;.pdopt&#x27;</span>)<br>            paddle.save(model.state_dict(), <span class="hljs-string">&#x27;./&#123;&#125;/mnist_epoch&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(save_path,i)+<span class="hljs-string">&#x27;.pdparams&#x27;</span>)<br>        self.save()<br></code></pre></td></tr></table></figure>

<h4 id="恢复训练"><a href="#恢复训练" class="headerlink" title="恢复训练"></a>恢复训练</h4><p><strong>模型恢复训练，需要重新组网，所以我们需要重启AIStudio，运行<code>MnistDataset</code>数据读取和<code>MNIST</code>网络定义、<code>Trainer</code>部分代码，再执行模型恢复代码</strong></p>
<p>在上述训练代码中，我们训练了五轮（epoch）。在每轮结束时，我们均保存了模型参数和优化器相关的参数。</p>
<ul>
<li>使用<code>model.state_dict()</code>获取模型参数。</li>
<li>使用<code>opt.state_dict</code>获取优化器和学习率相关的参数。</li>
<li>调用<code>paddle.save</code>将参数保存到本地。</li>
</ul>
<p>比如第一轮训练保存的文件是mnist_epoch0.pdparams，mnist_epoch0.pdopt，分别存储了模型参数和优化器参数。</p>
<p>使用<code>paddle.load</code>分别加载模型参数和优化器参数，如下代码所示。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">paddle.<span class="hljs-built_in">load</span>(params_path+<span class="hljs-string">&#x27;.pdparams&#x27;</span>)<br>paddle.<span class="hljs-built_in">load</span>(params_path+<span class="hljs-string">&#x27;.pdopt&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>如何判断模型是否准确的恢复训练呢？</p>
<p>理想的恢复训练是模型状态回到训练中断的时刻，恢复训练之后的梯度更新走向是和恢复训练前的梯度走向完全相同的。基于此，我们可以通过恢复训练后的损失变化，判断上述方法是否能准确的恢复训练。即从epoch 0结束时保存的模型参数和优化器状态恢复训练，校验其后训练的损失变化（epoch 1）是否和不中断时的训练相差不多。</p>
<hr>
<p><strong>说明：</strong></p>
<p>恢复训练有如下两个要点：</p>
<ul>
<li>保存模型时分别保存模型参数和优化器参数。</li>
<li>恢复参数时分别恢复模型参数和优化器参数。</li>
</ul>
<hr>
<p>下面的代码将展示恢复训练的过程，并验证恢复训练是否成功。加载模型参数并从第一个epoch开始训练，以便读者可以校验恢复训练后的损失变化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><span class="hljs-comment"># MLP继续训练</span><br>paddle.seed(<span class="hljs-number">1024</span>)<br><br>epochs = <span class="hljs-number">3</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br>model_path = <span class="hljs-string">&#x27;./mnist_retrain.pdparams&#x27;</span><br><br>train_dataset = MnistDataset(mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-comment"># 这里为了使每次的训练精度都保持一致，因此先选择了shuffle=False，真正训练时应改为shuffle=True</span><br>train_loader = paddle.io.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">4</span>) <br><br>model = MNIST()<br><span class="hljs-comment"># lr = 0.01</span><br>total_steps = (<span class="hljs-built_in">int</span>(<span class="hljs-number">50000</span>//BATCH_SIZE) + <span class="hljs-number">1</span>) * epochs<br>lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=<span class="hljs-number">0.01</span>, decay_steps=total_steps, end_lr=<span class="hljs-number">0.001</span>)<br>opt = paddle.optimizer.Momentum(learning_rate=lr, parameters=model.parameters())<br><br>params_dict = paddle.load(<span class="hljs-string">&#x27;./checkpoint/mnist_epoch0.pdparams&#x27;</span>)<br>opt_dict = paddle.load(<span class="hljs-string">&#x27;./checkpoint/mnist_epoch0.pdopt&#x27;</span>)<br><br><span class="hljs-comment"># 加载参数到模型</span><br>model.set_state_dict(params_dict)<br>opt.set_state_dict(opt_dict)<br><br>trainer = Trainer(<br>    model_path=model_path,<br>    model=model,<br>    optimizer=opt<br>)<br><span class="hljs-comment"># 前面训练模型都保存了，这里save_path设置为新路径，实际训练中保存在同一目录就可以</span><br>trainer.train(train_datasets=train_loader,start_epoch = <span class="hljs-number">1</span>, end_epoch = epochs, save_path=<span class="hljs-string">&#x27;checkpoint_con&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>从恢复训练的损失变化来看，加载模型参数继续训练的损失函数值和正常训练损失函数值是相差不多的，可见使用飞桨实现恢复训练是极其简单的。 总结一下：</p>
<ul>
<li>保存模型时同时保存模型参数和优化器参数；</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs py">paddle.save(opt.state_dict(), <span class="hljs-string">&#x27;model.pdopt&#x27;</span>)<br>paddle.save(model.state_dict(), <span class="hljs-string">&#x27;model.pdparams&#x27;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>恢复参数时同时恢复模型参数和优化器参数。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs py">model_dict = paddle.load(<span class="hljs-string">&quot;model.pdparams&quot;</span>)<br>opt_dict = paddle.load(<span class="hljs-string">&quot;model.pdopt&quot;</span>)<br><br>model.set_state_dict(model_dict)<br>opt.set_state_dict(opt_dict)<br></code></pre></td></tr></table></figure>

<h3 id="8-动静转换"><a href="#8-动静转换" class="headerlink" title="8.动静转换"></a>8.动静转换</h3><p>动态图有诸多优点，比如易用的接口、Python风格的编程体验、友好的调试交互机制等。在动态图模式下，代码可以按照我们编写的顺序依次执行。这种机制更符合Python程序员的使用习惯，可以很方便地将脑海中的想法快速地转化为实际代码，也更容易调试。</p>
<p>但在性能方面，由于Python执行开销较大，与C++有一定差距，因此在工业界的许多部署场景中（如大型推荐系统、移动端）都倾向于直接使用C++进行提速。相比动态图，静态图在部署方面更具有性能的优势。静态图程序在编译执行时，先搭建模型的神经网络结构，然后再对神经网络执行计算操作。预先搭建好的神经网络可以脱离Python依赖，在C++端被重新解析执行，而且拥有整体网络结构也能进行一些网络结构的优化。</p>
<p>那么，有没有可能，深度学习框架实现一个新的模式，同时具备动态图高易用性与静态图高性能的特点呢？飞桨从2.0版本开始，新增新增支持动静转换功能，编程范式的选择更加灵活。用户依然使用动态图编写代码，只需添加一行装饰器 @paddle.jit.to_static，即可实现动态图转静态图模式运行，进行模型训练或者推理部署。在本章节中，将介绍飞桨动态图转静态图的基本用法和相关原理。</p>
<h4 id="动态图转静态图训练"><a href="#动态图转静态图训练" class="headerlink" title="动态图转静态图训练"></a>动态图转静态图训练</h4><p>飞桨的动转静方式是基于源代码级别转换的ProgramTranslator实现，其原理是通过分析Python代码，将动态图代码转写为静态图代码，并在底层自动使用静态图执行器运行。其基本使用方法十分简便，只需要在要转化的函数（该函数也可以是用户自定义动态图Layer的forward函数）前添加一个装饰器 @paddle.jit.to_static。这种转换方式使得用户可以灵活使用Python语法及其控制流来构建神经网络模型。下面通过一个例子说明如何使用飞桨实现动态图转静态图训练。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-comment"># 定义手写数字识别模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MNIST</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MNIST, self).__init__()<br>        <br>        <span class="hljs-comment"># 定义一层全连接层，输出维度是1</span><br>        self.fc = paddle.nn.Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">10</span>)<br><br>    <span class="hljs-comment"># 定义网络结构的前向计算过程</span><br><span class="hljs-meta">    @paddle.jit.to_static  </span><span class="hljs-comment"># 添加装饰器，使动态图网络结构在静态图模式下运行</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc(inputs)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>

<p>上述代码构建了仅有一层全连接层的手写字符识别网络。特别注意，在forward函数之前加了装饰器<code>@paddle.jit.to_static</code>，要求模型在静态图模式下运行。下面是模型的训练代码，由于飞桨实现动转静的功能是在内部完成的，对使用者来说，动态图的训练代码和动转静模型的训练代码是完全一致的。训练代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment"># 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型</span><br>paddle.vision.set_image_backend(<span class="hljs-string">&#x27;cv2&#x27;</span>)<br><br><span class="hljs-comment"># 图像归一化函数，将数据范围为[0, 255]的图像归一化到[-1, 1]</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">norm_img</span>(<span class="hljs-params">img</span>):<br>    batch_size = img.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># 归一化图像数据</span><br>    img = img/<span class="hljs-number">127.5</span> - <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 将图像形式reshape为[batch_size, 784]</span><br>    img = paddle.reshape(img, [batch_size, <span class="hljs-number">784</span>])<br>    <br>    <span class="hljs-keyword">return</span> img<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model</span>):<br>    model.train()<br>    <span class="hljs-comment"># 加载训练集 batch_size 设为 16</span><br>    train_loader = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="hljs-string">&#x27;train&#x27;</span>), <br>                                        batch_size=<span class="hljs-number">16</span>, <br>                                        shuffle=<span class="hljs-literal">True</span>)<br>    opt = paddle.optimizer.SGD(learning_rate=<span class="hljs-number">0.001</span>, parameters=model.parameters())<br>    EPOCH_NUM = <span class="hljs-number">10</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH_NUM):<br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader()):<br>            images = norm_img(data[<span class="hljs-number">0</span>]).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            labels = data[<span class="hljs-number">1</span>].astype(<span class="hljs-string">&#x27;int64&#x27;</span>)<br>            <br>            <span class="hljs-comment">#前向计算的过程</span><br>            predicts = model(images)<br>            <br>            <span class="hljs-comment"># 计算损失</span><br>            loss = F.cross_entropy(predicts, labels)<br>            avg_loss = paddle.mean(loss)<br>            <br>            <span class="hljs-comment">#每训练了1000批次的数据，打印下当前Loss的情况</span><br>            <span class="hljs-keyword">if</span> batch_id % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch_id: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, batch_id, avg_loss.numpy()))<br>            <br>            <span class="hljs-comment">#后向传播，更新参数的过程</span><br>            avg_loss.backward()<br>            opt.step()<br>            opt.clear_grad()<br><br><br>model = MNIST() <br><br>train(model)<br><br>paddle.save(model.state_dict(), <span class="hljs-string">&#x27;./mnist.pdparams&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;==&gt;Trained model saved in ./mnist.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>我们可以观察到，动转静的训练方式与动态图训练代码是完全相同的。因此，在动转静训练的时候，开发者只需要在动态图的组网前向计算函数上添加一个装饰器即可实现动转静训练。 在模型构建和训练中，飞桨更希望借用动态图的易用性优势，实际上，在加上@to_static装饰器运行的时候，飞桨内部是在静态图模式下执行OP的，但是展示给开发者的依然是动态图的使用方式。</p>
<p>动转静更能体现静态图的方面在于模型部署上。下面将介绍动态图转静态图的部署方式。</p>
<h4 id="动态图转静态图模型保存"><a href="#动态图转静态图模型保存" class="headerlink" title="动态图转静态图模型保存"></a>动态图转静态图模型保存</h4><p>在推理&amp;部署场景中，需要同时保存推理模型的结构和参数，但是动态图是即时执行即时得到结果，并不会记录模型的结构信息。动态图在保存推理模型时，需要先将动态图模型转换为静态图写法，编译得到对应的模型结构再保存，而飞桨框架2.0版本推出paddle.jit.save和paddle.jit.load接口，无需重新实现静态图网络结构，直接实现动态图模型转成静态图模型格式。paddle.jit.save接口会自动调用飞桨框架2.0推出的动态图转静态图功能，使得用户可以做到使用动态图编程调试，自动转成静态图训练部署。</p>
<p>这两个接口的基本关系如下图所示：</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/018ac3d24c22423a8a263dfd0f0f7f49898b29e707c14dbdb8f9f5abdde75449" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>当用户使用paddle.jit.save保存Layer对象时，飞桨会自动将用户编写的动态图Layer模型转换为静态图写法，并编译得到模型结构，同时将模型结构与参数保存。paddle.jit.save需要适配飞桨沿用已久的推理模型与参数格式，做到前向完全兼容，因此其保存格式与paddle.save有所区别，具体包括三种文件：保存模型结构的*.pdmodel文件；保存推理用参数的*.pdiparams文件和保存兼容变量信息的*.pdiparams.info文件，这几个文件后缀均为paddle.jit.save保存时默认使用的文件后缀。</p>
<p>比如，如果保存上述手写字符识别的inference模型用于部署，可以直接用下面代码实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># save inference model</span><br><span class="hljs-keyword">from</span> paddle.static <span class="hljs-keyword">import</span> InputSpec<br><span class="hljs-comment"># 加载训练好的模型参数</span><br>state_dict = paddle.load(<span class="hljs-string">&quot;./mnist.pdparams&quot;</span>)<br><span class="hljs-comment"># 将训练好的参数读取到网络中</span><br>model.set_state_dict(state_dict)<br><span class="hljs-comment"># 设置模型为评估模式</span><br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># 保存inference模型</span><br>paddle.jit.save(<br>    layer=model,<br>    path=<span class="hljs-string">&quot;inference/mnist&quot;</span>,<br>    input_spec=[InputSpec(shape=[<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>], dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;==&gt;Inference model saved in inference/mnist.&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>其中，paddle.jit.save API 将输入的网络存储为 paddle.jit.TranslatedLayer 格式的模型，载入后可用于预测推理或者fine-tune训练。 该接口会将输入网络转写后的模型结构 Program 和所有必要的持久参数变量存储至输入路径 path 。</p>
<p>path 是存储目标的前缀，存储的模型结构 Program 文件的后缀为 .pdmodel ，存储的持久参数变量文件的后缀为 .pdiparams ，同时这里也会将一些变量描述信息存储至文件，文件后缀为 .pdiparams.info。</p>
<p>通过调用对应的paddle.jit.load接口，可以把存储的模型载入为 paddle.jit.TranslatedLayer格式，用于预测推理或者fine-tune训练。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment"># 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型</span><br>paddle.vision.set_image_backend(<span class="hljs-string">&#x27;cv2&#x27;</span>)<br><br><span class="hljs-comment"># 读取mnist测试数据，获取第一个数据</span><br>mnist_test = paddle.vision.datasets.MNIST(mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br>test_image, label = mnist_test[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># 获取读取到的图像的数字标签</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The label of readed image is : &quot;</span>, label)<br><br><span class="hljs-comment"># 将测试图像数据转换为tensor，并reshape为[1, 784]</span><br>test_image = paddle.reshape(paddle.to_tensor(test_image), [<span class="hljs-number">1</span>, <span class="hljs-number">784</span>])<br><span class="hljs-comment"># 然后执行图像归一化</span><br>test_image = norm_img(test_image)<br><span class="hljs-comment"># 加载保存的模型</span><br>loaded_model = paddle.jit.load(<span class="hljs-string">&quot;./inference/mnist&quot;</span>)<br><span class="hljs-comment"># 利用加载的模型执行预测</span><br>preds = loaded_model(test_image)<br>pred_label = paddle.argmax(preds)<br><span class="hljs-comment"># 打印预测结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The predicted label is : &quot;</span>, pred_label.numpy())<br>The label of readed image <span class="hljs-keyword">is</span> :  [<span class="hljs-number">7</span>]<br>The predicted label <span class="hljs-keyword">is</span> :  [<span class="hljs-number">7</span>]<br></code></pre></td></tr></table></figure>

<p>paddle.jit.save API 可以把输入的网络结构和参数固化到一个文件中，所以通过加载保存的模型，可以不用重新构建网络结构而直接用于预测，易于模型部署。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Numpy/">#Numpy</a>
      
        <a href="/tags/Pandas/">#Pandas</a>
      
        <a href="/tags/PIL/">#PIL</a>
      
        <a href="/tags/Matplotlib/">#Matplotlib</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>ML_DL</div>
      <div>https://alleyf.github.io/2022/09/b217f7fd8a2d.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>alleyf</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年9月11日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年3月27日</div>
        </div>
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/09/d3c714db982c.html" title="SignalSample">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">SignalSample</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/09/cb13af24c3a7.html" title="Html5+Css+Js">
                        <span class="hidden-mobile">Html5+Css+Js</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.7.2/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"636f2864e051a199b91c","clientSecret":"5185fd11115bbcd8d2f636bc80de6bed98ed14c1","repo":"Gitalk","owner":"Alleyf","admin":["Alleyf"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":false,"proxy":"https://shielded-brushlands-08810.herokuapp.com/https://github.com/login/oauth/access_token"},
          {
            id: 'cd910494a38effa58342bda87f04b74e'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://github.com/Alleyf" target="_blank" rel="nofollow noopener"><span>Alleyf</span></a> <i class="iconfont icon-love"></i> <a href="https://fcsy.fit" target="_blank" rel="nofollow noopener"><span>Homepage</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      陕ICP备2022010038号
    </a>
  </span>
  
    
      <span>
        <a
          href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2022010038"
          rel="nofollow noopener"
          class="beian-police"
          target="_blank"
        >
          
            <span style="visibility: hidden; width: 0">|</span>
            <img src="http://qnpicmap.fcsluck.top/pics/202311161820757.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
          
          <span>陕公网安备2022010038号</span>
        </a>
      </span>
    
  
</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start --><script src="/assets/mmedia/mmedia-loader.js"></script><!-- hexo injector body_end end --></body>
</html>
